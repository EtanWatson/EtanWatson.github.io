<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EWSUN</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-10-08T13:27:08.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>EtanWatson</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python数据采集（采集JavaScript）</title>
    <link href="http://yoursite.com/2017/10/07/python%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%EF%BC%88%E9%87%87%E9%9B%86JavaScript%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/07/python数据采集（采集JavaScript）/</id>
    <published>2017-10-07T06:33:28.000Z</published>
    <updated>2017-10-08T13:27:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="采集JavaScript"><a href="#采集JavaScript" class="headerlink" title="采集JavaScript"></a>采集JavaScript</h1><h2 id="常用的JavaScript库"><a href="#常用的JavaScript库" class="headerlink" title="常用的JavaScript库"></a>常用的JavaScript库</h2><h3 id="jQuery"><a href="#jQuery" class="headerlink" title="jQuery"></a>jQuery</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;script src=<span class="string">"http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"</span>&gt;&lt;/ script&gt;</div></pre></td></tr></table></figure><h3 id="Google-Analytics"><a href="#Google-Analytics" class="headerlink" title="Google Analytics"></a>Google Analytics</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">&lt;!-- Google Analytics --&gt;&lt;script <span class="built_in">type</span>=<span class="string">"text/javascript"</span>&gt;</div><div class="line">var _gaq = _gaq || []; </div><div class="line">_gaq.push([<span class="string">'_setAccount'</span>, <span class="string">'UA-4591498-1'</span>]); </div><div class="line">_gaq.push([<span class="string">'_setDomainName'</span>, <span class="string">'oreilly.com'</span>]); </div><div class="line">_gaq.push([<span class="string">'_addIgnoredRef'</span>, <span class="string">'oreilly.com'</span>]); </div><div class="line">_gaq.push([<span class="string">'_setSiteSpeedSampleRate'</span>, 50]); </div><div class="line">_gaq.push([<span class="string">'_trackPageview'</span>]);(<span class="function"><span class="title">function</span></span>() &#123; var ga = document.createElement(<span class="string">'script'</span>); </div><div class="line">ga.type = <span class="string">'text/javascript'</span>; </div><div class="line">ga.async = <span class="literal">true</span>; </div><div class="line">ga.src = (<span class="string">'https:'</span> == document.location.protocol ? <span class="string">'https://ssl'</span> : <span class="string">'http://www'</span>) + <span class="string">'.google-analytics.com/ga.js'</span>; </div><div class="line">var s = document.getElementsByTagName(<span class="string">'script'</span>)[0]; s.parentNode.insertBefore(ga, s); &#125;();&lt;/script&gt;</div><div class="line">``` </div><div class="line"><span class="comment">### Google地图</span></div><div class="line">``` bash </div><div class="line">var marker = new google.maps.Marker(&#123;position: new google.maps.LatLng(-25.363882,131.044922), </div><div class="line">map: map,title: <span class="string">'Some marker text'</span>&#125;);</div><div class="line"><span class="comment">### Ajax（异步JavaScript和XML）和动态HTML（有没有用JavaScript控制HTML和CSS元素）</span></div><div class="line">解决方案：</div><div class="line">1.直接从JavaScript代码里采集内容。</div><div class="line">2.用Python的第三方运行JavaScript，直接采集你在浏览器里看到的页面</div><div class="line"><span class="comment">### 在Python中用Selenium执行JavaScript</span></div><div class="line">Selenium是一个强大的网络数据采集工具，它还被广泛用于获取精确的网站快照，因为它们可以直接运行在浏览器上</div><div class="line">phantomJS是一个无头（headless）浏览器</div><div class="line">Selenium库是一个在WebDriver上调用的API。WebDriver有点儿想可以加载网站的浏览器，但是它也可以像BeautifulSoup对象一样用来查找页面元素，与页面上的元素进行交互（发送文本，点击等），以及执行其他动作来运行</div><div class="line">网络爬虫</div><div class="line">下面代码可以获取前面测试页面上Ajax”墙”后面的内容</div><div class="line">``` bash </div><div class="line">from selenium import webdriver</div><div class="line">import time</div><div class="line">diver = webdriver.PhantomJS(executable_path=‘’) <span class="comment">#你的PhantomJS可执行文件的路径</span></div><div class="line">driver.get(<span class="string">"http://pythonscraping.com/pages/javascript/ajaxDemo.html"</span>)</div><div class="line">time.sleep(3)</div><div class="line"><span class="built_in">print</span>(driver.find_element_by_id(‘content’).text)</div><div class="line">driver.close()</div></pre></td></tr></table></figure><p>#这种方法虽然奏效，但是效率还不高，由于页面加载时间的不确定性，所有有很大的弊端</p><h4 id="Selenium的选择器"><a href="#Selenium的选择器" class="headerlink" title="Selenium的选择器"></a>Selenium的选择器</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">driver.find_element_by_id(<span class="string">'content'</span>).text</div><div class="line">driver.find_element_by_css_selector(<span class="string">"#content"</span>)driver.find_element_by_tag_name(<span class="string">"div"</span>)</div><div class="line">driver.find_elements_by_css_selector(<span class="string">"#content"</span>)driver.find_elements_by_css_selector(<span class="string">"div"</span>)</div><div class="line"><span class="comment">#另外，如果你还想用BeautifulSoup来解析网页内容，可以用WebDriver的page_source</span></div><div class="line">pageSource = driver.page_sourcebsObj = BeautifulSoup(pageSource) </div><div class="line"><span class="built_in">print</span>(bsObj.find(id=<span class="string">"content"</span>).get_text())</div><div class="line">``` </div><div class="line"><span class="comment">#### 使用Selenium不断检查，是否完全加载</span></div><div class="line">``` bash </div><div class="line">from selenium.webdriver.common.by import By</div><div class="line">from selenium.webdriver.support.ui import WebDriverWait</div><div class="line">from selenium.webdriver.support import expected_conditions as EC</div><div class="line">from selenium import webdriver</div><div class="line">import time</div><div class="line"></div><div class="line">driver = webdriver.PhantomJS(executable_path=<span class="string">''</span>)</div><div class="line">driver.get(<span class="string">"http://pythonscraping.com/pages/javascript/ajaxDemo.html"</span>)</div><div class="line">try:</div><div class="line">    element = WebDriverWait(driver, 10).until(</div><div class="line">        EC.presence_of_all_elements_located((By.ID, <span class="string">"loadedButton"</span>)))</div><div class="line">finally:</div><div class="line">    <span class="built_in">print</span>(driver.find_element_by_id(<span class="string">"content"</span>).text)</div><div class="line">    driver.close()</div></pre></td></tr></table></figure><p>需要注意的就是WebDriverWait和expected_conditions,这两个模块组合起来，构成了Selenium的隐式等待</p><h5 id="Selenium隐式等待"><a href="#Selenium隐式等待" class="headerlink" title="Selenium隐式等待"></a>Selenium隐式等待</h5><p>1.没有明确的等待时间，但是有最大等待时间</p><p>#DOM触发的状态是用expected_conditions定义的<br>• 弹出一个提示框<br>• 一个元素被选中(比如文本框)<br>• 页面的标题改变了，或者某个文字显示在页面上或者某个元素里 • 一个元素在DOM中变成可见的，或者一个元素从DOM中消失了</p><h5 id="定位器"><a href="#定位器" class="headerlink" title="定位器"></a>定位器</h5><p>大多数的期望条件在使用前都需要你先指定等待的目标元素，定位器是一种抽象的查询语言，用 By 对象表示，可以用于不同的场合，包括创建选择器。<br>一个定位器被用来查找id是loadedButton -&gt; EX：<br>EC.presence_of_element_located((By.ID, “loadedButton”))<br>定位器还可以用来创建选择器，配合WebDriver的find_element函数使用：<br>print(driver.find_element(By.ID, “content”).text)<br>下面这行代码的功能和示例代码中一样：<br>print(driver.find_element_by_id(“content”).text)</p><p>下面是定位器通过By对象进行选择的策略<br>• ID<br>• CLASS_NAME(HTML的class属性)<br>• CSS_SELECTOR:通过 CSS 的 class、id、tag 属性名来查找元素，<br>           用 #idName、.className、tagName 表示。<br>• LINK_TEXT：通过链接文字查找 HTML 的 <a> 标签。例如，如果一个链接的文字是“Next”，就可以 用(By.LINK_TEXT, “Next”)来选择。<br>• PARTIAL_LINK_TEXT：与 LINK_TEXT 类似，只是通过部分链接文字来查找。<br>• NAME：通过 HTML 标签的 name 属性查找。这在处理 HTML 表单时非常方便。<br>• TAG_NAME：通过 HTML 标签的名称查找。<br>• XPATH：用 XPath 表达式(语法在下面介绍)选择匹配的元素。</a></p><h6 id="XPath语法"><a href="#XPath语法" class="headerlink" title="XPath语法"></a>XPath语法</h6><p>在XPath语法中有四个重要概念。<br>一、根节点和非根节点</p><ol><li>/div选择div节点，只有当它是文档的根节点时</li><li>//div选择文档中所有的div节点（包括非根节点）<br>二、通过属性选择节点</li><li>//@href选择带href属性的所有节点</li><li>//a[@href=‘<a href="http://google.com’]选择页面中所有指向Google网站的链接" target="_blank" rel="external">http://google.com’]选择页面中所有指向Google网站的链接</a><br>三、通过位置选择节点</li><li>//a[3]选着文档中的第三个链接</li><li>//table[last()]选择文档中的最后一个表</li><li>//a[positon() &lt; 3]选择文档中的前三个链接<br>四、星号（*）匹配任意字符串或节点，可以在不同条件下使用</li><li>//table/tr/*选择所有表格行tr标签的所有子节点（这很适合选择th和td标签）</li><li>//div[@0]选择带有任意属性的所有div标签<br>更多，请参考微软的XPath语法页面：<a href="https://msdn.microsoft.com/en-us/enus/library/ms256471" target="_blank" rel="external">https://msdn.microsoft.com/en-us/enus/library/ms256471</a></li></ol><h2 id="处理重定向"><a href="#处理重定向" class="headerlink" title="处理重定向"></a>处理重定向</h2><h3 id="客户端重定向"><a href="#客户端重定向" class="headerlink" title="客户端重定向"></a>客户端重定向</h3><p>是在服务器将页面内容发送到浏览器之前，由浏览器执行 JavaScript 完成的 页面跳转，而不是服务器完成的跳转。<br>在网络采集是的差异：（客户端重定向和服务端重定向）<br>根据具体情况，<br>服务器端重定向一般都可以轻松地通过 Python 的 urllib 库解决，不需要使用 Selenium (更多的介绍请参考第 3 章)。客户端重定向却不能这样处理，除非你有工具可以执行<br>JavaScript。<br>Selenium的问题在于怎么识别一个页面已经完成重定向了</p><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>首先从页面开始加载时就”监视”DOM中的一个元素，然后重复调用这个元素直到Selenium抛出一个StaleElementRefereceException异常，也就是说，元素不在页面的DOM里了，说明这时网站已经跳转：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">from selenium import webdriver</div><div class="line">import time</div><div class="line">from selenium.webdriver.remote.webelement import WebElement</div><div class="line">from selenium.common.exceptions import StaleElementReferenceException</div><div class="line"></div><div class="line">def waitForLoad(driver):</div><div class="line">    elem = driver.find_element_by_tag_name(<span class="string">"html"</span>)</div><div class="line">    count = 0</div><div class="line">    <span class="keyword">while</span> True:</div><div class="line">        count += 1</div><div class="line">        <span class="keyword">if</span> count &gt; 20:</div><div class="line">            <span class="built_in">print</span>(<span class="string">"Timing out after 10 secods and returning"</span>)</div><div class="line">            <span class="built_in">return</span></div><div class="line">        time.sleep(.5)</div><div class="line">        try:</div><div class="line">            elem == driver.find_element_by_tag_name(<span class="string">"html"</span>)</div><div class="line">        except StaleElementReferenceException:</div><div class="line">            <span class="built_in">return</span></div><div class="line">driver = webdriver.PhantomJS(executable_path=“/****/**/MachineLearning/phantomjs-2.1.1-macosx/bin/phantomjs<span class="string">")</span></div><div class="line"><span class="string">driver.get("</span>http://pythonscraping.com/pages/javascript/redirectDemo1.html<span class="string">")</span></div><div class="line"><span class="string">waitForLoad(driver)</span></div><div class="line"><span class="string">print(driver.page_source)</span></div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;采集JavaScript&quot;&gt;&lt;a href=&quot;#采集JavaScript&quot; class=&quot;headerlink&quot; title=&quot;采集JavaScript&quot;&gt;&lt;/a&gt;采集JavaScript&lt;/h1&gt;&lt;h2 id=&quot;常用的JavaScript库&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（穿越网页表单与登录窗口进行采集）</title>
    <link href="http://yoursite.com/2017/10/07/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E7%A9%BF%E8%B6%8A%E7%BD%91%E9%A1%B5%E8%A1%A8%E5%8D%95%E4%B8%8E%E7%99%BB%E5%BD%95%E7%AA%97%E5%8F%A3%E8%BF%9B%E8%A1%8C%E9%87%87%E9%9B%86%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/07/python数据收集（穿越网页表单与登录窗口进行采集）/</id>
    <published>2017-10-07T02:47:05.000Z</published>
    <updated>2017-10-07T06:32:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="穿越网页表单与登录窗口进行采集"><a href="#穿越网页表单与登录窗口进行采集" class="headerlink" title="穿越网页表单与登录窗口进行采集"></a>穿越网页表单与登录窗口进行采集</h1><p>如何获取登录窗口背后的信息呢？？？这一节我们重点介绍POST方法，即把消息推送给网络服务器进行存储和分析，像网站搞得URL链接可以帮助用户发送GET请求一样，HTML表单可以帮助用户发出POST请求</p><h2 id="Python-Requests-库（http-www-python-requests-org-）"><a href="#Python-Requests-库（http-www-python-requests-org-）" class="headerlink" title="Python Requests 库（http://www.python-requests.org/）"></a>Python Requests 库（<a href="http://www.python-requests.org/）" target="_blank" rel="external">http://www.python-requests.org/）</a></h2><p>是一个擅长处理那些复杂的HTTP请求。cookie，header（响应头和请求头）等内容的Python第三方库。<br>安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; pip(pip3) install Requests</div></pre></td></tr></table></figure></p><h2 id="提交一个基本表单"><a href="#提交一个基本表单" class="headerlink" title="提交一个基本表单"></a>提交一个基本表单</h2><p>注：如果你想模拟表单提交数据的行为，你就需要保证你的变量名称与字段名称是一一对应的</p><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"></div><div class="line">params = &#123;<span class="string">'firstname'</span>: <span class="string">'Ryan'</span>, <span class="string">'listname'</span>: <span class="string">'Mitchell'</span>&#125;</div><div class="line">r = requests.post(<span class="string">"http://pythonscraping.com/files/processing.php"</span>, data=params)</div><div class="line"><span class="built_in">print</span>(r.text)</div><div class="line"><span class="comment">#在大多数情况下，你只需关注两件事：</span></div><div class="line"><span class="comment">#• 你想提交数据的字段名称（name字段）(在这个例子中是email_addr)</span><span class="comment">#• 表单的action属性，也就是表单提交后网站会显示的页面(在这个例子中是http://post.oreilly.com</span></div><div class="line"><span class="comment">#/client/o/oreilly/forms/quicksignup.cgi)</span></div><div class="line"><span class="comment">#运行代码示例</span></div><div class="line">import requestsparams = &#123;<span class="string">'email_addr'</span>: <span class="string">'ryan.e.mitchell@gmail.com'</span>&#125;r = requests.post(<span class="string">"http://post.oreilly.com/client/o/oreilly/forms/                        quicksignup.cgi"</span>, data=params)<span class="built_in">print</span>(r.text)</div></pre></td></tr></table></figure><h2 id="单选按钮、复选框和其他输入"><a href="#单选按钮、复选框和其他输入" class="headerlink" title="单选按钮、复选框和其他输入"></a>单选按钮、复选框和其他输入</h2><p>无论html提供了多么复杂的控件，仍然只有亮剑事是需要关注的：字段名称（name）和值（比较复杂，有可能是通过JavaScript生成的，<br>而取色器有类似于#F03030这样的值）</p><h3 id="跟踪GET请求获取值"><a href="#跟踪GET请求获取值" class="headerlink" title="跟踪GET请求获取值"></a>跟踪GET请求获取值</h3><p>get请求的值一般会在URL中体现，类似于：<a href="http://domainname.com?thing1=foo&amp;thing2=bar" target="_blank" rel="external">http://domainname.com?thing1=foo&amp;thing2=bar</a></p><p>你就会明白这个请求就是下面这种表单：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;form method=<span class="string">"GET"</span> action=<span class="string">"someProcessor.php"</span>&gt;&lt;input <span class="built_in">type</span>=<span class="string">"someCrazyInputType"</span> name=<span class="string">"thing1"</span> value=<span class="string">"foo"</span> /&gt; &lt;input <span class="built_in">type</span>=<span class="string">"anotherCrazyInputType"</span> name=<span class="string">"thing2"</span> value=<span class="string">"bar"</span> /&gt; &lt;input <span class="built_in">type</span>=<span class="string">"submit"</span> value=<span class="string">"Submit"</span> /&gt;&lt;/form&gt;</div></pre></td></tr></table></figure></p><p>对应的python参数就是：<br>{‘thing1’:’foo’, ‘thing2’:’bar’}</p><h2 id="提交文件和图像"><a href="#提交文件和图像" class="headerlink" title="提交文件和图像"></a>提交文件和图像</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">files = &#123;<span class="string">'uploadFile'</span>: open(<span class="string">'./files/header.png'</span>, <span class="string">'rb'</span>)&#125;</div><div class="line">r = requests.post(<span class="string">"http://pythonscraping.com/pages/processing2.php"</span>,files=files)</div><div class="line"></div><div class="line"><span class="built_in">print</span>(r.text)</div></pre></td></tr></table></figure><h2 id="处理登录和cookie"><a href="#处理登录和cookie" class="headerlink" title="处理登录和cookie"></a>处理登录和cookie</h2><p>问题：你可以一整天只提交一次登录表单，但是如果你没有一直关注表单后来回传给你的那个cookie，那么一段时间以后再次访问新页面<br>时，你的登录状态就会丢失，需要重新登录<br>Requests库跟踪cookie同样简单：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">params = &#123;‘username’:’Ryan’,’password’:’password’&#125;</div><div class="line">r = requests.post(<span class="string">"http://pythonscraping.com/pages/cookies/welcome.php"</span>, params) </div><div class="line"><span class="built_in">print</span>(<span class="string">"Cookie is set to:"</span>)<span class="built_in">print</span>(r.cookies.get_dict())<span class="built_in">print</span>(<span class="string">"-----------"</span>)<span class="built_in">print</span>(<span class="string">"Going to profile page..."</span>)r = requests.get(<span class="string">"http://pythonscraping.com/pages/cookies/profile.php"</span>,                      cookies=r.cookies)<span class="built_in">print</span>(r.text)</div></pre></td></tr></table></figure></p><h3 id="使用session"><a href="#使用session" class="headerlink" title="使用session"></a>使用session</h3><p>如果你面对的网站比较复杂，它经常暗自调整cookie，或者如果你从一开始就完全不想要用cookie，该如何处理呢</p><h3 id="HTTP基本接入认证"><a href="#HTTP基本接入认证" class="headerlink" title="HTTP基本接入认证"></a>HTTP基本接入认证</h3><p>RRequests库有一个auth模块专门用来处理HTTP认证：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">from requests.auth import AuthBase</div><div class="line">from request.auth import HTTPBasicAuth</div><div class="line"></div><div class="line">auth = HTTPBasicAuth(<span class="string">'ryan'</span>, <span class="string">'password'</span>)     r = requests.post(url=<span class="string">"http://pythonscraping.com/pages/auth/login.php"</span>, auth=auth)<span class="built_in">print</span>(r.text)</div></pre></td></tr></table></figure></p><h2 id="其他表单问题（CAPTCHA-验证码）"><a href="#其他表单问题（CAPTCHA-验证码）" class="headerlink" title="其他表单问题（CAPTCHA:验证码）"></a>其他表单问题（CAPTCHA:验证码）</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;穿越网页表单与登录窗口进行采集&quot;&gt;&lt;a href=&quot;#穿越网页表单与登录窗口进行采集&quot; class=&quot;headerlink&quot; title=&quot;穿越网页表单与登录窗口进行采集&quot;&gt;&lt;/a&gt;穿越网页表单与登录窗口进行采集&lt;/h1&gt;&lt;p&gt;如何获取登录窗口背后的信息呢？？？这
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（自然语言处理）</title>
    <link href="http://yoursite.com/2017/10/04/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/04/python数据收集（自然语言处理）/</id>
    <published>2017-10-04T03:08:41.000Z</published>
    <updated>2017-10-05T03:30:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="自然语言的处理"><a href="#自然语言的处理" class="headerlink" title="自然语言的处理"></a>自然语言的处理</h1><h2 id="概括数据"><a href="#概括数据" class="headerlink" title="概括数据"></a>概括数据</h2><p>前面已经介绍过了n-gram模型，即n个单词长度的词组<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import string</div><div class="line">import operator</div><div class="line"></div><div class="line">def cleanInput(input):</div><div class="line">    input = re.sub(<span class="string">'\n+'</span>,<span class="string">" "</span>,input).lower()</div><div class="line">    input = re.sub(<span class="string">'\[[0-9]*\]'</span>,<span class="string">""</span>,input)</div><div class="line">    input = re.sub(<span class="string">' +'</span>, <span class="string">" "</span>, input)</div><div class="line">    input = bytes(input, <span class="string">"UTF-8"</span>)</div><div class="line">    input = input.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>)</div><div class="line">    cleanInput = []</div><div class="line">    input = input.split(<span class="string">' '</span>)</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> input:</div><div class="line">        item = item.strip(string.punctuation)</div><div class="line">        <span class="keyword">if</span> len(item) &gt; 1 or (item.lower() == <span class="string">'a'</span> or item.lower() == <span class="string">'i'</span>):</div><div class="line">            cleanInput.append(item)</div><div class="line">    <span class="built_in">return</span> cleanInput</div><div class="line"></div><div class="line">def ngrams(input, n):</div><div class="line">    input = cleanInput(input)</div><div class="line">    output = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input)-n+1):</div><div class="line">        ngramTemp =<span class="string">" "</span>.join(input[i:i+n])</div><div class="line">        <span class="keyword">if</span> ngramTemp not <span class="keyword">in</span> output:</div><div class="line">            output[ngramTemp] = 0</div><div class="line">        output[ngramTemp] += 1</div><div class="line">    <span class="built_in">return</span> output</div><div class="line"></div><div class="line">content = str(urlopen(<span class="string">"http://pythonscraping.com/files/inaugurationSpeech.txt"</span>).<span class="built_in">read</span>(),<span class="string">'utf-8'</span>)</div><div class="line">ngrams = ngrams(content, 2)</div><div class="line">sortedNGrams = sorted(ngrams.items(), key=operator.itemgetter(1), reverse=True)</div><div class="line"><span class="built_in">print</span>(sortedNGrams)</div><div class="line"><span class="comment">#结果</span></div><div class="line">&gt;&gt;&gt; (<span class="string">'of the'</span>, 213), (<span class="string">'in the'</span>, 65), (<span class="string">'to the'</span>, 61), (<span class="string">'by the'</span>, 41), (<span class="string">'the constitution'</span>, 34),</div></pre></td></tr></table></figure></p><p>我们会发现，其实像of the，in the ，对我们来讲一点儿都不重要，而 the constitution相对来说就比较重要</p><h3 id="去掉看上去无用的字符"><a href="#去掉看上去无用的字符" class="headerlink" title="去掉看上去无用的字符"></a>去掉看上去无用的字符</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import string</div><div class="line">import operator</div><div class="line"></div><div class="line"><span class="comment">#最常用的5000个单词列表可以免费获取，我们现在提取出前100个</span></div><div class="line"><span class="comment">#返回boolean类型，如果包含，就返回true</span></div><div class="line">def isCommon(ngram):</div><div class="line">    commonWords = [<span class="string">"the"</span>, <span class="string">"be"</span>, <span class="string">"and"</span>, <span class="string">"of"</span>, <span class="string">"a"</span>, <span class="string">"in"</span>, <span class="string">"to"</span>, <span class="string">"have"</span>, <span class="string">"it"</span>,</div><div class="line">             <span class="string">"i"</span>, <span class="string">"that"</span>, <span class="string">"for"</span>, <span class="string">"you"</span>, <span class="string">"he"</span>, <span class="string">"with"</span>, <span class="string">"on"</span>, <span class="string">"do"</span>, <span class="string">"say"</span>, <span class="string">"this"</span>,</div><div class="line">             <span class="string">"they"</span>, <span class="string">"is"</span>, <span class="string">"an"</span>, <span class="string">"at"</span>, <span class="string">"but"</span>,<span class="string">"we"</span>, <span class="string">"his"</span>, <span class="string">"from"</span>, <span class="string">"that"</span>, <span class="string">"not"</span>,</div><div class="line">             <span class="string">"by"</span>, <span class="string">"she"</span>, <span class="string">"or"</span>, <span class="string">"as"</span>, <span class="string">"what"</span>, <span class="string">"go"</span>, <span class="string">"their"</span>,<span class="string">"can"</span>, <span class="string">"who"</span>, <span class="string">"get"</span>,</div><div class="line">             <span class="string">"if"</span>, <span class="string">"would"</span>, <span class="string">"her"</span>, <span class="string">"all"</span>, <span class="string">"my"</span>, <span class="string">"make"</span>, <span class="string">"about"</span>, <span class="string">"know"</span>, <span class="string">"will"</span>,</div><div class="line">             <span class="string">"as"</span>, <span class="string">"up"</span>, <span class="string">"one"</span>, <span class="string">"time"</span>, <span class="string">"has"</span>, <span class="string">"been"</span>, <span class="string">"there"</span>, <span class="string">"year"</span>, <span class="string">"so"</span>,</div><div class="line">             <span class="string">"think"</span>, <span class="string">"when"</span>, <span class="string">"which"</span>, <span class="string">"them"</span>, <span class="string">"some"</span>, <span class="string">"me"</span>, <span class="string">"people"</span>, <span class="string">"take"</span>,</div><div class="line">             <span class="string">"out"</span>, <span class="string">"into"</span>, <span class="string">"just"</span>, <span class="string">"see"</span>, <span class="string">"him"</span>, <span class="string">"your"</span>, <span class="string">"come"</span>, <span class="string">"could"</span>, <span class="string">"now"</span>,</div><div class="line">             <span class="string">"than"</span>, <span class="string">"like"</span>, <span class="string">"other"</span>, <span class="string">"how"</span>, <span class="string">"then"</span>, <span class="string">"its"</span>, <span class="string">"our"</span>, <span class="string">"two"</span>, <span class="string">"more"</span>,</div><div class="line">             <span class="string">"these"</span>, <span class="string">"want"</span>, <span class="string">"way"</span>, <span class="string">"look"</span>, <span class="string">"first"</span>, <span class="string">"also"</span>, <span class="string">"new"</span>, <span class="string">"because"</span>,</div><div class="line">             <span class="string">"day"</span>, <span class="string">"more"</span>, <span class="string">"use"</span>, <span class="string">"no"</span>, <span class="string">"man"</span>, <span class="string">"find"</span>, <span class="string">"here"</span>, <span class="string">"thing"</span>, <span class="string">"give"</span>,</div><div class="line">             <span class="string">"many"</span>, <span class="string">"well"</span>]</div><div class="line"></div><div class="line">    <span class="keyword">if</span> ngram <span class="keyword">in</span> commonWords:</div><div class="line">        <span class="built_in">return</span> True</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="built_in">return</span> False</div><div class="line"></div><div class="line">def cleanInput(input):</div><div class="line">    input = re.sub(<span class="string">'\n+'</span>,<span class="string">" "</span>,input).lower() <span class="comment"># 匹配换行,用空格替换换行符</span></div><div class="line">    input = re.sub(<span class="string">'\[[0-9]*\]'</span>,<span class="string">""</span>,input) <span class="comment"># 剔除类似[1]这样的引用标记</span></div><div class="line">    input = re.sub(<span class="string">' +'</span>, <span class="string">" "</span>, input) <span class="comment">#把连续多个空格替换成一个空格</span></div><div class="line">    input = bytes(input, <span class="string">"UTF-8"</span>)</div><div class="line">    input = input.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>)</div><div class="line">    cleanInput = []</div><div class="line">    input = input.split(<span class="string">' '</span>)</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> input:</div><div class="line">        item = item.strip(string.punctuation)</div><div class="line">        <span class="keyword">if</span> len(item) &gt; 1 or (item.lower() == <span class="string">'a'</span> or item.lower() == <span class="string">'i'</span>):</div><div class="line">            cleanInput.append(item)</div><div class="line">    <span class="built_in">return</span> cleanInput</div><div class="line"></div><div class="line">def ngrams(input, n):</div><div class="line">    input = cleanInput(input)</div><div class="line">    output = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input)-n+1):</div><div class="line">        ngramTemp =<span class="string">" "</span>.join(input[i:i+n]) <span class="comment">#这句话将n-grams拆分成n个元素组成的列表</span></div><div class="line">        <span class="keyword">if</span> isCommon(ngramTemp.split()[0]) or isCommon(ngramTemp.split()[1]):</div><div class="line">            pass</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">if</span> ngramTemp not <span class="keyword">in</span> output:</div><div class="line">                output[ngramTemp] = 0</div><div class="line">            output[ngramTemp] += 1</div><div class="line">    <span class="built_in">return</span> output</div><div class="line"></div><div class="line">content = str(urlopen(<span class="string">"http://pythonscraping.com/files/inaugurationSpeech.txt"</span>).<span class="built_in">read</span>(),<span class="string">'utf-8'</span>)</div><div class="line">ngrams = ngrams(content, 2)</div><div class="line">sortedNGrams = sorted(ngrams.items(), key=operator.itemgetter(1), reverse=True)</div><div class="line"><span class="built_in">print</span>(sortedNGrams)</div></pre></td></tr></table></figure><p>#结果</p><blockquote><blockquote><blockquote><p>[(‘united states’, 10), (‘general government’, 4), (‘executive department’, 4), (‘mr jefferson’, 3), (‘same causes’, 3),……]<br>我的疑惑：在进行”看似无用”的单词过滤的时候，是不是会将有用的单词过滤掉类似the constitution</p><h3 id="通过中心主题词，归纳文章核心"><a href="#通过中心主题词，归纳文章核心" class="headerlink" title="通过中心主题词，归纳文章核心"></a>通过中心主题词，归纳文章核心</h3><p>一种方法是搜索包含每个核心 n-gram 序列的第一句话，这个方法的理论是英语中段落的首句 往往是对后面内容的概述<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import string</div><div class="line">import operator</div><div class="line"></div><div class="line"><span class="comment">#最常用的5000个单词列表可以免费获取，我们现在提取出前100个</span></div><div class="line"><span class="comment">#返回boolean类型，如果包含，就返回true</span></div><div class="line">def isCommon(ngram):</div><div class="line">    commonWords = [<span class="string">"the"</span>, <span class="string">"be"</span>, <span class="string">"and"</span>, <span class="string">"of"</span>, <span class="string">"a"</span>, <span class="string">"in"</span>, <span class="string">"to"</span>, <span class="string">"have"</span>, <span class="string">"it"</span>,</div><div class="line">             <span class="string">"i"</span>, <span class="string">"that"</span>, <span class="string">"for"</span>, <span class="string">"you"</span>, <span class="string">"he"</span>, <span class="string">"with"</span>, <span class="string">"on"</span>, <span class="string">"do"</span>, <span class="string">"say"</span>, <span class="string">"this"</span>,</div><div class="line">             <span class="string">"they"</span>, <span class="string">"is"</span>, <span class="string">"an"</span>, <span class="string">"at"</span>, <span class="string">"but"</span>,<span class="string">"we"</span>, <span class="string">"his"</span>, <span class="string">"from"</span>, <span class="string">"that"</span>, <span class="string">"not"</span>,</div><div class="line">             <span class="string">"by"</span>, <span class="string">"she"</span>, <span class="string">"or"</span>, <span class="string">"as"</span>, <span class="string">"what"</span>, <span class="string">"go"</span>, <span class="string">"their"</span>,<span class="string">"can"</span>, <span class="string">"who"</span>, <span class="string">"get"</span>,</div><div class="line">             <span class="string">"if"</span>, <span class="string">"would"</span>, <span class="string">"her"</span>, <span class="string">"all"</span>, <span class="string">"my"</span>, <span class="string">"make"</span>, <span class="string">"about"</span>, <span class="string">"know"</span>, <span class="string">"will"</span>,</div><div class="line">             <span class="string">"as"</span>, <span class="string">"up"</span>, <span class="string">"one"</span>, <span class="string">"time"</span>, <span class="string">"has"</span>, <span class="string">"been"</span>, <span class="string">"there"</span>, <span class="string">"year"</span>, <span class="string">"so"</span>,</div><div class="line">             <span class="string">"think"</span>, <span class="string">"when"</span>, <span class="string">"which"</span>, <span class="string">"them"</span>, <span class="string">"some"</span>, <span class="string">"me"</span>, <span class="string">"people"</span>, <span class="string">"take"</span>,</div><div class="line">             <span class="string">"out"</span>, <span class="string">"into"</span>, <span class="string">"just"</span>, <span class="string">"see"</span>, <span class="string">"him"</span>, <span class="string">"your"</span>, <span class="string">"come"</span>, <span class="string">"could"</span>, <span class="string">"now"</span>,</div><div class="line">             <span class="string">"than"</span>, <span class="string">"like"</span>, <span class="string">"other"</span>, <span class="string">"how"</span>, <span class="string">"then"</span>, <span class="string">"its"</span>, <span class="string">"our"</span>, <span class="string">"two"</span>, <span class="string">"more"</span>,</div><div class="line">             <span class="string">"these"</span>, <span class="string">"want"</span>, <span class="string">"way"</span>, <span class="string">"look"</span>, <span class="string">"first"</span>, <span class="string">"also"</span>, <span class="string">"new"</span>, <span class="string">"because"</span>,</div><div class="line">             <span class="string">"day"</span>, <span class="string">"more"</span>, <span class="string">"use"</span>, <span class="string">"no"</span>, <span class="string">"man"</span>, <span class="string">"find"</span>, <span class="string">"here"</span>, <span class="string">"thing"</span>, <span class="string">"give"</span>,</div><div class="line">             <span class="string">"many"</span>, <span class="string">"well"</span>]</div><div class="line"></div><div class="line">    <span class="keyword">if</span> ngram <span class="keyword">in</span> commonWords:</div><div class="line">        <span class="built_in">return</span> True</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="built_in">return</span> False</div><div class="line"></div><div class="line">def cleanInput(input):</div><div class="line">    input = re.sub(<span class="string">'\n+'</span>,<span class="string">" "</span>,input).lower() <span class="comment"># 匹配换行,用空格替换换行符</span></div><div class="line">    input = re.sub(<span class="string">'\[[0-9]*\]'</span>,<span class="string">""</span>,input) <span class="comment"># 剔除类似[1]这样的引用标记</span></div><div class="line">    input = re.sub(<span class="string">' +'</span>, <span class="string">" "</span>, input) <span class="comment">#把连续多个空格替换成一个空格</span></div><div class="line">    input = bytes(input, <span class="string">"UTF-8"</span>)</div><div class="line">    input = input.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>)</div><div class="line">    cleanInput = []</div><div class="line">    input = input.split(<span class="string">' '</span>)</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> input:</div><div class="line">        item = item.strip(string.punctuation)</div><div class="line">        <span class="keyword">if</span> len(item) &gt; 1 or (item.lower() == <span class="string">'a'</span> or item.lower() == <span class="string">'i'</span>):</div><div class="line">            cleanInput.append(item)</div><div class="line">    <span class="built_in">return</span> cleanInput</div><div class="line"></div><div class="line">def ngrams(input, n):</div><div class="line">    input = cleanInput(input)</div><div class="line">    output = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input)-n+1):</div><div class="line">        ngramTemp =<span class="string">" "</span>.join(input[i:i+n]) <span class="comment">#这句话将n-grams拆分成n个元素组成的列表</span></div><div class="line">        <span class="keyword">if</span> isCommon(ngramTemp.split()[0]) or isCommon(ngramTemp.split()[1]):</div><div class="line">            pass</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">if</span> ngramTemp not <span class="keyword">in</span> output:</div><div class="line">                output[ngramTemp] = 0</div><div class="line">            output[ngramTemp] += 1</div><div class="line">    <span class="built_in">return</span> output</div><div class="line"></div><div class="line"><span class="comment">#获取核心词在的句子</span></div><div class="line">def getFirstSentenceCOntaining(ngram, content):</div><div class="line">    sentences = content.split(<span class="string">'.'</span>)</div><div class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</div><div class="line">        <span class="keyword">if</span> ngram <span class="keyword">in</span> sentence:</div><div class="line">            <span class="built_in">return</span> sentence</div><div class="line">    <span class="built_in">return</span> <span class="string">""</span></div><div class="line">content = str(urlopen(<span class="string">"http://pythonscraping.com/files/inaugurationSpeech.txt"</span>).<span class="built_in">read</span>(),<span class="string">'utf-8'</span>)</div><div class="line">ngrams = ngrams(content, 2)</div><div class="line"><span class="comment">#核心词</span></div><div class="line">sortedNGrams = sorted(ngrams.items(), key=operator.itemgetter(1), reverse=True)</div><div class="line"><span class="built_in">print</span>(sortedNGrams)</div><div class="line"><span class="comment">#核心句</span></div><div class="line"><span class="keyword">for</span> top3 <span class="keyword">in</span> range(3):</div><div class="line">    <span class="built_in">print</span>(<span class="string">"&gt;"</span>+getFirstSentenceCOntaining(sortedNGrams[top3][0],content.lower()))</div><div class="line">``` </div><div class="line">结果：</div><div class="line">&gt; the constitution of the united states is the instrument containing this grant of power to the several departments composing the government</div><div class="line"></div><div class="line">&gt; the general government has seized upon none of the reserved rights of the states</div><div class="line"></div><div class="line">&gt; such a one was afforded by the executive department constituted by the constitution</div><div class="line"></div><div class="line">我的困惑：这里返回的仅仅是匹配到的第一句话（也就是该核心词匹配的到第一句话，后面的都放弃了，是否会有不妥）</div><div class="line"></div><div class="line"><span class="comment">## 马尔可夫模型</span></div><div class="line">随机事件的特点 是一个离散事件发生之后，另一个离散事件将在前一个事件的条件下以一定的概率发生。</div><div class="line">图：马尔科夫模型描述理论天气系统</div><div class="line">• 任何一个节点引出的所有可能的总和必须等于100%。无论是多么复杂的系统，必然会 在下一步发生若干事件中的一个事件。• 虽然这个天气系统在任一时间都只有三种可能，但是你可以用这个模型生成一个天气状 态的无限次转移列表。• 只有当前节点的状态会影响后一天的状态。如果你在“晴天”节点上，即使前100天都 是晴天或雨天都没关系，明天晴天的概率还是 70%。• 有些节点可能比其他节点较难到达。这个现象的原因用数学来解释非常复杂，但是可以 直观地看出，在这个系统中任意时间节点上，第二天是“雨天”的可能性(指向它的箭 头概率之和小于“100%”)比“晴天”或“多云”要小很多</div><div class="line">``` bash </div><div class="line"><span class="comment">#生成链为100的马尔可夫链</span></div><div class="line">from urllib.request import urlopen</div><div class="line">from random import randint</div><div class="line"></div><div class="line">def wordlistSum(wordList):</div><div class="line">    sum = 0</div><div class="line">    <span class="keyword">for</span> word, value <span class="keyword">in</span> wordList.items():</div><div class="line">        sum += value</div><div class="line">    <span class="built_in">return</span> sum</div><div class="line"></div><div class="line">def retrieveRandomWord(wordlist):</div><div class="line">    randIndex = randint(1, wordlistSum(wordlist))</div><div class="line">    <span class="keyword">for</span> word, value <span class="keyword">in</span> wordlist.items():</div><div class="line">        randIndex -= value</div><div class="line">        <span class="keyword">if</span> randIndex &lt;= 0:</div><div class="line">            <span class="built_in">return</span> word</div><div class="line"></div><div class="line">def buildWordDict(text):</div><div class="line">    <span class="comment"># 剔除换行符和引号</span></div><div class="line">    text = text.replace(<span class="string">"\n"</span>, <span class="string">" "</span>)</div><div class="line">    text = text.replace(<span class="string">"\""</span>, <span class="string">""</span>)</div><div class="line">    <span class="comment"># 保证每个标点符号都和前面的单词在一起</span></div><div class="line">    <span class="comment"># 这样不会被剔除，保留在马尔可夫链中</span></div><div class="line">    punctuation = [<span class="string">','</span>, <span class="string">'.'</span>, <span class="string">';'</span>,<span class="string">':'</span>]</div><div class="line">    <span class="keyword">for</span> symbol <span class="keyword">in</span> punctuation:</div><div class="line">        text = text.replace(symbol, <span class="string">" "</span>+symbol+<span class="string">" "</span>)</div><div class="line">    words = text.split(<span class="string">" "</span>)</div><div class="line">    <span class="comment"># 过滤空单词</span></div><div class="line">    words = [word <span class="keyword">for</span> word <span class="keyword">in</span> words <span class="keyword">if</span> word != <span class="string">""</span>]</div><div class="line"></div><div class="line">    wordDict = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(1, len(words)):</div><div class="line">        <span class="keyword">if</span> words[i-1] not <span class="keyword">in</span> wordDict: <span class="comment"># 为单词新建一个词典</span></div><div class="line">            wordDict[words[i-1]] = &#123;&#125;</div><div class="line">        <span class="keyword">if</span> words[i] not <span class="keyword">in</span> wordDict[words[i-1]]:</div><div class="line">            wordDict[words[i-1]][words[i]] = 0</div><div class="line">        wordDict[words[i-1]][words[i]] = wordDict[words[i-1]][words[i]] + 1</div><div class="line">    <span class="built_in">return</span> wordDict</div><div class="line"></div><div class="line">text = str(urlopen(<span class="string">"http://pythonscraping.com/files/inaugurationSpeech.txt"</span>)</div><div class="line">           .<span class="built_in">read</span>(), <span class="string">'utf-8'</span>)</div><div class="line">wordDict = buildWordDict(text)</div><div class="line">length = 100</div><div class="line">chain = <span class="string">""</span></div><div class="line">currentWord = <span class="string">"I"</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(0, length):</div><div class="line">    chain += currentWord+<span class="string">" "</span></div><div class="line">    currentWord = retrieveRandomWord(wordDict[currentWord])</div><div class="line"><span class="built_in">print</span>(chain)</div><div class="line">``` </div><div class="line">上述代码会随机生成一段100个单词的马尔可夫链，至于句子的含义，就是胡言乱语</div><div class="line"><span class="comment">## 维基百科六度分割：终结篇</span></div><div class="line">在寻找有向图的最短路径问题中，即找出维基百科中凯文 ·贝肯词条和其他词条之间最短链接路径的方法中，效果 最好且最常用的一种方法是广度优先搜索(breadth-first search)。</div><div class="line">``` bash </div><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import pymysql</div><div class="line"></div><div class="line">conn = pymysql.connect(host=<span class="string">'127.0.0.1'</span>, unix_socket=<span class="string">'/tmp/mysql.sock'</span>,</div><div class="line">                       user=<span class="string">'root'</span>, passwd=<span class="string">'wyt629szk'</span>, db=<span class="string">'mysql'</span>, charset=<span class="string">'utf8'</span>)</div><div class="line">cur = conn.cursor()</div><div class="line">cur.execute(<span class="string">"USE wikipedia"</span>)</div><div class="line"></div><div class="line">class SolutionFound(RuntimeError):</div><div class="line">    def __init__(self, message):</div><div class="line">        self.message = message</div><div class="line"></div><div class="line">def getLinks(fromPageId):</div><div class="line">    cur.execute(<span class="string">"SELECT toPageId FROM links WHERE fromPageId = %s"</span>, (fromPageId))</div><div class="line">    <span class="keyword">if</span> cur.rowcount == 0:</div><div class="line">        <span class="built_in">return</span> None</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="built_in">return</span> [x[0] <span class="keyword">for</span> x <span class="keyword">in</span> cur.fetchall()]</div><div class="line"></div><div class="line">def constructDict(currentPageId):</div><div class="line">    links = getLinks(currentPageId)</div><div class="line">    <span class="keyword">if</span> links:</div><div class="line">        <span class="built_in">return</span> dict(zip(links,[&#123;&#125;]*len(links)))</div><div class="line">    <span class="built_in">return</span> &#123;&#125;</div><div class="line"></div><div class="line"><span class="comment"># 链接树要么为空，要么包含多个链接</span></div><div class="line">def searchDepth(targetPageId, currentPageId, linkTree, depth):</div><div class="line">    <span class="keyword">if</span> depth == 0:</div><div class="line">        <span class="comment">#停止递归，返回结果</span></div><div class="line">        <span class="built_in">return</span> linkTree</div><div class="line">    <span class="keyword">if</span> not linkTree:</div><div class="line">        <span class="comment">#如果函数获取的链接字典是空的，就对当前页面的链接进行搜索。如果当前页面也没链</span></div><div class="line">        <span class="comment"># 接，就返回空链接字典。</span></div><div class="line">        linkTree = constructDict(currentPageId)</div><div class="line">        <span class="keyword">if</span> not linkTree:</div><div class="line">            <span class="comment">#若此节点无连接，则跳过此节点</span></div><div class="line">            <span class="built_in">return</span> &#123;&#125;</div><div class="line">    <span class="keyword">if</span> targetPageId <span class="keyword">in</span> linkTree.keys():</div><div class="line">        <span class="built_in">print</span>(<span class="string">"TARGET"</span> + str(targetPageId) + <span class="string">" FOUND!"</span>)</div><div class="line">        raise SolutionFound(<span class="string">"PAGE: "</span>+ str(currentPageId))</div><div class="line"></div><div class="line">    <span class="keyword">for</span> branchKey, branchValue <span class="keyword">in</span> linkTree.items():</div><div class="line">        try:</div><div class="line">            <span class="comment">#递归建立链接树</span></div><div class="line">            linkTree[branchKey] = searchDepth(targetPageId, branchKey, branchValue,</div><div class="line">                                              depth-1)</div><div class="line">        except SolutionFound as e:</div><div class="line">            <span class="built_in">print</span>(e.message)</div><div class="line">            raise  SolutionFound(<span class="string">"PAGE: "</span>+str(currentPageId))</div><div class="line">    <span class="built_in">return</span> linkTree</div><div class="line">try:</div><div class="line">    searchDepth(134951, 1, &#123;&#125;, 4)</div><div class="line">    <span class="built_in">print</span>(<span class="string">"No solution found"</span>)</div><div class="line">except SolutionFound as e:</div><div class="line">    <span class="built_in">print</span>(e.message)</div></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><p>由于数据库中的数据被我删除了，所以没有测试（^~^）</p><h2 id="自然语言工具包（NLTK）"><a href="#自然语言工具包（NLTK）" class="headerlink" title="自然语言工具包（NLTK）"></a>自然语言工具包（NLTK）</h2><p>一个python库，用于识别和标记英语文本中各个词的词性（<a href="http://www.nltk.org/install.html）" target="_blank" rel="external">http://www.nltk.org/install.html）</a></p><blockquote><blockquote><blockquote><p>import nltk<br>nltk.download()<br>两行命令会打开NLTK的下载器</p></blockquote></blockquote></blockquote><h3 id="用NLTK做统计分析"><a href="#用NLTK做统计分析" class="headerlink" title="用NLTK做统计分析"></a>用NLTK做统计分析</h3><p>文字的单词数量，单词频率和单词词性<br>一般从Text对象开始<br>from nltk import word_tokenize<br>from nltk import Text</p><p>tokens = word_tokenize(“Here is some not very interesting text”)<br>text = Text(tokens)</p><p>NLTK库里面已经内置了几本书，可以用import函数导入：<br>from nltk.book import *</p><blockquote><blockquote><blockquote><p>len(text6)/len(words)</p></blockquote></blockquote></blockquote><p>你还可以将文本对象放到一个频率分布对象FreqDist中，查看哪些单词是最常用的，以及单词的频率是多少</p><blockquote><blockquote><blockquote><p>from nltk import FreqDist<br>fdist = FreqDist(text6)<br>fdist.most_common(10)<br>[(‘:’, 1197), (‘.’, 816), (‘!’, 801), (‘,’, 731), (“‘“, 421), (‘[‘, 3 19), (‘]’, 312), (‘the’, 299), (‘I’, 255), (‘ARTHUR’, 225)]<br>fdist[“Grail”]<br>34</p></blockquote></blockquote></blockquote><p>你可以用NLTK非常轻松的创建并搜索一个2-gram模型（还有一个trigrams 即：3-grams）：</p><blockquote><blockquote><blockquote><p>from nltk import bigrams<br>bigrams = bigrams(text6)<br>bigramsDist = FreqDist(bigrams) &gt;&gt;&gt; bigramsDist[(“Sir”, “Robin”)]<br>18</p></blockquote></blockquote></blockquote><p>对于更一般的情形，你还可以导入ngrams模块：</p><blockquote><blockquote><blockquote><p>from nltk import ngrams<br>fourgrams = ngrams(text6, 4)<br>fourgramsDist = FreqDist(fourgrams)<br>fourgramsDist[(“father”, “smelt”, “of”, “elderberries”)]<br>1</p></blockquote></blockquote></blockquote><p>频率分布，文本对象和n-gram还可以整合在一个循环中进行迭代（下面程序就是打印文本中所以以”coconut”）<br>from nltk.book import *<br>from nltk import ngrams<br>fourgrams = ngrams(text6, 4) for fourgram in fourgrams:<br>    if fourgram[0] == “coconut”:<br>        print(fourgram)</p><h3 id="用NLTK做词性分析"><a href="#用NLTK做词性分析" class="headerlink" title="用NLTK做词性分析"></a>用NLTK做词性分析</h3><p>考虑同一个词在不同的语境中可能会导致意思混乱<br>ex:”He was objective in achieving his objective of writing an objective philosophy, primarily using verbs in the objective case”<br>爬虫会认为（objective）被用了四次，进而简单地忽略这四个单词各自不同的含义</p><p>还有要分析普通英文单词组成的公司名称，或者分析某个人对一个公司的评价，像<br>ACME Products is good”和“ACME Products is not bad”意思是一样的</p><p>Penn Treebank语意标记</p><p>除了度量语言，NLTK还可以用它的超级大字典分析文本内容，帮助人们寻找单词的含义。<br>NLTK的一个基本功能就是识别句子中各个词性</p><blockquote><blockquote><blockquote><p>from nltk.book import *<br>from nltk import word_tokenize<br>text = word_tokenize(“Strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.”)<br>from nltk import pos_tag<br>pos_tag(text)<br>[(‘Strange’, ‘NNP’), (‘women’, ‘NNS’), (‘lying’, ‘VBG’), (‘in’, ‘IN’)<br>, (‘ponds’, ‘NNS’), (‘distributing’, ‘VBG’), (‘swords’, ‘NNS’), (‘is’<br>, ‘VBZ’), (‘no’, ‘DT’), (‘basis’, ‘NN’), (‘for’, ‘IN’), (‘a’, ‘DT’),<br>(‘system’, ‘NN’), (‘of’, ‘IN’), (‘government’, ‘NN’), (‘.’, ‘.’),<br>(‘Supreme’, ‘NNP’), (‘executive’, ‘NN’), (‘power’, ‘NN’), (‘derives’, ‘NNS’),<br>(‘from’, ‘IN’), (‘a’, ‘DT’), (‘mandate’, ‘NN’), (‘from’, ‘IN’),<br>(‘the’, ‘DT’), (‘masses’, ‘NNS’), (‘,’, ‘,’), (‘not’, ‘RB’), (‘from’, ‘IN’),<br> (‘some’, ‘DT’), (‘farcical’, ‘JJ’), (‘aquatic’, ‘JJ’), (‘ceremony’, ‘NN’), (‘.’, ‘.’)]</p></blockquote></blockquote></blockquote><p>但是要正确的完成任务其实很复杂，用下面的例子看更直观</p><blockquote><blockquote><blockquote><p>text = word_tokenize(“The dust was thick so he had to dust”)<br>pos_tag(text)<br>[(‘The’, ‘DT’), (‘dust’, ‘NN’), (‘was’, ‘VBD’), (‘thick’, ‘JJ’), (‘so’, ‘RB’),<br>(‘he’, ‘PRP’), (‘had’, ‘VBD’), (‘to’, ‘TO’), (‘dust’, ‘VB’)]</p></blockquote></blockquote></blockquote><p>需要注意的是dust出现了两次，一个是名称，另外一个是动词（NLTK用英语的上下文无关法识别词性）</p><p>注：机器学习和机器训练<br>你也可以对NLTK进行训练，创建一个全新的上下文无关文法规则，比如，一种外语 的上下文无关文法规则。如果你用 Penn Treebank 词性标记手工完成了那种语言的大部 分文本的语义标记，那么你就可以把结果传给NLTK，然后训练它对其他未标记的文 本进行语义标记</p><h4 id="示例（找出google作为名称而不是动词的句子）"><a href="#示例（找出google作为名称而不是动词的句子）" class="headerlink" title="示例（找出google作为名称而不是动词的句子）"></a>示例（找出google作为名称而不是动词的句子）</h4><p>``` bash<br>from nltk import word_tokenize, sent_tokenize, pos_tag<br>sentences = sent_tokenize(“Google is one of the best companies in the world. I constantly google myself to see what I’m up to.”)<br>nouns = [‘NN’, ‘NNS’, ‘NNP’, ‘NNPS’]<br>for sentence in sentences:<br>    if “google” in sentence.lower():<br>        taggedWords = pos_tag(word_tokenize(sentence))<br>            for word in taggleWords:<br>                if word[0].lower() == “google” and word[1] in nouns:                         print(sentence)</p><h3 id="其他资源"><a href="#其他资源" class="headerlink" title="其他资源"></a>其他资源</h3><p>Natural Language Processing with Python(http:// shop.oreilly.com/product/9780596516499.do)<br>Natural Language Annotation for Machine Learning(<a href="http://shop.oreilly.com/product/0636920020578.do" target="_blank" rel="external">http://shop.oreilly.com/product/0636920020578.do</a>)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;自然语言的处理&quot;&gt;&lt;a href=&quot;#自然语言的处理&quot; class=&quot;headerlink&quot; title=&quot;自然语言的处理&quot;&gt;&lt;/a&gt;自然语言的处理&lt;/h1&gt;&lt;h2 id=&quot;概括数据&quot;&gt;&lt;a href=&quot;#概括数据&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理</title>
    <link href="http://yoursite.com/2017/10/04/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2017/10/04/自然语言处理/</id>
    <published>2017-10-04T03:07:53.000Z</published>
    <updated>2017-10-04T03:07:53.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python数据收集（数据清洗）</title>
    <link href="http://yoursite.com/2017/10/03/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/03/python数据收集（数据清洗）/</id>
    <published>2017-10-03T12:51:22.000Z</published>
    <updated>2017-10-04T02:52:09.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><h2 id="编写代码清洗数据"><a href="#编写代码清洗数据" class="headerlink" title="编写代码清洗数据"></a>编写代码清洗数据</h2><p>n-gram<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line"></div><div class="line"><span class="comment">#清洗规则函数</span></div><div class="line"></div><div class="line"><span class="comment">#剔除单字符的“单词”，除非这个字符是“i”或“a”;</span></div><div class="line"><span class="comment">#剔除维基百科的引用标记(方括号包裹的数字，如[1]);</span></div><div class="line"><span class="comment">#剔除标点符号(注意:这个规则有点儿矫枉过正，在第9章我们将详细介绍，本例暂时这样处理)。</span></div><div class="line">import string</div><div class="line"></div><div class="line">def cleanInput(input):</div><div class="line">    input = re.sub(<span class="string">'\n+'</span>,<span class="string">' '</span>, input)</div><div class="line">    input = re.sub(<span class="string">'\[[0-9]*\]'</span>,<span class="string">""</span>, input)</div><div class="line">    input = re.sub(<span class="string">' +'</span>,<span class="string">" "</span>, input)</div><div class="line">    input = bytes(input,<span class="string">"UTF-8"</span>)</div><div class="line">    input = input.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>)</div><div class="line">    cleanInput = []</div><div class="line">    input = input.split(<span class="string">' '</span>)</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> input:</div><div class="line">        item = item.strip(string.punctuation)</div><div class="line">        <span class="keyword">if</span> len(item) &gt; 1 or (item.lower() == <span class="string">'i'</span>):</div><div class="line">            cleanInput.append(item)</div><div class="line">    <span class="built_in">return</span> cleanInput</div><div class="line"></div><div class="line">def ngrams(input, n):</div><div class="line">    input = input.upper()</div><div class="line">    input = cleanInput(input)</div><div class="line">    output = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input)-n+1):</div><div class="line">        output.append(input[i:i+n])</div><div class="line">    <span class="built_in">return</span> output</div><div class="line">html = urlopen(<span class="string">"http://en.wikipedia.org/wiki/Python_(programming_language)"</span>)</div><div class="line">bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)</div><div class="line">content = bsObj.find(<span class="string">'div'</span>,&#123;<span class="string">'id'</span>:<span class="string">'mw-content-text'</span>&#125;).get_text()</div><div class="line">ngrams = ngrams(content, 2)</div><div class="line"><span class="built_in">print</span>(ngrams)</div><div class="line"><span class="built_in">print</span>(<span class="string">"2-grams count is: "</span>+str(len(ngrams)))</div></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><p>import string<br>print(string.punctuation)<br>!”#$%&amp;’()*+,-./:;&lt;=&gt;?@[]^_`{|}~</p><h3 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h3><p>python的字典是无序的，不能想数组一样直接对n-gran序列频率进行排序，字典内部的元素位置排序以后再次使用时还是<br>会变化，在python的collections库里面有一个OrderedDict可以解决这个问题</p><pre><code class="bash">from collection import OrderedDict……ngrams = OrderedDict(sorted(ngrams.items(), key=lambda t: t[1), reverse=True)</code></pre><p>没有跑起来，网速不好（下次测试）</p></blockquote></blockquote></blockquote><h2 id="数据存储后在清洗"><a href="#数据存储后在清洗" class="headerlink" title="数据存储后在清洗"></a>数据存储后在清洗</h2><p>OpenRefine</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据清洗&quot;&gt;&lt;a href=&quot;#数据清洗&quot; class=&quot;headerlink&quot; title=&quot;数据清洗&quot;&gt;&lt;/a&gt;数据清洗&lt;/h1&gt;&lt;h2 id=&quot;编写代码清洗数据&quot;&gt;&lt;a href=&quot;#编写代码清洗数据&quot; class=&quot;headerlink&quot; title=&quot;编
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（六）</title>
    <link href="http://yoursite.com/2017/10/03/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/03/python数据收集（六）/</id>
    <published>2017-10-03T07:36:23.000Z</published>
    <updated>2017-10-03T12:49:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="读取文档"><a href="#读取文档" class="headerlink" title="读取文档"></a>读取文档</h1><h2 id="文档编码"><a href="#文档编码" class="headerlink" title="文档编码"></a>文档编码</h2><p>纯文本文件，视频文件和图像文件的唯一区别，就是它们的0和1面向用户的转换方式不同</p><h3 id="纯文本"><a href="#纯文本" class="headerlink" title="纯文本"></a>纯文本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">textPage = urlopen(<span class="string">"http://www.pythonscraping.com/pages/warandpeace/chapter1.txt"</span>)</div><div class="line"><span class="built_in">print</span>(textPage.read())</div></pre></td></tr></table></figure><p>像这种纯文本的，使用BeautifulSoup库就没有用了，如果变成BeautifulSoup反而适得其反</p><h2 id="文本编码和全球互联网"><a href="#文本编码和全球互联网" class="headerlink" title="文本编码和全球互联网"></a>文本编码和全球互联网</h2><p>UTF-8:<br>在 UTF-8 设计过程中，设计师决定利用 ASCII 文档里的“填充位”，让所有以“0”开头的 字节表示这个字符只用 1 个字节，从而把 ASCII 和 UTF-8 编码完美地结合在一起。因此， 下面的字符在 ASCII 和 UTF-8 两种编码方式中都是有效的:<br>      01000001 - A<br>      01000010 - B<br>      01000011 - C<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># from urllib.request import urlopen</div><div class="line"># textPage = urlopen(&quot;http://www.pythonscraping.com/pages/warandpeace/chapter1.txt&quot;)</div><div class="line"># print(textPage.read())</div><div class="line"></div><div class="line"></div><div class="line"># from urllib.request import urlopen</div><div class="line"># textPage = urlopen(&quot;http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt&quot;)</div><div class="line"># print(str(textPage.read(),&apos;utf-8&apos;))</div><div class="line"></div><div class="line">#用BeautifulSoup和python3.x对文档进行UTF-8编码，如下所示</div><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">html = urlopen(&quot;http://en.wikipedia.org/wiki/Python_(programming_language)&quot;)</div><div class="line">bsObj = BeautifulSoup(html)</div><div class="line">content = bsObj.find(&quot;div&quot;, &#123;&quot;id&quot;:&quot;mw-content-text&quot;&#125;).get_text()</div><div class="line">content = bytes(content, &quot;UTF-8&quot;)</div><div class="line">content = content.decode(&quot;UTF-8&quot;)</div></pre></td></tr></table></figure></p><h2 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h2><h3 id="读取CSV文件"><a href="#读取CSV文件" class="headerlink" title="读取CSV文件"></a>读取CSV文件</h3><p>• 手动把CSV文件下载到本机，然后用Python定位文件位置;<br>• 写Python程序下载文件，读取之后再把源文件删除;<br>• 从网上直接把文件读成一个字符串，然后转换成一个StringIO对象，使它具有文件的<br>属性。（这个方法比较可行）</p><h2 id="PDF"><a href="#PDF" class="headerlink" title="PDF"></a>PDF</h2><p>pdf转字符串(直接上代码)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#把PDF 读成字符串，然后用 StringIO 转换成文件对象</span></div><div class="line">from urllib.request import urlopen</div><div class="line">from pdfminer.pdfinterp import PDFResourceManager, process_pdf</div><div class="line">from pdfminer.converter import TextConverter</div><div class="line">from pdfminer.layout import LAParams</div><div class="line">from io import StringIO</div><div class="line">from io import open</div><div class="line"></div><div class="line">def readPDF(pdfFile):</div><div class="line">    rsrcmgr = PDFResourceManager()</div><div class="line">    retstr = StringIO()</div><div class="line">    laparams = LAParams()</div><div class="line">    device = TextConverter(rsrcmgr, retstr, laparams=laparams)</div><div class="line">    process_pdf(rsrcmgr, device, pdfFile)</div><div class="line">    device.close()</div><div class="line">    content = retstr.getvalue()</div><div class="line">    retstr.close()</div><div class="line">    <span class="built_in">return</span> content</div><div class="line">pdfFile = urlopen(<span class="string">"http://pythonscraping.com/pages/warandpeace/chapter1.pdf"</span>)</div><div class="line">outputString = readPDF(pdfFile)</div><div class="line"><span class="built_in">print</span>(outputString)</div><div class="line">pdfFile.close()</div><div class="line">``` </div><div class="line"><span class="comment">#如果格式里面有图片，各式各样的文本格式，或者带有表格和数据图的时候，输出结果可能不是很完美</span></div><div class="line"></div><div class="line"><span class="comment">## 微软Word和.docx</span></div><div class="line"><span class="comment">### 读取Microsoft Office 文件</span></div><div class="line">第一步是从文件读取XML</div><div class="line">``` bash </div><div class="line">from zipfile import ZipFile</div><div class="line">from urllib.request import urlopen</div><div class="line">from io import BytesIO</div><div class="line">from bs4 import BeautifulSoup</div><div class="line"></div><div class="line">wordFile = urlopen(<span class="string">"http://pythonscraping.com/pages/AWordDocument.docx"</span>).<span class="built_in">read</span>()</div><div class="line">wordFile = BytesIO(wordFile)</div><div class="line">document = ZipFile(wordFile)</div><div class="line">xml_content = document.read(<span class="string">'word/document.xml'</span>)</div><div class="line"></div><div class="line">wordObj = BeautifulSoup(xml_content.decode(<span class="string">'utf-8'</span>),<span class="string">'html.parser'</span>)</div><div class="line"><span class="comment"># textStrings = wordObj.findAll("w:t")</span></div><div class="line"><span class="comment"># for textElem in textStrings:</span></div><div class="line"><span class="comment">#     print(textElem.text)</span></div><div class="line"><span class="comment">#print(wordObj.text)</span></div><div class="line">textStrings = wordObj.findAll(<span class="string">"w:t"</span>)</div><div class="line"><span class="keyword">for</span> textElem <span class="keyword">in</span> textStrings:</div><div class="line">    closeTag = <span class="string">""</span></div><div class="line">    try:</div><div class="line">        style = textElem.parent.previousSibling.find(<span class="string">"w:pstyle"</span>)</div><div class="line">        <span class="keyword">if</span> style is not None and style[<span class="string">"w:val"</span>] == <span class="string">"Title"</span>: <span class="built_in">print</span>(<span class="string">"&lt;h1&gt;"</span>)</div><div class="line">        closeTag = <span class="string">"&lt;/h1&gt;"</span></div><div class="line">    except AttributeError: <span class="comment">#不打印标签</span></div><div class="line">        pass</div><div class="line">        <span class="built_in">print</span>(textElem.text)</div><div class="line">        <span class="built_in">print</span>(closeTag)</div></pre></td></tr></table></figure></p><p>由于按照书上的做法是得到的是一个空的textStrings,但是可用wordObj.text属性找出，但是格式不太对</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;读取文档&quot;&gt;&lt;a href=&quot;#读取文档&quot; class=&quot;headerlink&quot; title=&quot;读取文档&quot;&gt;&lt;/a&gt;读取文档&lt;/h1&gt;&lt;h2 id=&quot;文档编码&quot;&gt;&lt;a href=&quot;#文档编码&quot; class=&quot;headerlink&quot; title=&quot;文档编码&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>读取文档</title>
    <link href="http://yoursite.com/2017/10/03/%E8%AF%BB%E5%8F%96%E6%96%87%E6%A1%A3/"/>
    <id>http://yoursite.com/2017/10/03/读取文档/</id>
    <published>2017-10-03T07:35:49.000Z</published>
    <updated>2017-10-03T07:35:49.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python数据收集（五）</title>
    <link href="http://yoursite.com/2017/10/02/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/02/python数据收集（五）/</id>
    <published>2017-10-02T07:56:58.000Z</published>
    <updated>2017-10-03T07:34:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="存储数据"><a href="#存储数据" class="headerlink" title="存储数据"></a>存储数据</h1><h2 id="媒体文件"><a href="#媒体文件" class="headerlink" title="媒体文件"></a>媒体文件</h2><h3 id="盗链"><a href="#盗链" class="headerlink" title="盗链"></a>盗链</h3><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><h4 id="下载单个图片"><a href="#下载单个图片" class="headerlink" title="下载单个图片"></a>下载单个图片</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlretrieve </div><div class="line">from urllib.request import urlopen </div><div class="line">from bs4 import BeautifulSouphtml = urlopen(<span class="string">"http://www.pythonscraping.com"</span>)bsObj = BeautifulSoup(html)imageLocation = bsObj.find(<span class="string">"a"</span>, &#123;<span class="string">"id"</span>: <span class="string">"logo"</span>&#125;).find(<span class="string">"img"</span>)[<span class="string">"src"</span>]urlretrieve (imageLocation, <span class="string">"logo.jpg"</span>)</div><div class="line">注：</div><div class="line">open newline可选参数：None，’’，\n，\r，\r\n</div></pre></td></tr></table></figure><h4 id="下载src下的所有资源（该页面）"><a href="#下载src下的所有资源（该页面）" class="headerlink" title="下载src下的所有资源（该页面）"></a>下载src下的所有资源（该页面）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#将 http://pythonscraping. com 主页上的所有src属性的文件下载下来</span></div><div class="line">import os</div><div class="line">from urllib.request import urlretrieve</div><div class="line">from urllib.request import urlopen</div><div class="line">from  bs4 import BeautifulSoup</div><div class="line"></div><div class="line">downloadDirectory = <span class="string">"downloaded"</span></div><div class="line">baseUrl = <span class="string">"http://pythonscraping.com"</span></div><div class="line"></div><div class="line">def getAbsoluteURL(baseUrl, <span class="built_in">source</span>):</div><div class="line">    <span class="keyword">if</span> source.startswith(<span class="string">"http://www."</span>):</div><div class="line">        url = <span class="string">"http://"</span>+<span class="built_in">source</span>[11:]</div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"http://"</span>):</div><div class="line">        url = <span class="built_in">source</span></div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"www."</span>):</div><div class="line">        url = <span class="built_in">source</span>[4:]</div><div class="line">        url = <span class="string">"http://"</span> + <span class="built_in">source</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        url = baseUrl+<span class="string">"/"</span>+<span class="built_in">source</span></div><div class="line">    <span class="keyword">if</span> baseUrl not <span class="keyword">in</span> url:</div><div class="line">        <span class="built_in">return</span> None</div><div class="line">    <span class="built_in">return</span> url</div><div class="line"></div><div class="line">def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):</div><div class="line">    path = absoluteUrl.replace(<span class="string">"www."</span>,<span class="string">""</span>)</div><div class="line">    path = path.replace(baseUrl,<span class="string">""</span>)</div><div class="line">    path = downloadDirectory + path</div><div class="line">    directory = os.path.dirname(path)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> not os.path.exists(directory):</div><div class="line">        os.makedirs(directory)</div><div class="line">    <span class="built_in">return</span> path</div><div class="line"></div><div class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com"</span>)</div><div class="line">bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)</div><div class="line">downloadList = bsObj.findAll(src = True)</div><div class="line"></div><div class="line"><span class="keyword">for</span> download <span class="keyword">in</span> downloadList:</div><div class="line">    fileUrl = getAbsoluteURL(baseUrl, download[<span class="string">"src"</span>])</div><div class="line">    <span class="keyword">if</span> fileUrl is not None:</div><div class="line">        <span class="built_in">print</span>(fileUrl)</div><div class="line">        urlretrieve(fileUrl,getDownloadPath(baseUrl, fileUrl, downloadDirectory))</div></pre></td></tr></table></figure><h4 id="保存为CSV格式"><a href="#保存为CSV格式" class="headerlink" title="保存为CSV格式"></a>保存为CSV格式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#将 http://pythonscraping. com 主页上的所有src属性的文件下载下来</span></div><div class="line">import os</div><div class="line">from urllib.request import urlretrieve</div><div class="line">from urllib.request import urlopen</div><div class="line">from  bs4 import BeautifulSoup</div><div class="line"></div><div class="line">downloadDirectory = <span class="string">"downloaded"</span></div><div class="line">baseUrl = <span class="string">"http://pythonscraping.com"</span></div><div class="line"></div><div class="line">def getAbsoluteURL(baseUrl, <span class="built_in">source</span>):</div><div class="line">    <span class="keyword">if</span> source.startswith(<span class="string">"http://www."</span>):</div><div class="line">        url = <span class="string">"http://"</span>+<span class="built_in">source</span>[11:]</div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"http://"</span>):</div><div class="line">        url = <span class="built_in">source</span></div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"www."</span>):</div><div class="line">        url = <span class="built_in">source</span>[4:]</div><div class="line">        url = <span class="string">"http://"</span> + <span class="built_in">source</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        url = baseUrl+<span class="string">"/"</span>+<span class="built_in">source</span></div><div class="line">    <span class="keyword">if</span> baseUrl not <span class="keyword">in</span> url:</div><div class="line">        <span class="built_in">return</span> None</div><div class="line">    <span class="built_in">return</span> url</div><div class="line"></div><div class="line">def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):</div><div class="line">    path = absoluteUrl.replace(<span class="string">"www."</span>,<span class="string">""</span>)</div><div class="line">    path = path.replace(baseUrl,<span class="string">""</span>)</div><div class="line">    path = downloadDirectory + path</div><div class="line">    directory = os.path.dirname(path)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> not os.path.exists(directory):</div><div class="line">        os.makedirs(directory)</div><div class="line">    <span class="built_in">return</span> path</div><div class="line"></div><div class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com"</span>)</div><div class="line">bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)</div><div class="line">downloadList = bsObj.findAll(src = True)</div><div class="line"></div><div class="line"><span class="keyword">for</span> download <span class="keyword">in</span> downloadList:</div><div class="line">    fileUrl = getAbsoluteURL(baseUrl, download[<span class="string">"src"</span>])</div><div class="line">    <span class="keyword">if</span> fileUrl is not None:</div><div class="line">        <span class="built_in">print</span>(fileUrl)</div><div class="line">        urlretrieve(fileUrl,getDownloadPath(baseUrl, fileUrl, downloadDirectory))</div></pre></td></tr></table></figure><h2 id="MySql"><a href="#MySql" class="headerlink" title="MySql"></a>MySql</h2><p>之前安装好了mysql，但是今天从终端连接的时候 mysql -u root -p居然报错了（心中有一万个草泥马飞过）<br>俺是文明人，错误代码忘记截图了，忽略以上信息<br>du -sh *<br>lsof -i:3306<br>ps -A|grep mysql<br>查看MySQL的默认日志文件的位置<br>show variables like ‘general_log_file’;</p><p>#我的默认位置<br>/usr/local/mysql-5.7.19-macos10.12-x86_64/data/<strong>*</strong>MacBook-Air.log</p><h3 id="与python整合"><a href="#与python整合" class="headerlink" title="与python整合"></a>与python整合</h3><p>开源库 PyMySQL<br>OK 默认你的数据库中已经有一张pages的表<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import pymysql</div><div class="line">conn = pymysql.connect(host=<span class="string">'127.0.0.1'</span>, unix_socket=<span class="string">'/tmp/mysql.sock'</span>,</div><div class="line">                       user=<span class="string">'root'</span>,passwd=‘******’, db=<span class="string">'mysql'</span>)</div><div class="line">cur = conn.cursor()</div><div class="line">cur.execute(<span class="string">"USE scraping"</span>)</div><div class="line"></div><div class="line">cur.execute(<span class="string">"SELECT * FROM pages WHERE id = 1"</span>)</div><div class="line"><span class="built_in">print</span>(cur.fetchone())</div><div class="line">cur.close()</div><div class="line">conn.close()</div></pre></td></tr></table></figure></p><p>测试数据库是否能正常连接，以及从数据库中读取数据</p><h4 id="连接-光标模式"><a href="#连接-光标模式" class="headerlink" title="连接/光标模式"></a>连接/光标模式</h4><p>连接模式除了要链接数据库外，还要发送数据库信息，处理回滚操作，创建新的光标对象等等<br>注意：用完光标和链接后，如果不关闭就会导致连接泄露，造成一种为未关闭连接的现象，这种现象会一直想好数据库资源<br>所以用完数据库之后记得关闭连接</p><h4 id="设置数据库字符"><a href="#设置数据库字符" class="headerlink" title="设置数据库字符"></a>设置数据库字符</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ALTER DATABASE scraping CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci; </div><div class="line">ALTER TABLE pages CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; </div><div class="line">ALTER TABLE pages CHANGE title title VARCHAR(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;ALTER TABLE pages CHANGE content content VARCHAR(10000) CHARACTER SET utf8mb4 CO LLATE utf8mb4_unicode_ci;</div></pre></td></tr></table></figure><p>虽然不晓得utf8mb4与utf8mb4_unicode_ci有什么区别，但是还是设置了（好像这是一种东西，使用COLLATE不同）<br>不过要比utf-8要多一个位元</p><h4 id="将网页上爬取的信息插入到数据库中"><a href="#将网页上爬取的信息插入到数据库中" class="headerlink" title="将网页上爬取的信息插入到数据库中"></a>将网页上爬取的信息插入到数据库中</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import datetime</div><div class="line">import random</div><div class="line">import pymysql</div><div class="line"></div><div class="line">conn = pymysql.connect(host=<span class="string">'127.0.0.1'</span>, unix_socket=<span class="string">'/tmp/mysql.sock'</span>,</div><div class="line">                       user=<span class="string">'root'</span>,passwd=<span class="string">'wyt629szk'</span>, db=<span class="string">'mysql'</span>, charset=<span class="string">'utf-8'</span>)</div><div class="line">cur = conn.cursor()</div><div class="line">cur.execute(<span class="string">"USE scraping"</span>)</div><div class="line"></div><div class="line">random.seed(datetime.datetime.now())</div><div class="line"></div><div class="line">def store(title, content):</div><div class="line">    cur.execute(<span class="string">"INSERT INTO pages(title, content) VALUES (\"%s\",\"%s\"),(title. content)"</span>)</div><div class="line">    cur.connection.commit()</div><div class="line"></div><div class="line">def getLinks(articleUrl):</div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+articleUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    title = bsObj.find(<span class="string">"h1"</span>).get_text()</div><div class="line">    content = bsObj.find(<span class="string">"div"</span>,&#123;<span class="string">"id"</span>:<span class="string">"mw-content-text"</span>&#125;).find(<span class="string">"p"</span>).get_text()</div><div class="line">    store(title, content)</div><div class="line">    <span class="built_in">return</span> bsObj.find(<span class="string">"div"</span>,&#123;<span class="string">"id"</span>:<span class="string">"bodyContent"</span>&#125;).findAll(<span class="string">"a"</span>,href=re.compile(<span class="string">"^(/wiki/)((?!:).)*$"</span>))</div><div class="line"></div><div class="line">links = getLinks(<span class="string">"/wiki/Kevin_Bacon"</span>)</div><div class="line">try:</div><div class="line">    <span class="keyword">while</span> len(links) &gt; 0:</div><div class="line">        newArticle = links[random.randint(0, len(links)-1)].attrs[<span class="string">'href'</span>]</div><div class="line">        <span class="built_in">print</span>(newArticle)</div><div class="line">        links = getLinks(newArticle)</div><div class="line">finally:</div><div class="line">    cur.close()</div><div class="line">    conn.close()</div></pre></td></tr></table></figure><h3 id="数据库技术与最佳实践"><a href="#数据库技术与最佳实践" class="headerlink" title="数据库技术与最佳实践"></a>数据库技术与最佳实践</h3><h4 id="主动创建一个id字段"><a href="#主动创建一个id字段" class="headerlink" title="主动创建一个id字段"></a>主动创建一个id字段</h4><h4 id="用智能索引"><a href="#用智能索引" class="headerlink" title="用智能索引"></a>用智能索引</h4><p>额外的索引血药占用更多的空间，而且插入新行的时候也需要花费更多的时间<br>例如：如果你经常要查询一个字段<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;SELECT * FROM dictionary WHERE definition=<span class="string">"A small furry animal that says meow"</span>;</div><div class="line"><span class="comment">#你可以给definition建立一个该字段前16个字符的智能索引</span></div><div class="line">CREATE INDEX definition ON dictionary (id, definition(16));</div></pre></td></tr></table></figure></p><h4 id="数据查询时间和数据库空间的问题"><a href="#数据查询时间和数据库空间的问题" class="headerlink" title="数据查询时间和数据库空间的问题"></a>数据查询时间和数据库空间的问题</h4><p>拆表 -&gt; 可以去除冗余</p><h3 id="MySQL里的”六度空间游戏”"><a href="#MySQL里的”六度空间游戏”" class="headerlink" title="MySQL里的”六度空间游戏”"></a>MySQL里的”六度空间游戏”</h3><p>设计一个带有两张表的数据库来分别存储页面和链接，两张表都带有创建时间和独立的ID号，代码如下所示:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE `wikipedia`.`pages` (`id` INT NOT NULL AUTO_INCREMENT,`url` VARCHAR(255) NOT NULL,`created` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`));</div><div class="line"></div><div class="line">CREATE TABLE `wikipedia`.`links` ( `id` INT NOT NULL AUTO_INCREMENT, `fromPageId` INT NULL, `toPageId` INT NULL,`created` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`));</div></pre></td></tr></table></figure></p><p>注：因为页面标题要在你进入页面后读取内容才能抓到。那么如果我们想创建一个高效的爬虫来填充这些数据表，那么只存储页的<br>链接就可以保存词条页面了，甚至不需要访问词条页面（不是很懂）</p><h2 id="Email（代码没有跑通，Connection-refused，先把书上的源码贴上来）"><a href="#Email（代码没有跑通，Connection-refused，先把书上的源码贴上来）" class="headerlink" title="Email（代码没有跑通，Connection refused，先把书上的源码贴上来）"></a>Email（代码没有跑通，Connection refused，先把书上的源码贴上来）</h2><pre><code class="bash">import smtplibfrom email.mime.text import MIMETextfrom bs4 import BeautifulSoupfrom urllib.request import urlopenimport timedef sendMail(subject, body):    msg = MIMEText(body)    msg[<span class="string">'Subject'</span>] = subject    msg[<span class="string">'From'</span>] = <span class="string">"christmas_alerts@pythonscraping.com"</span>    msg[<span class="string">'To'</span>] = <span class="string">"ryan@pythonscraping.com"</span>    s = smtplib.SMTP(<span class="string">'localhost'</span>)    s.send_message(msg)    s.quit()bsObj = BeautifulSoup(urlopen(<span class="string">"https://isitchristmas.com/"</span>))<span class="keyword">while</span>(bsObj.find(<span class="string">"a"</span>, {<span class="string">"id"</span>:<span class="string">"answer"</span>}).attrs[<span class="string">'title'</span>] == <span class="string">"NO"</span>):    <span class="built_in">print</span>(<span class="string">"It is not Christmas yet."</span>)    time.sleep(3600)    bsObj = BeautifulSoup(urlopen(<span class="string">"https://isitchristmas.com/"</span>))    sendMail(<span class="string">"It's Christmas!"</span>,              <span class="string">"According to http://itischristmas.com, it is Christmas!"</span>)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;存储数据&quot;&gt;&lt;a href=&quot;#存储数据&quot; class=&quot;headerlink&quot; title=&quot;存储数据&quot;&gt;&lt;/a&gt;存储数据&lt;/h1&gt;&lt;h2 id=&quot;媒体文件&quot;&gt;&lt;a href=&quot;#媒体文件&quot; class=&quot;headerlink&quot; title=&quot;媒体文件&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（四）</title>
    <link href="http://yoursite.com/2017/10/02/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/02/python数据收集（四）/</id>
    <published>2017-10-02T03:03:10.000Z</published>
    <updated>2017-10-02T07:55:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用API"><a href="#使用API" class="headerlink" title="使用API"></a>使用API</h1><h2 id="API与普通网站的区别"><a href="#API与普通网站的区别" class="headerlink" title="API与普通网站的区别"></a>API与普通网站的区别</h2><p>1.API请求使用非常严谨的语法 2.其次API用JSON或XML格式表示数据，而不是HTML</p><h2 id="API的通用规则"><a href="#API的通用规则" class="headerlink" title="API的通用规则"></a>API的通用规则</h2><p>第一次使用API的时候，建议阅读文档</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>利用HTTP从网络服务获取信息的四种方式：<br>1.GET<br>2.POST<br>3.PUT<br>4.DELETE</p><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><h3 id="服务器响应"><a href="#服务器响应" class="headerlink" title="服务器响应"></a>服务器响应</h3><p>XML/JSON -&gt; json更小，在js框架中更好处理<br>API调用</p><h3 id="Echo-Nest"><a href="#Echo-Nest" class="headerlink" title="Echo Nest"></a>Echo Nest</h3><h3 id="解析JSON数据"><a href="#解析JSON数据" class="headerlink" title="解析JSON数据"></a>解析JSON数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">import json</div><div class="line">from urllib.request import urlopen</div><div class="line"></div><div class="line">def getCountry(ipAddress):</div><div class="line">    response = urlopen(<span class="string">"http://freegeoip.net/json/"</span>+ipAddress).<span class="built_in">read</span>().decode(<span class="string">'utf-8'</span>)</div><div class="line">    responseJson = json.loads(response)</div><div class="line">    <span class="built_in">return</span> responseJson.get(<span class="string">'country_code'</span>)</div><div class="line"><span class="built_in">print</span>(getCountry(<span class="string">"50.78.253.58"</span>))</div><div class="line">下面的例子将json字符串处理成python可以处理的json对象</div><div class="line">import json</div><div class="line">jsonString = <span class="string">'&#123;"arrayOfNums":[&#123;"number":0&#125;,&#123;"number":1&#125;,&#123;"number":2&#125;],'</span> \</div><div class="line">             <span class="string">'"arrayOfFruits":[&#123;"fruit":"apple"&#125;,&#123;"fruit":"banana"&#125;,&#123;"fruit":"pear"&#125;]&#125;'</span></div><div class="line">jsonObj = json.loads(jsonString)</div><div class="line"><span class="built_in">print</span>(jsonObj.get(<span class="string">"arrayOfNums"</span>))</div><div class="line"><span class="built_in">print</span>(jsonObj.get(<span class="string">"arrayOfNums"</span>)[1])</div><div class="line"><span class="built_in">print</span>(jsonObj.get(<span class="string">"arrayOfNums"</span>)[1].get(<span class="string">"number"</span>)+</div><div class="line">jsonObj.get(<span class="string">"arrayOfNums"</span>)[2].get(<span class="string">"number"</span>))</div><div class="line"><span class="built_in">print</span>(jsonObj.get(<span class="string">"arrayOfFruits"</span>)[2].get(<span class="string">"fruit"</span>))</div><div class="line">可以着重注意一下load()函数</div></pre></td></tr></table></figure><h3 id="Python的集合类型简介"><a href="#Python的集合类型简介" class="headerlink" title="Python的集合类型简介"></a>Python的集合类型简介</h3><p>集合是无序的，set集合的好处是它存储的值不会重复<br>在未来可能需要扩展的代码，在决定使用集合还是列表时，有两件事情需要考虑：虽然列表迭代速度比集合稍微快一点，<br>但集合查找速度更快</p><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><pre><code class="bash">from urllib.request import urlopen,HTTPErrorfrom bs4 import BeautifulSoupimport datetimeimport randomimport reimport jsonrandom.seed(datetime.datetime.now())def getLinks(articleUrl):    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+articleUrl)    bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)    <span class="built_in">return</span> bsObj.find(<span class="string">"div"</span>,{<span class="string">"id"</span>:<span class="string">"bodyContent"</span>}).findAll(<span class="string">"a"</span>,                                                          href=re.compile(<span class="string">"^(/wiki/)((?!:).)*$"</span>))def getHistoryIPs(pageUrl):    <span class="comment"># 编辑历史页面URL链接格式是：</span>    <span class="comment"># http://en.wikipedia.org/w/index.php?title=Title_in_URL&amp;action=history</span>    pageUrl = pageUrl.replace(<span class="string">"/wiki/"</span>,<span class="string">""</span>)    historyUrl = <span class="string">"http://en.wikipedia.org/w/index.php?title="</span> +pageUrl+<span class="string">"&amp;action=history"</span>    <span class="built_in">print</span>(<span class="string">"history url is: "</span> + historyUrl)    html = urlopen(historyUrl)    bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)    <span class="comment"># 找出class属性是"mw-anonuserlink"的链接</span>    <span class="comment"># 它们用IP地址代替用户</span>    ipAddressse = bsObj.findAll(<span class="string">"a"</span>,{<span class="string">"class"</span>:<span class="string">"mw-anonuserlink"</span>})    addressList = <span class="built_in">set</span>()    <span class="keyword">for</span> ipAddress <span class="keyword">in</span> ipAddressse:        addressList.add(ipAddress.get_text())        <span class="built_in">return</span> addressListdef getCountry(ipAddress):    try:        response = urlopen(<span class="string">"http://freegeoip.net/json/"</span>+ipAddress)\            .<span class="built_in">read</span>().decode(<span class="string">'utf-8'</span>)    except HTTPError:        <span class="built_in">return</span> None    responseJson = json.loads(response)    <span class="built_in">return</span> responseJson.get(<span class="string">"country_code"</span>)links = getLinks(<span class="string">"/wiki/Python_(programming_language)"</span>)<span class="keyword">while</span>(len(links) &gt; 0):    <span class="keyword">for</span> link <span class="keyword">in</span> links:        <span class="built_in">print</span>(<span class="string">"------------------------"</span>)        historyIPs = getHistoryIPs(link.attrs[<span class="string">"href"</span>])        <span class="keyword">for</span> historyIP <span class="keyword">in</span> historyIPs:            country = getCountry(historyIP)            <span class="keyword">if</span> country is not None:                <span class="built_in">print</span>(historyIP+ <span class="string">" is from "</span> + country)    newLink = links[random.randint(0, len(links)-1)].attrs[<span class="string">'herf'</span>]    links = getLinks(newLink)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;使用API&quot;&gt;&lt;a href=&quot;#使用API&quot; class=&quot;headerlink&quot; title=&quot;使用API&quot;&gt;&lt;/a&gt;使用API&lt;/h1&gt;&lt;h2 id=&quot;API与普通网站的区别&quot;&gt;&lt;a href=&quot;#API与普通网站的区别&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（三）</title>
    <link href="http://yoursite.com/2017/09/30/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/30/python数据收集（三）/</id>
    <published>2017-09-30T12:10:29.000Z</published>
    <updated>2017-10-02T03:01:06.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始采集"><a href="#开始采集" class="headerlink" title="开始采集"></a>开始采集</h1><h2 id="遍历单个域名"><a href="#遍历单个域名" class="headerlink" title="遍历单个域名"></a>遍历单个域名</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import datetime</div><div class="line">import random</div><div class="line">import re</div><div class="line"></div><div class="line">random.seed(datetime.datetime.now())</div><div class="line">def getLinks(articleUrl):</div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+articleUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    <span class="built_in">return</span> bsObj.find(<span class="string">"div"</span>, &#123;<span class="string">"id"</span>:<span class="string">"bodyContent"</span>&#125;).findAll(<span class="string">"a"</span>,</div><div class="line">                          href=re.compile(<span class="string">"^(/wiki/)((?!:).)*$"</span>))</div><div class="line">links = getLinks(<span class="string">"/wiki/Kevin_Bacon"</span>)</div><div class="line"><span class="keyword">while</span> len(links) &gt; 0:</div><div class="line">    newArticle = links[random.randint(0, len(links)-1)].attrs[<span class="string">"href"</span>]</div><div class="line">    <span class="built_in">print</span>(newArticle)</div><div class="line">    links = getLinks(newArticle)</div><div class="line"><span class="comment">#伪随机数和随机数种子</span></div><div class="line">random.seed(datetime.datetime.now()) <span class="comment">#种子重复，会导致随机数重复，所以这里采用系统时间作为种子</span></div><div class="line">random.randint(0,len(links)-1) <span class="comment">#返回0-len(links)-1的随机整数</span></div><div class="line"><span class="comment">#python的伪随机数生成器用的是梅森旋转算法，它产生的随机数很难预测且均匀分布</span></div></pre></td></tr></table></figure><p>#要注意异常处理</p><h2 id="采集整个网站"><a href="#采集整个网站" class="headerlink" title="采集整个网站"></a>采集整个网站</h2><p>1.深网（deep Web）和暗网(dark Web)<br>2.遍历整个网站的网络数据采集有许多好处<br>  <1 生成网站地图="" <2="" 收集数据="" 链接去重="" 集合set类型="" <figure="" class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line"></div><div class="line">pages = <span class="built_in">set</span>()</div><div class="line">def getLinks(pageUrl):</div><div class="line">    global pages</div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+pageUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> bsObj.findAll(<span class="string">"a"</span>, href=re.compile(<span class="string">"^(/wiki/)"</span>)):</div><div class="line">        <span class="keyword">if</span> <span class="string">'href'</span> <span class="keyword">in</span> link.attrs:</div><div class="line">            <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] not <span class="keyword">in</span> pages:</div><div class="line">                <span class="comment"># 我们遇到了新页面</span></div><div class="line">                newPage = link.attrs[<span class="string">'href'</span>]</div><div class="line">                <span class="built_in">print</span>(newPage)</div><div class="line">                pages.add(newPage)</div><div class="line">                getLinks(newPage)</div><div class="line">getLinks(<span class="string">""</span>)</div><div class="line"><span class="comment">#关于递归的警告</span></div><div class="line">如果递归运行的次数非常多，前面的递归程序就很可能崩溃</div><div class="line">python默认的递归限制（自我调用次数）是1000次</div><div class="line"><span class="comment">### 收集整个网站数据</span></div><div class="line">``` bash</div><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">pages = <span class="built_in">set</span>()</div><div class="line">def getLinks(pageUrl):</div><div class="line">    global pages</div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+pageUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    try:</div><div class="line">        <span class="built_in">print</span>(bsObj.h1.get_text())</div><div class="line">        <span class="built_in">print</span>(bsObj.find(id=<span class="string">"mw-content-text"</span>).findAll(<span class="string">"p"</span>)[0])</div><div class="line">        <span class="built_in">print</span>(bsObj.find(id=<span class="string">"ca-edit"</span>).find(<span class="string">"span"</span>).find(<span class="string">"a"</span>).attrs[<span class="string">'href'</span>])</div><div class="line">    except AttributeError: <span class="built_in">print</span>(<span class="string">"页面缺少一些属性!不过不用担心!"</span>)</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> bsObj.findAll(<span class="string">"a"</span>, href=re.compile(<span class="string">"^(/wiki/)"</span>)):</div><div class="line">        <span class="keyword">if</span> <span class="string">'href'</span> <span class="keyword">in</span> link.attrs:</div><div class="line">            <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] not <span class="keyword">in</span> pages:</div><div class="line">                <span class="comment"># 我们遇到了新页面</span></div><div class="line">                newPage = link.attrs[<span class="string">'href'</span>]</div><div class="line">                <span class="built_in">print</span>(<span class="string">"----------------\n"</span>+newPage)</div><div class="line">                pages.add(newPage)</div><div class="line">                getLinks(newPage)</div><div class="line">getLinks(<span class="string">""</span>)</div></pre></td></tr></table></1></p><p>#在一个异常处理语句中包裹多岗语句显然是有点儿危险的，你没有办法识别除究竟哪行代码出现了异常，其次，如果有个页面没</p><p>#有前面的标题内容，却有”编辑”按，那么由于前面已经发生异常，后面的编辑连接就不会出现</p><p>#偶尔都是一些数据，只要保存详细的日志就不是什么问题了</p><h2 id="通过互联网采集（多个域名采集）"><a href="#通过互联网采集（多个域名采集）" class="headerlink" title="通过互联网采集（多个域名采集）"></a>通过互联网采集（多个域名采集）</h2><p>相比之前的单个域名采集，互联网采集要难得多——不同网站的布局迥然不同。这就意味着我们必须在要寻找的信息<br>以及查找方式上都极具灵活性<br>在你写爬虫随意跟外链跳转之前，请问自己几个问题<br>1.我要搜集那些数据？这些数据可以通过采集几个已经确定的网站完成吗？（永远是最简单的做法）？<br>  或者我的爬虫需要发现那些我可能不知道的网站吗？<br>2.当我的爬虫到某个网站，它是立即顺着下一个出站链接跳到一个新网站，还是在网站上待一会儿，深入采集网站的内容<br>3.有没有我不想采集的一类网站？我对非英文网站的内容感兴趣吗？<br>4.如果我的网络爬虫引起了某个网管的怀疑，我该如何避免法律责任</p><p>几个灵活的python函数组合起来就可以实现不同类型的网络爬虫<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import datetime</div><div class="line">import random</div><div class="line">pages = <span class="built_in">set</span>()</div><div class="line">random.seed(datetime.datetime.now())</div><div class="line"><span class="comment"># 获取页面所有内链的列表</span></div><div class="line">def getInternalLinks(bsObj, includeUrl):</div><div class="line">    internalLinks = []</div><div class="line">    <span class="comment"># 找出所有以"/"开头的链接</span></div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> bsObj.findAll(<span class="string">"a"</span>, href=re.compile(<span class="string">"^(/|.*"</span>+includeUrl+<span class="string">")"</span>)):</div><div class="line">        <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] is not None:</div><div class="line">            <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] not <span class="keyword">in</span> internalLinks:</div><div class="line">                internalLinks.append(link.attrs[<span class="string">'href'</span>])</div><div class="line">    <span class="built_in">return</span> internalLinks</div><div class="line"><span class="comment"># 获取页面所有外链的列表</span></div><div class="line">def getExternalLinks(bsObj, excludeUrl):</div><div class="line">    externalLinks = []</div><div class="line">    <span class="comment"># 找出所有以"http"或"www"开头且不包含当前URL的链接</span></div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> bsObj.findAll(<span class="string">"a"</span>,href=re.compile(<span class="string">"^(http|www)((?!"</span> + excludeUrl + <span class="string">").)*$"</span>)):</div><div class="line">        <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] is not None:</div><div class="line">            <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] not <span class="keyword">in</span> externalLinks: externalLinks.append(link.attrs[<span class="string">'href'</span>])</div><div class="line">    <span class="built_in">return</span> externalLinks</div><div class="line">def splitAddress(address):</div><div class="line">    addressParts = address.replace(<span class="string">"http://"</span>, <span class="string">""</span>).split(<span class="string">"/"</span>)</div><div class="line">    <span class="built_in">return</span> addressParts</div><div class="line">def getRandomExternalLink(startingPage):</div><div class="line">    html = urlopen(startingPage)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0])</div><div class="line">    <span class="keyword">if</span> len(externalLinks) == 0:</div><div class="line">        internalLinks = getInternalLinks(startingPage)</div><div class="line">        <span class="built_in">return</span> getExternalLinks(internalLinks[random.randint(0,len(internalLinks) - 1)])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="built_in">return</span> externalLinks[random.randint(0, len(externalLinks) - 1)]</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def followExternalOnly(startingSite):</div><div class="line">    externalLink = getRandomExternalLink(<span class="string">"http://oreilly.com"</span>)</div><div class="line">    <span class="built_in">print</span>(<span class="string">"随机外链是:"</span>+externalLink)</div><div class="line">    followExternalOnly(externalLink)</div><div class="line">followExternalOnly(<span class="string">"http://oreilly.com"</span>)</div></pre></td></tr></table></figure></p><p>#网站首页上并不能保证一直能发现外链。这时为了能够发现外链，就需要用一种类似前面案例中使用的采集方法，即递归</p><p>#深入一个网站直到找到一个外链才停止<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">allExtLinks = <span class="built_in">set</span>()</div><div class="line">allIntLinks = <span class="built_in">set</span>()</div><div class="line">def getAllExternalLinks(siteUrl):</div><div class="line">    html = urlopen(siteUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    internalLinks = getInternalLinks(bsObj, splitAddress(siteUrl)[0])</div><div class="line">    externalLinks = getExternalLinks(bsObj, splitAddress(siteUrl)[0])</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> externalLinks:</div><div class="line">        <span class="keyword">if</span> link not <span class="keyword">in</span> allExtLinks:</div><div class="line">            allExtLinks.add(link)</div><div class="line">        <span class="built_in">print</span>(link)</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> internalLinks:</div><div class="line">        <span class="keyword">if</span> link not <span class="keyword">in</span> allIntLinks:</div><div class="line">            <span class="built_in">print</span>(<span class="string">"即将获取链接的URL是:"</span> + link)</div><div class="line">        allIntLinks.add(link)</div><div class="line">        getAllExternalLinks(link)</div><div class="line">getAllExternalLinks(<span class="string">"http://oreilly.com"</span>)</div></pre></td></tr></table></figure></p><p>#写代码之前拟个大纲或画个流程图是很好的编程习惯，这么做不仅可以为你后期处理节省好多时间，更重要的是可以防止<br>自己在爬虫变得越来越复杂是乱了方寸<br>*处理网页重定向<br> 重定向分为两种：服务器端重定向，客户端重定向；在服务器端的重定向urllib库一般会给你自动处理，但是你要注意，<br> 你要采集的页面URL可能并不是你当前所在的页面的URL</p><h2 id="用Scrapy采集"><a href="#用Scrapy采集" class="headerlink" title="用Scrapy采集"></a>用Scrapy采集</h2><h3 id="创建新的Scrapy项目"><a href="#创建新的Scrapy项目" class="headerlink" title="创建新的Scrapy项目"></a>创建新的Scrapy项目</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$scrapy</span> startproject wikiSpider</div><div class="line"><span class="comment">#然后在spiders文件夹（一定要注意路径）里创建一个articleSpider.py文件，另外需要在items.py文件中，我们需要定义一个Article类</span></div><div class="line">from scrapy import Item,Field</div><div class="line">class Ariticle(Item): <span class="comment"># 每个Item对象表示网站上的一个页面 我们现在只收集title字段</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    title = Field()</div></pre></td></tr></table></figure><h3 id="填坑？"><a href="#填坑？" class="headerlink" title="填坑？"></a>填坑？</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#from scrapy.contrib.spiders import CrawlSpider, Rule</span><span class="comment">#from wikiSpider.items import Article</span><span class="comment">#from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor</span></div><div class="line">from scrapy.spiders import CrawlSpider, Rule</div><div class="line">from wikiSpider.items import Article</div><div class="line">from scrapy.linkextractors import LinkExtractor</div><div class="line"></div><div class="line">class ArticleSpider(CrawlSpider):</div><div class="line">    name=<span class="string">"article"</span></div><div class="line">    allowed_domains = [<span class="string">"en.wikipedia.org"</span>]</div><div class="line">    start_urls = [<span class="string">"http://en.wikipedia.org/wiki/Main_Page"</span>,</div><div class="line">                    <span class="string">"http://en.wikipedia.org/wiki/Python_%28programming_language%29"</span>]</div><div class="line">    rules = [Rule(LinkExtractor(allow=(<span class="string">'(/wiki/)((?!:).)*$'</span>),),</div><div class="line">                                            callback=<span class="string">"parse_item"</span>, follow=True)]</div><div class="line">    def parse(self, response):</div><div class="line">        item = Article()</div><div class="line">        title = response.xpath(<span class="string">'//h1/text()'</span>)[0].extract()</div><div class="line">        <span class="built_in">print</span>(<span class="string">"Title is: "</span>+title)</div><div class="line">        item[<span class="string">'title'</span>] = title</div><div class="line">        <span class="built_in">return</span> item</div></pre></td></tr></table></figure><p>由于我用的是scrapy1.4+python3,上面是书上原来的代码scrapy版本不详，python2.7，使用上面的代码运行，你会发现好多的函数和包都丢弃或者换位置了，虽然本机有2.7python 但是scrapy的版本不知道该如何弄，所以找了1.4的版本的文旦<br>查阅了一下，改了一下包引用以及函数，但是结果不知道是否达到预期，慢慢学习吧</p><h3 id="Scrapy处理日志"><a href="#Scrapy处理日志" class="headerlink" title="Scrapy处理日志"></a>Scrapy处理日志</h3><p>五种级别日志 CRITICAL ERROR WARNING DEBUG INFO<br>如果级别设置为ERROR，那么只有CRITICAL 和ERROR日志会显示出来<br>将日志输出到一个独立的文件中<br>$ scrapy crawl article -s LOG_FILE=wiki.log<br>如果目录中没有wiki.log,那么运行程序会创建一个新文件，然后把所有的日志都保存在里面，如果已经存在，会在原文后面加入新的日志文件</p><h3 id="输出不同格式"><a href="#输出不同格式" class="headerlink" title="输出不同格式"></a>输出不同格式</h3><p>$ scrapy crawl article -o articles.csv -t csv<br>$ scrapy crawl article -o articles.json -t json<br>$ scrapy crawl article -o articles.xml -t xml<br>当然，你也可以自定义Item对象</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;开始采集&quot;&gt;&lt;a href=&quot;#开始采集&quot; class=&quot;headerlink&quot; title=&quot;开始采集&quot;&gt;&lt;/a&gt;开始采集&lt;/h1&gt;&lt;h2 id=&quot;遍历单个域名&quot;&gt;&lt;a href=&quot;#遍历单个域名&quot; class=&quot;headerlink&quot; title=&quot;遍历单个域
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（二）</title>
    <link href="http://yoursite.com/2017/09/30/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/30/python数据收集（二）/</id>
    <published>2017-09-30T07:54:29.000Z</published>
    <updated>2017-09-30T12:03:43.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="复杂的HTML解析"><a href="#复杂的HTML解析" class="headerlink" title="复杂的HTML解析"></a>复杂的HTML解析</h1><h2 id="不是一直都要用锤子"><a href="#不是一直都要用锤子" class="headerlink" title="不是一直都要用锤子"></a>不是一直都要用锤子</h2><p>例如：<br>$ bsObj.findAll(“table”)[4].findAll(“tr”)[2].find(“td”).findAll(“div”)[1].find(“a”)<br>虽然可以达到目标，但是除了代码欠缺美感之外，还有就是管理员对网页稍作修改，代码就会失效，甚至会毁了整个<br>网络爬虫<br>如何解决该问题呢？<br>1.寻找样式更友好的移动版或者看看有没有打印此页的链接<br>2.寻找隐藏在javascript文件里的信息<br>3.你要的信息或许也可以从网页的URL链接里获取<br>4.找找你要的信息是不是该网站从别的网站上抓取出来的</p><h2 id="再端一碗BeautifulSoup"><a href="#再端一碗BeautifulSoup" class="headerlink" title="再端一碗BeautifulSoup"></a>再端一碗BeautifulSoup</h2><h3 id="利用好css中的class和ID属性"><a href="#利用好css中的class和ID属性" class="headerlink" title="利用好css中的class和ID属性"></a>利用好css中的class和ID属性</h3><p>ex:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line"></div><div class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com/pages/warandpeace.html"</span>)</div><div class="line">bsObj = BeautifulSoup(html,<span class="string">"html.parser"</span>)</div><div class="line">nameList = bsObj.findAll(<span class="string">"span"</span>,&#123;<span class="string">"class"</span>:<span class="string">"green"</span>&#125;)</div><div class="line"><span class="keyword">for</span> name <span class="keyword">in</span> nameList:</div><div class="line">    <span class="built_in">print</span>(name.get_text())</div><div class="line"><span class="comment">#使用name.get_text()会将标签中的内容提取出来，如果你正在处理一个包含许多超链接，段落和标签的</span></div><div class="line"><span class="comment">#大段源码，.get_text()会将这些处理成只剩下一串不带标签的文字，一般情况下，尽可能的保留HTML文档的标签结构</span></div></pre></td></tr></table></figure></p><h3 id="BeautifulSoup的find-和findAll"><a href="#BeautifulSoup的find-和findAll" class="headerlink" title="BeautifulSoup的find()和findAll()"></a>BeautifulSoup的find()和findAll()</h3><p>函数定义：<br>findAll(tag, attributes, recursive, text, limit, keywords)<br>find(tag, attributes, recursive, text, keywords)<br>下面来介绍一下各个参数<br>recursive:bool型 True：查找标签参数的所有子标签（默认）<br>        False：只查找文档的一级标签<br>text:更具标签的文本内容去匹配<br>$ nameList = bsObj.findAll(text=”the prince”)<br>$ print(len(nameList))<br>limit:find等价于finall的limit等于1，获取前几项是按照网页上的顺序排序的<br>keyword<br>以下每组效果相同<br>$ bsObj.findAll(id=”text”)<br>$ bsObj.findAll(“”, {“id”:”text”})</p><p>$ bsObj.findAll(class<em>=”green”) #由于class是python的保留字，所以必须在其后面添加</em><br>$ bsObj.findAll(“”, {“class”:”green”})</p><h3 id="其他BeautifulSoup对象"><a href="#其他BeautifulSoup对象" class="headerlink" title="其他BeautifulSoup对象"></a>其他BeautifulSoup对象</h3><p>常用对象： 1.BeautifulSoup 2.Tag对象（前面已经提到过，例如：bsObj.div.h1）<br>不常用对象：NavigableString对象：用来表示标签里的文字<br>     Comment 对象：用来查找HTML文档的注释标签 &lt;!—- 像这样 —&gt;</p><h3 id="导航树"><a href="#导航树" class="headerlink" title="导航树"></a>导航树</h3><p>如果你需要通过标签在文档中的位置来查找标签，就使用导航树(Navigating Trees)<br>$ bsObj.tag.subTag.anotherSubTag</p><h4 id="处理子标签和其他后代标签"><a href="#处理子标签和其他后代标签" class="headerlink" title="处理子标签和其他后代标签"></a>处理子标签和其他后代标签</h4><p>$ bsObj.div.findAll(“img”)<br>如果你只想找出子标签，可以用.children标签<br>$ from urllib.request import urlopen<br>$ from bs4 import BeautifulSoup<br>$ html = urlopen(“<a href="http://www.pythonscraping.com/pages/page3.html" target="_blank" rel="external">http://www.pythonscraping.com/pages/page3.html</a>“)<br>$ bsObj = BeautifulSoup(html)<br>$ for child in bsObj.find(“table”,{“id”:”giftList”}).children:<br>    print(child)<br>如果你用descendants()函数，就会将其所有后代标签打印出来</p><h4 id="处理兄弟标签"><a href="#处理兄弟标签" class="headerlink" title="处理兄弟标签"></a>处理兄弟标签</h4><p>$ from urllib.request import urlopen<br>$ from bs4 import BeautifulSoup<br>$ html = urlopen(“<a href="http://www.pythonscraping.com/pages/page3.html" target="_blank" rel="external">http://www.pythonscraping.com/pages/page3.html</a>“)<br>$ bsObj = BeautifulSoup(html)<br>$ for sibling in bsObj.find(“table”,{“id”:”giftList”}).tr.next_siblings:<br>    print(sibling)</p><p>#这里需要注意的是，next_siblings只会找到他后面的兄弟标签</p><p>#让标签的选择更具体(如果可能的话）<br>$ bsObj.find(“table”,{“id”:”giftList”}).tr</p><p>#罗列一下查找子标签的四个函数<br>next_sibling 和 previous_sibling<br>next_siblings 和 previous_siblings<br>从命名规范中我们也可以发现其各自的用途以及区别，这里不在赘述</p><h4 id="父标签处理"><a href="#父标签处理" class="headerlink" title="父标签处理"></a>父标签处理</h4><p>$ from urllib.request import urlopen<br>$ from bs4 import BeautifulSoup<br>$ html = urlopen(“<a href="http://www.pythonscraping.com/pages/page3.html" target="_blank" rel="external">http://www.pythonscraping.com/pages/page3.html</a>“)<br>$ bsObj = BeautifulSoup(html)<br>$ print(bsObj.find(“img”,{“src”:”../img/gifts/img1.jpg”}).parent.previous_sibling.get_text())</p><h2 id="正则表达式：如果你有一个问题打算用正则表达式来解决，那么就是两个问题了"><a href="#正则表达式：如果你有一个问题打算用正则表达式来解决，那么就是两个问题了" class="headerlink" title="正则表达式：如果你有一个问题打算用正则表达式来解决，那么就是两个问题了"></a>正则表达式：如果你有一个问题打算用正则表达式来解决，那么就是两个问题了</h2><p>ex：邮箱正则表达式：[A-Za-z0-9._+]+@[A-Za-z]+.(com|org\edu\net)<br>OK,接下来我们学习一下12中python中最常用的正则表达式符号</p><ul><li>: 匹配前面的字符，子表达式或括号里的字符0次或多次    a<em>b</em>      aaaaaaaaa,aaabbb,bbbb</li></ul><ul><li>: 匹配前面的字符，子表达式或括号里的字符至少一次    a+b+       aaaabbb ab<br>[]: 匹配任意一个字符（相当于”任选一个”）        [A-Z]<em>   APPLe,GAPITALS<br>(): 表达式编组（在正则表达式的规划里编组会优先运行）    (a</em>b)<em>      aaabaab<br>{m,n}: 匹配前面的字符，子表达式或括号里的字符m到n次（包含m或n）     a{2,3}b{2,3}<br>[^]: 匹配任意一个不在括号里的字符            [^A-Z]</em>     sdasdasd<br>| : 匹配任意一个由竖线分割哥的字符，子表达式        b(a|i|e) bad,bid,bed<br>. : 匹配任意单个字符（包括符号，数字和空格等）        b.d     bad,bzd,b$d,b d<br>^ : 字符串开始位置的字符或子表达式            ^a     apple,asdf,a<br>\ : 转义字符（把有特殊含义的字符转换成字面形式）         . | \<br>$ : 经常用在正则表达式的末尾，表示”从字符串的末端匹配”。如果不用它，<br>  每个正则表达式实际都带着’.<em>’模式，只会从字符串的开头进行匹配。这个符号<br>  可以看做是^符号的反义词<br>?!  “不包含”。这个奇怪的组合通常会放在字符或正则表达式前面，表示字符不能<br>  出现在诺表字符串里。字符通常会在字符窜的不同部位出现。如果要在整个字符串<br>  中全部排除某个字符，就加上^和$符号               ^((?![A-Z]).)</em>$    <h2 id="正则表达式和BeautifulSoup"><a href="#正则表达式和BeautifulSoup" class="headerlink" title="正则表达式和BeautifulSoup"></a>正则表达式和BeautifulSoup</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com/pages/page3.html"</span>)</div><div class="line">bsObj = BeautifulSoup(html)</div><div class="line">images = bsObj.findAll(<span class="string">"img"</span>,&#123;<span class="string">"src"</span>:re.compile(<span class="string">"\.\.\/img\/gifts/img.*\.jpg"</span>)&#125;)</div><div class="line"><span class="keyword">for</span> image <span class="keyword">in</span> images:</div><div class="line">    <span class="built_in">print</span>(image[<span class="string">"src"</span>])</div></pre></td></tr></table></figure></li></ul><h2 id="获取属性"><a href="#获取属性" class="headerlink" title="获取属性"></a>获取属性</h2><p>对于一个标签对象，可以用下面的代码获取它的全部属性：<br>$ myTag.attrs #返回的是Python字典对象<br>$ myImgTag.attrs[“src”]</p><h2 id="Lambda表达式：本质上就是一个函数"><a href="#Lambda表达式：本质上就是一个函数" class="headerlink" title="Lambda表达式：本质上就是一个函数"></a>Lambda表达式：本质上就是一个函数</h2><p>BeautifulSoup允许我们把特定函数类型当做findAll函数的参数-&gt;限定：标签作为参数，返回结果是boolean型<br>BeautifulSoup用这个函数来评估他遇到的每个标签对象，最后把评估结果为真的标签保留，把其他标签剔除<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.findAll(lambda tag: len(tag.attrs) == 2) <span class="comment">#这个在我的环境下不起作用，先标注一下</span></div><div class="line"><span class="comment">#返回结果</span>&lt;div class=<span class="string">"body"</span> id=<span class="string">"content"</span>&gt;&lt;/div&gt;&lt;span style=<span class="string">"color:red"</span> class=<span class="string">"title"</span>&gt;&lt;/span&gt;</div></pre></td></tr></table></figure></p><h2 id="超越BeautifulSoup"><a href="#超越BeautifulSoup" class="headerlink" title="超越BeautifulSoup"></a>超越BeautifulSoup</h2><p>lxml -&gt; <a href="http://lxml.de/" target="_blank" rel="external">http://lxml.de/</a> 用c语言写的，处理绝大多数HTML文档时的速度都非常快（学习成本高）<br>HTML parser -&gt; python自带解析库，不需要重新安装</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;复杂的HTML解析&quot;&gt;&lt;a href=&quot;#复杂的HTML解析&quot; class=&quot;headerlink&quot; title=&quot;复杂的HTML解析&quot;&gt;&lt;/a&gt;复杂的HTML解析&lt;/h1&gt;&lt;h2 id=&quot;不是一直都要用锤子&quot;&gt;&lt;a href=&quot;#不是一直都要用锤子&quot; class
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（一）</title>
    <link href="http://yoursite.com/2017/09/30/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/30/python数据收集（一）/</id>
    <published>2017-09-30T02:46:43.000Z</published>
    <updated>2017-09-30T07:52:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前的python爬虫初体验，是看视频教学的，感觉不太踏实，正好现在需要系统的学一下爬虫，所有就找了一本python数据采集的书来看看，为了区分学习计划，所以将这个作为数据采集第一篇笔记，加油，坚持下去！</p><h1 id="初见爬虫"><a href="#初见爬虫" class="headerlink" title="初见爬虫"></a>初见爬虫</h1><h2 id="关于urllib"><a href="#关于urllib" class="headerlink" title="关于urllib"></a>关于urllib</h2><p>urllib被分为：urllib.request、urllib.parse和urllib.error 三个子模块</p><h2 id="安装BeautifulSoup"><a href="#安装BeautifulSoup" class="headerlink" title="安装BeautifulSoup"></a>安装BeautifulSoup</h2><h2 id="用虚拟环境保存库文件"><a href="#用虚拟环境保存库文件" class="headerlink" title="用虚拟环境保存库文件"></a>用虚拟环境保存库文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ virtualenv scrapingEnv</div><div class="line"><span class="comment">#这样就创建了一个叫作scrapingEnv的新环境，你需要先激活它再使用：</span></div><div class="line">$ <span class="built_in">cd</span> scrapingEnv/</div><div class="line">$ <span class="built_in">source</span> bin/activate</div><div class="line"><span class="comment">#你可以在新建的scrapingEnv环境里，安装并使用BeautifulSoup</span></div><div class="line"><span class="comment">#当你不在使用虚拟机环境的库时，可以通过释放命令来退出环境：</span></div><div class="line">$ deactivate</div></pre></td></tr></table></figure><h2 id="运行BeautifulSoup"><a href="#运行BeautifulSoup" class="headerlink" title="运行BeautifulSoup"></a>运行BeautifulSoup</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(<span class="string">"http://www.pythonscraping.com/pages/page1.html"</span>) bsObj = BeautifulSoup(html.read())<span class="built_in">print</span>(bsObj.h1)</div></pre></td></tr></table></figure><h2 id="可靠的网络连接"><a href="#可靠的网络连接" class="headerlink" title="可靠的网络连接"></a>可靠的网络连接</h2><h3 id="html-urlopen-“http-news-baidu-com-“-是如何处理异常的？"><a href="#html-urlopen-“http-news-baidu-com-“-是如何处理异常的？" class="headerlink" title="html = urlopen(“http://news.baidu.com/“)是如何处理异常的？"></a>html = urlopen(“<a href="http://news.baidu.com/“)是如何处理异常的？" target="_blank" rel="external">http://news.baidu.com/“)是如何处理异常的？</a></h3><p>这行代码主要可能会发生两种异常</p><ol><li><p>页面在服务器上不存在<br> 404 Page Not Found<br> 500 Internal Server Error<br>所有这些类似的情形，urlopen函数都会抛出”HTTPError” 异常<br>处理这种异常的方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">try:html = urlopen(<span class="string">"http://news.baidu.com/"</span>)except HTTPError as e: </div><div class="line"><span class="built_in">print</span>(e)<span class="comment"># 返回空值，中断程序，或者执行另一个方案 </span></div><div class="line"><span class="keyword">else</span>:<span class="comment"># 程序继续。注意:如果你已经在上面异常捕捉那一段代码里返回或中断(break)， </span></div><div class="line"><span class="comment"># 那么就不需要使用else语句了，这段代码也不会执行</span></div></pre></td></tr></table></figure></li><li><p>服务器不存在</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> html is None:<span class="built_in">print</span>(<span class="string">"URL is not found"</span>)<span class="keyword">else</span>:<span class="comment"># 程序继续</span></div></pre></td></tr></table></figure></li><li><p>标签不存在</p><pre><code class="bash"> <span class="comment">#如果BeautifulSoup对象里面没有nonExistentTag标签</span> <span class="built_in">print</span>(bsObj.nonExistentTag) <span class="comment">#返回None对象</span> <span class="comment">#如果这个时候在继续调用nonExistentTag的子标签</span> <span class="built_in">print</span>(bsObj.nonExistentTag.someTag) <span class="comment">#会抛出 AttributeError: 'NoneType' object has no attribute 'someTag'</span>解决方法： try:     badContent = bsObj.nonExistingTag.anotherTag except AttributeError as e:      <span class="built_in">print</span>(<span class="string">"Tag was not found"</span>) <span class="keyword">else</span>:     <span class="keyword">if</span> badContent == None:         <span class="built_in">print</span> (<span class="string">"Tag was not found"</span>)      <span class="keyword">else</span>:         <span class="built_in">print</span>(badContent)</code></pre><p>我们会发现，这样写会有些累赘，换一种写法<br>from urllib.request import urlopen<br>from urllib.error import HTTPError<br>from bs4 import BeautifulSoup<br>def getTitle(url):<br> try:</p><pre><code>html = urlopen(url) </code></pre><p> except HTTPError as e:</p><pre><code>return None </code></pre><p> try:</p><pre><code>        bsObj = BeautifulSoup(html.read())title = bsObj.body.h1 </code></pre><p> except AttributeError as e:</p><pre><code>return None </code></pre><p> return title<br>title = getTitle(“<a href="http://news.baidu.com/" target="_blank" rel="external">http://news.baidu.com/</a>“)<br>if title == None:<br> print(“Title could not be found”)<br>else:<br> print(title)</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前的python爬虫初体验，是看视频教学的，感觉不太踏实，正好现在需要系统的学一下爬虫，所有就找了一本python数据采集的书来看看，为了区分学习计划，所以将这个作为数据采集第一篇笔记，加油，坚持下去！&lt;/p&gt;
&lt;h1 id=&quot;初见爬虫&quot;&gt;&lt;a href=&quot;#初见爬虫&quot; 
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>概率论与数理统计（一）</title>
    <link href="http://yoursite.com/2017/09/28/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/28/概率论与数理统计（一）/</id>
    <published>2017-09-28T10:25:13.000Z</published>
    <updated>2017-09-28T12:02:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="离散型随机变量及其分布规律"><a href="#离散型随机变量及其分布规律" class="headerlink" title="离散型随机变量及其分布规律"></a>离散型随机变量及其分布规律</h1><h2 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a>0-1分布</h2><h2 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h2><h2 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h2><h1 id="随机变量的分布函数"><a href="#随机变量的分布函数" class="headerlink" title="随机变量的分布函数"></a>随机变量的分布函数</h1><p>#连续性随机变量及其概率密度</p><h2 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h2><h2 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h2><h2 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h2><h1 id="多维随机变量及其分布"><a href="#多维随机变量及其分布" class="headerlink" title="多维随机变量及其分布"></a>多维随机变量及其分布</h1><h2 id="二维随机变量"><a href="#二维随机变量" class="headerlink" title="二维随机变量"></a>二维随机变量</h2><h2 id="边缘分布"><a href="#边缘分布" class="headerlink" title="边缘分布"></a>边缘分布</h2><h2 id="条件分布"><a href="#条件分布" class="headerlink" title="条件分布"></a>条件分布</h2><h2 id="互相独立的随机变量"><a href="#互相独立的随机变量" class="headerlink" title="互相独立的随机变量"></a>互相独立的随机变量</h2><h2 id="两个随机变量的函数的分布"><a href="#两个随机变量的函数的分布" class="headerlink" title="两个随机变量的函数的分布"></a>两个随机变量的函数的分布</h2><h3 id="卷积公式"><a href="#卷积公式" class="headerlink" title="卷积公式"></a>卷积公式</h3><h1 id="随机变量的数字特征"><a href="#随机变量的数字特征" class="headerlink" title="随机变量的数字特征"></a>随机变量的数字特征</h1><h2 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h2><h2 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h2><h2 id="协方差与相关系数"><a href="#协方差与相关系数" class="headerlink" title="协方差与相关系数"></a>协方差与相关系数</h2><h2 id="矩、协方差矩阵"><a href="#矩、协方差矩阵" class="headerlink" title="矩、协方差矩阵"></a>矩、协方差矩阵</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;离散型随机变量及其分布规律&quot;&gt;&lt;a href=&quot;#离散型随机变量及其分布规律&quot; class=&quot;headerlink&quot; title=&quot;离散型随机变量及其分布规律&quot;&gt;&lt;/a&gt;离散型随机变量及其分布规律&lt;/h1&gt;&lt;h2 id=&quot;0-1分布&quot;&gt;&lt;a href=&quot;#0-1分
      
    
    </summary>
    
      <category term="概率论" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="概率论" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（四）</title>
    <link href="http://yoursite.com/2017/09/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/27/机器学习实战（四）/</id>
    <published>2017-09-27T13:38:18.000Z</published>
    <updated>2017-09-29T12:51:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>为什么先写四，而没有二，三呢？二三的代码都码了一遍，理解的还不是很充分，今天看到四，所以先把四的概念写写：朴素贝叶斯</p><h1 id="基于概率论的分类方法：朴素贝叶斯"><a href="#基于概率论的分类方法：朴素贝叶斯" class="headerlink" title="基于概率论的分类方法：朴素贝叶斯"></a>基于概率论的分类方法：朴素贝叶斯</h1><p>贝叶斯决策论的核心思想：选着具有最高概率的决策</p><h2 id="条件概率-p-x-y-c1"><a href="#条件概率-p-x-y-c1" class="headerlink" title="条件概率 p(x,y|c1)"></a>条件概率 p(x,y|c1)</h2><p><img src="https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D113/sign=62231e180df431adb8d247387837ac0f/35a85edf8db1cb1399c0c799dc54564e93584b8b.jpg" alt="“条件概率公式”"><br><img src="https://gss1.bdstatic.com/9vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D280/sign=51f198b6c4fdfc03e178e4b0e43f87a9/aec379310a55b31972c9ec3d44a98226cffc1741.jpg" alt="“贝叶斯公式”"></p><h2 id="使用条件概率来分类"><a href="#使用条件概率来分类" class="headerlink" title="使用条件概率来分类"></a>使用条件概率来分类</h2><h2 id="词集模型-词袋模型"><a href="#词集模型-词袋模型" class="headerlink" title="词集模型 词袋模型"></a>词集模型 词袋模型</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为什么先写四，而没有二，三呢？二三的代码都码了一遍，理解的还不是很充分，今天看到四，所以先把四的概念写写：朴素贝叶斯&lt;/p&gt;
&lt;h1 id=&quot;基于概率论的分类方法：朴素贝叶斯&quot;&gt;&lt;a href=&quot;#基于概率论的分类方法：朴素贝叶斯&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（一）</title>
    <link href="http://yoursite.com/2017/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/24/机器学习实战（一）/</id>
    <published>2017-09-24T11:27:53.000Z</published>
    <updated>2017-09-24T14:00:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天开始了《机器学习实战》的学习，下面就是读书笔记咯</p><h1 id="numpy函数库基础"><a href="#numpy函数库基础" class="headerlink" title="numpy函数库基础"></a>numpy函数库基础</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">random.rand(4,4)</div><div class="line"><span class="comment">#调用mat()函数可以将数组转化为矩阵</span></div><div class="line">randMat = mat(random.rand(4,4)</div><div class="line"><span class="comment">#逆矩阵</span></div><div class="line">invRandMat = randMat.I</div><div class="line"><span class="comment">#逆矩阵*矩阵</span></div><div class="line">invRandMat * randMat <span class="comment">#这里应该是单位矩阵的，但是计算机运算有误差</span></div><div class="line"><span class="comment">#计算误差值</span></div><div class="line">myEye = randMat * invRandMat</div><div class="line">myEye - eye(4) <span class="comment">#用我们计算得到的矩阵减单位矩阵</span></div></pre></td></tr></table></figure><h1 id="K-临近算法"><a href="#K-临近算法" class="headerlink" title="K-临近算法"></a>K-临近算法</h1><p>简单来说，K-临近算法采用测量不同特征值之间的距离方法进行分类</p><h2 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h2><pre><code>优点：精度高，对异常值不敏感，无数据输入假定缺点：计算复杂度高，空间复杂度高适用数据范围：数值型和标称型</code></pre><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">计算已知类别数据集中的每个点以此执行以下操作</div><div class="line">按照距离递增次序排序</div><div class="line">选取与当前点距离最小的k个点</div><div class="line">确定前k个点所在类别的出现频率</div><div class="line">返回前k个点出现频率最高的类别的类当做当前点的预测分类</div></pre></td></tr></table></figure><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">def classify0(inX,dataSet,labels,k):</div><div class="line">    dataSetSize = dataSet.shape[0]</div><div class="line">    diffMat = tile(inX,(dataSetSize,1)) - dataSet</div><div class="line">    sqDiffMat = diffMat**2</div><div class="line">    sqDistances = sqDiffMat.sum(axis=1)</div><div class="line">    distances = sqDistances**0.5</div><div class="line">    sortedDistIndicies = distances.argsort()</div><div class="line">    classCount = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">        voteIlabel = labels[sortedDistIndicies[i]]</div><div class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1</div><div class="line">    sortedClassCount = sorted(classCount.items(),</div><div class="line">                              key = operator.itemgetter(1),reverse=True)</div><div class="line">    <span class="built_in">return</span> sortedClassCount[0][0]</div></pre></td></tr></table></figure><p>需要注意的是，当你在终端导入一个模块的时候，如果你给这个模块增加了一个新的函数的时候，你必须先退出python3环境，重新导入才能生效<br>array.shape[0] #放回数组的长度<br>tile() # Construct an array by repeating A the number of times given by reps.<br>tile(a,x):   x是控制a重复几次的，结果是一个一维数组<br>tile(a,(x,y))：   结果是一个二维矩阵，其中行数为x，列数是一维数组a的长度和y的乘积<br>tile(a,(x,y,z)):   结果是一个三维矩阵，其中矩阵的行数为x，矩阵的列数为y，而z表示矩阵每个单元格里a重复的次数。(三维矩阵可以看成一个二维矩阵，每个矩阵的单元格里存者一个一维矩阵a)<br>sqDiffMat.sum(axis=1)：将二维数组按行相加，结果是一个一维数组<br>classCount.get(voteIlabel,0)：字典的get方法，查找第一个参数key，如果不存在，返回参数2<br>sorted(classCount.items(),key = operator.itemgetter(1),reverse=True):根据字典中的第二域进行排序<br>sortedClassCount[0][0]：将匹配到的key值取出来</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天开始了《机器学习实战》的学习，下面就是读书笔记咯&lt;/p&gt;
&lt;h1 id=&quot;numpy函数库基础&quot;&gt;&lt;a href=&quot;#numpy函数库基础&quot; class=&quot;headerlink&quot; title=&quot;numpy函数库基础&quot;&gt;&lt;/a&gt;numpy函数库基础&lt;/h1&gt;&lt;figure
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python网络爬虫 初体验（一）</title>
    <link href="http://yoursite.com/2017/09/24/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-%E5%88%9D%E4%BD%93%E9%AA%8C%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/24/python网络爬虫-初体验（一）/</id>
    <published>2017-09-24T09:12:19.000Z</published>
    <updated>2017-09-24T09:16:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天看了一个关于爬虫的视频，心血来潮，就想弄弄爬虫，BUT只弄了一个简单页面，不过也算是体验一把，以后会慢慢深入研究（以后是什么鬼？？）</p><h1 id="代码示例（大神勿喷吐槽）"><a href="#代码示例（大神勿喷吐槽）" class="headerlink" title="代码示例（大神勿喷吐槽）"></a>代码示例（大神勿喷吐槽）</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">from datetime import datetime</div><div class="line"></div><div class="line">res = requests.get(<span class="string">'https://news.sina.cn/gn/2017-09-24/'</span></div><div class="line">                   <span class="string">'detail-ifymenmt6515409.d.html?vt=4&amp;pos=8&amp;wm=8017_0001&amp;cid=56261'</span>)</div><div class="line">res.encoding = <span class="string">'utf-8'</span></div><div class="line">soup = BeautifulSoup(res.text,<span class="string">'html.parser'</span>)</div><div class="line">header = soup.select(<span class="string">'h1'</span>)</div><div class="line"></div><div class="line"><span class="comment">#找出含有a标签的元素</span></div><div class="line"><span class="comment"># a_link = soup.select('a')</span></div><div class="line"><span class="comment"># print(a_link[0])</span></div><div class="line"></div><div class="line"><span class="comment">#找出class为art_p的元素</span></div><div class="line"><span class="comment"># for art_p in soup.select('.art_p'):</span></div><div class="line"><span class="comment">#     print(art_p)</span></div><div class="line"></div><div class="line"><span class="comment">#找出所有a标签内的链接</span></div><div class="line"><span class="comment"># alinks = soup.select('a')</span></div><div class="line"><span class="comment"># for link in alinks:</span></div><div class="line"><span class="comment">#     print(link['href'])</span></div><div class="line"></div><div class="line"><span class="comment">#找出文章标题，文章内容，以及文章时间和出处</span></div><div class="line">art_detail = &#123;&#125;</div><div class="line">art_time = soup.select(<span class="string">'.weibo_time'</span>)[0].contents[1].text + \</div><div class="line">           soup.select(<span class="string">'.weibo_time'</span>)[0].contents[2].strip()</div><div class="line">art_title = soup.select(<span class="string">'h1'</span>)[0].text</div><div class="line"><span class="comment">#字符串转时间</span></div><div class="line">art_time = <span class="string">'2017年'</span>+art_time</div><div class="line">dt = datetime.strptime(art_time,<span class="string">'%Y年%m月%d日%H:%M'</span>)</div><div class="line">dt = datetime.strftime(dt,<span class="string">'%Y年%m月%d日%H:%M'</span>)</div><div class="line"><span class="comment">#时间转字符串</span></div><div class="line"><span class="comment">#datetime.strftime("%Y-M-%d")</span></div><div class="line"><span class="comment">#处理内容</span></div><div class="line">art_content = soup.select(<span class="string">'.art_p'</span>)</div><div class="line">new_source = art_content[-1].text.lstrip(<span class="string">'来源：'</span>)</div><div class="line">art_content =<span class="string">'\n'</span>.join([p.text.strip() <span class="keyword">for</span> p <span class="keyword">in</span> art_content[:-1]])</div><div class="line"><span class="comment">#将拿到的信息放入到art_detail字典中</span></div><div class="line">art_detail[<span class="string">'art_content'</span>] = art_content</div><div class="line">art_detail[<span class="string">'art_time'</span>] = dt</div><div class="line">art_detail[<span class="string">'art_title'</span>] = art_title</div><div class="line">art_detail[<span class="string">'new_source'</span>] = new_source</div><div class="line"><span class="built_in">print</span>(art_detail)</div></pre></td></tr></table></figure><h1 id="备注一个问题"><a href="#备注一个问题" class="headerlink" title="备注一个问题"></a>备注一个问题</h1><p>object of type ‘Response’ has no len()<br>解决方案：soup = BeautifulSoup(res.text,’html.parser’)</p><p>OK 今天就这样 ^~^</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天看了一个关于爬虫的视频，心血来潮，就想弄弄爬虫，BUT只弄了一个简单页面，不过也算是体验一把，以后会慢慢深入研究（以后是什么鬼？？）&lt;/p&gt;
&lt;h1 id=&quot;代码示例（大神勿喷吐槽）&quot;&gt;&lt;a href=&quot;#代码示例（大神勿喷吐槽）&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>英语学习笔记之（翻译）</title>
    <link href="http://yoursite.com/2017/09/20/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/20/英语学习笔记之（翻译）/</id>
    <published>2017-09-20T13:05:01.000Z</published>
    <updated>2017-09-20T13:20:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天整理一下老师讲的英语翻译（汉译英）技巧</p><h1 id="英语翻译技巧"><a href="#英语翻译技巧" class="headerlink" title="英语翻译技巧"></a>英语翻译技巧</h1><h2 id="单词"><a href="#单词" class="headerlink" title="单词"></a>单词</h2><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><h2 id="完整性"><a href="#完整性" class="headerlink" title="完整性"></a>完整性</h2><h2 id="“的”的处理"><a href="#“的”的处理" class="headerlink" title="“的”的处理"></a>“的”的处理</h2><h2 id="一般词与具体词"><a href="#一般词与具体词" class="headerlink" title="一般词与具体词"></a>一般词与具体词</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ex:indicate</div><div class="line">show(一般词）：   reveal</div><div class="line">imply</div><div class="line">suggest</div></pre></td></tr></table></figure><h2 id="冠词"><a href="#冠词" class="headerlink" title="冠词"></a>冠词</h2><h2 id="名词的单复数"><a href="#名词的单复数" class="headerlink" title="名词的单复数"></a>名词的单复数</h2><p>``` bash<br> 单数 - &gt; 特指<br> 复数 - &gt; 泛指</p><h2 id="谓语"><a href="#谓语" class="headerlink" title="谓语"></a>谓语</h2><p>``` bash<br>    时态<br>    语态<br>    语气</p><h2 id="动词的选择"><a href="#动词的选择" class="headerlink" title="动词的选择"></a>动词的选择</h2><h2 id="平衡"><a href="#平衡" class="headerlink" title="平衡"></a>平衡</h2><h2 id="用词的多样性"><a href="#用词的多样性" class="headerlink" title="用词的多样性"></a>用词的多样性</h2><h2 id="平行结构：前后一致"><a href="#平行结构：前后一致" class="headerlink" title="平行结构：前后一致"></a>平行结构：前后一致</h2><pre><code>动名词 - &gt; 抽象不定式 - &gt; 具体</code></pre><h2 id="语意"><a href="#语意" class="headerlink" title="语意"></a>语意</h2><h3 id="越来越的译法"><a href="#越来越的译法" class="headerlink" title="越来越的译法"></a>越来越的译法</h3><pre><code>increasing adj.  ex: An increasing number of people increasingly adv.growing adj.  ex：a growing number</code></pre><h3 id="面临的译法-face-（倒过来用ing）"><a href="#面临的译法-face-（倒过来用ing）" class="headerlink" title="面临的译法 face （倒过来用ing）"></a>面临的译法 face （倒过来用ing）</h3><h3 id="现在-currently"><a href="#现在-currently" class="headerlink" title="现在 currently"></a>现在 currently</h3><h3 id="短缺-be-shirt-of"><a href="#短缺-be-shirt-of" class="headerlink" title="短缺 be shirt of"></a>短缺 be shirt of</h3><h3 id="缩小-the-shrink-in"><a href="#缩小-the-shrink-in" class="headerlink" title="缩小 the shrink in"></a>缩小 the shrink in</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天整理一下老师讲的英语翻译（汉译英）技巧&lt;/p&gt;
&lt;h1 id=&quot;英语翻译技巧&quot;&gt;&lt;a href=&quot;#英语翻译技巧&quot; class=&quot;headerlink&quot; title=&quot;英语翻译技巧&quot;&gt;&lt;/a&gt;英语翻译技巧&lt;/h1&gt;&lt;h2 id=&quot;单词&quot;&gt;&lt;a href=&quot;#单词&quot; cl
      
    
    </summary>
    
      <category term="English" scheme="http://yoursite.com/categories/English/"/>
    
    
      <category term="English" scheme="http://yoursite.com/tags/English/"/>
    
  </entry>
  
  <entry>
    <title>python初体验（十）</title>
    <link href="http://yoursite.com/2017/09/20/python%E5%88%9D%E4%BD%93%E9%AA%8C%EF%BC%88%E5%8D%81%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/20/python初体验（十）/</id>
    <published>2017-09-20T08:15:06.000Z</published>
    <updated>2017-09-20T11:41:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天我们来学习一下python的代码测试吧</p><h1 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h1><h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>python的标准库中的模块unittest提供了代码测试工具，单元测试用于核实函数在某个方面没有问题<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> import unittest</div><div class="line"> from name_function import get_formatted_name</div><div class="line"></div><div class="line"> class namesTestCase(unittest.TestCase):  <span class="comment">#类名可以随便命名</span></div><div class="line">def test_first_last_name(self):</div><div class="line">formatted_name = get_formatted_name(<span class="string">'janis'</span>,<span class="string">'jopin'</span>)</div><div class="line">self.assertEqual(formatted_name,<span class="string">'Janis Jopin'</span>)</div><div class="line"> unittest.main()</div><div class="line"><span class="comment">#当我们运行该程序的时候，所有的以test_开头的方法都将自动运行</span></div></pre></td></tr></table></figure></p><h2 id="用例测试"><a href="#用例测试" class="headerlink" title="用例测试"></a>用例测试</h2><p>用例测试是一组单元测试，这些单元测试一起核实函数在各种情况下的行为都符合要求。</p><h2 id="测试类"><a href="#测试类" class="headerlink" title="测试类"></a>测试类</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"> import unittest</div><div class="line"> from survey import AnonymousSurvey <span class="comment">#被测试类</span></div><div class="line"></div><div class="line"> class TestAnonymousSurvey(unittest.TestCase):</div><div class="line">def test_store_single_respones(self):</div><div class="line">question = <span class="string">"What "</span></div><div class="line">my_survey = AnonymousSurvey(question)</div><div class="line">my_survey.store_response(<span class="string">'English'</span>)</div><div class="line"></div><div class="line">self.assertIn(<span class="string">"English"</span>,my_survey.response)</div><div class="line"> unittest.main()</div><div class="line">```</div><div class="line"><span class="comment">### setUp()方法</span></div><div class="line">因为我们每创建一个测试方法都得创建一个被测试类的实例，所以使用setUp()可以避免一直创建实例，python会先运行setUp方法，然后在运行以test_开头的方法</div><div class="line">``` bash </div><div class="line"> class TestAnonymousSurvey(unittest.TestCase):</div><div class="line">def setUp(self):</div><div class="line">question = <span class="string">"What "</span></div><div class="line">self.my_survey = AnonymousSurvey(question)</div><div class="line">self.responses = [<span class="string">'English'</span>,<span class="string">'Spanish'</span>,<span class="string">'Mandarin'</span>]</div><div class="line"></div><div class="line">def test_store_single_respones(self):</div><div class="line">question = <span class="string">"What "</span></div><div class="line">self.my_survey.store_response(self.responses[0])</div><div class="line">self.assertIn(self.responses[0],self.my_survey.response)</div><div class="line">def test_store_three_respones(self):</div><div class="line"><span class="keyword">for</span> response <span class="keyword">in</span> self.responses:</div><div class="line">self.my_survey.store_response(response)</div><div class="line"><span class="keyword">for</span> response <span class="keyword">in</span> self.responses:</div><div class="line">self.assertIn(response,self.my_survey.response)</div><div class="line"> unittest.main()</div></pre></td></tr></table></figure><p>接下来就要来到令人激动人心的项目实战啦</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天我们来学习一下python的代码测试吧&lt;/p&gt;
&lt;h1 id=&quot;测试代码&quot;&gt;&lt;a href=&quot;#测试代码&quot; class=&quot;headerlink&quot; title=&quot;测试代码&quot;&gt;&lt;/a&gt;测试代码&lt;/h1&gt;&lt;h2 id=&quot;单元测试&quot;&gt;&lt;a href=&quot;#单元测试&quot; class=
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python初体验（九）</title>
    <link href="http://yoursite.com/2017/09/20/python%E5%88%9D%E4%BD%93%E9%AA%8C%EF%BC%88%E4%B9%9D%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/20/python初体验（九）/</id>
    <published>2017-09-20T06:57:05.000Z</published>
    <updated>2017-09-20T10:58:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>OK 现在我们来学存储数据</p><h1 id="存储数据"><a href="#存储数据" class="headerlink" title="存储数据"></a>存储数据</h1><h2 id="使用模块json来存储数据"><a href="#使用模块json来存储数据" class="headerlink" title="使用模块json来存储数据"></a>使用模块json来存储数据</h2><h3 id="使用json-dump-和json-load"><a href="#使用json-dump-和json-load" class="headerlink" title="使用json.dump()和json.load()"></a>使用json.dump()和json.load()</h3><h4 id="json-dump"><a href="#json-dump" class="headerlink" title="json.dump()"></a>json.dump()</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">import json</div><div class="line">numbers = [2,3,4,5,6,7]</div><div class="line"></div><div class="line">filename = <span class="string">'numbers.json'</span></div><div class="line">with open(filename,<span class="string">'w'</span>) as f_obj:</div><div class="line">json.dump(numbers,f_obj)</div></pre></td></tr></table></figure><h4 id="json-load"><a href="#json-load" class="headerlink" title="json.load()"></a>json.load()</h4><p>OK 那我们接下来看看json.load()是怎么用的把<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> import json</div><div class="line"> filename = <span class="string">'numbers.json'</span></div><div class="line"> with open(filename) as f_obj:</div><div class="line">numbers = json.load(f_obj)</div><div class="line"> <span class="built_in">print</span>(numbers)</div><div class="line"><span class="comment">#与dump正好相反，load()是将文件加载到内存中</span></div></pre></td></tr></table></figure></p><h3 id="未能解决的问题"><a href="#未能解决的问题" class="headerlink" title="未能解决的问题"></a>未能解决的问题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"> import json</div><div class="line"> filename = <span class="string">"username.json"</span></div><div class="line"> try:</div><div class="line">with open(filename) as file_obj:</div><div class="line">username = json.load(file_obj)</div><div class="line"> except FileNotFoundError:</div><div class="line">username = input(<span class="string">"What is your name: "</span>)</div><div class="line">with open(filename,<span class="string">'w'</span>) as file_obj:</div><div class="line">json.dump(username,file_obj)</div><div class="line"> <span class="keyword">else</span>:</div><div class="line"><span class="built_in">print</span>(<span class="string">"Welcome back, "</span> + username + <span class="string">"!"</span>)</div><div class="line"><span class="comment">#Subline Text2中无法运行input(),如果要运行，需要在终端中运行</span></div></pre></td></tr></table></figure><h1 id="重构"><a href="#重构" class="headerlink" title="重构"></a>重构</h1><p>核心要点：每一个函数都执行单一而清晰的任务</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;OK 现在我们来学存储数据&lt;/p&gt;
&lt;h1 id=&quot;存储数据&quot;&gt;&lt;a href=&quot;#存储数据&quot; class=&quot;headerlink&quot; title=&quot;存储数据&quot;&gt;&lt;/a&gt;存储数据&lt;/h1&gt;&lt;h2 id=&quot;使用模块json来存储数据&quot;&gt;&lt;a href=&quot;#使用模块json来存储
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python初体验（八）</title>
    <link href="http://yoursite.com/2017/09/20/python%E5%88%9D%E4%BD%93%E9%AA%8C%EF%BC%88%E5%85%AB%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/20/python初体验（八）/</id>
    <published>2017-09-20T06:02:53.000Z</published>
    <updated>2017-09-20T06:53:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天学习python中的异常！ ^…^</p><h1 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h1><h2 id="ZeroDivisionError"><a href="#ZeroDivisionError" class="headerlink" title="ZeroDivisionError"></a>ZeroDivisionError</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"> try:</div><div class="line"><span class="built_in">print</span>(5/0)</div><div class="line"> except ZeroDivisionError:</div><div class="line"><span class="built_in">print</span>(<span class="string">"You can`t divide by zero!"</span>)</div><div class="line"> <span class="keyword">else</span>:</div><div class="line"><span class="built_in">print</span>(<span class="string">"Hello,you are succssful!"</span>)</div><div class="line"> finally:</div><div class="line"><span class="built_in">print</span>(<span class="string">"this is finally"</span>)</div><div class="line"><span class="comment">#稍微解释一下：try模块中的代码执行成功的话，会执行else中和finally，try中代码执行失败的话，会执行except中的代#码和finally，也就是说，finally的代码都会执行，这和JAVA中的异常比较相似</span></div></pre></td></tr></table></figure><h2 id="FileNotFoundError异常"><a href="#FileNotFoundError异常" class="headerlink" title="FileNotFoundError异常"></a>FileNotFoundError异常</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"> filename = <span class="string">"alice.txt"</span></div><div class="line"> try:</div><div class="line">with open(filename) as f_obj:</div><div class="line">contents = f_obj.read()</div><div class="line"> except FileNotFoundError:</div><div class="line"><span class="built_in">print</span>(<span class="string">"No Such file!"</span>)</div><div class="line"> <span class="keyword">else</span>:</div><div class="line"><span class="built_in">print</span>(<span class="string">"file has been read"</span>)</div><div class="line"> finally:</div><div class="line"><span class="built_in">print</span>(<span class="string">"finish"</span>)</div><div class="line"><span class="comment">#需要注意的是，一定要注意缩进级别，否则，你找错误的时候会非常痛苦</span></div><div class="line"><span class="comment">#遇到一个比较烦心的事儿，就是python3 在 Sumline Text 2中默认的输出编码是</span></div><div class="line"> import sys</div><div class="line"> <span class="built_in">print</span>(sys.stdout.encoding)</div><div class="line"> &gt;&gt;US-ASCII</div><div class="line"> 我会在后续附上该问题的解决方案的</div><div class="line"><span class="comment">#再说明一下文档字符串的注释方式”””注释””” 两边各有三个双引号</span></div></pre></td></tr></table></figure><h2 id="失败时一声不吭"><a href="#失败时一声不吭" class="headerlink" title="失败时一声不吭"></a>失败时一声不吭</h2><p> 在except中使用pass，它会使python失败时一声不吭（~.~）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天学习python中的异常！ ^…^&lt;/p&gt;
&lt;h1 id=&quot;异常&quot;&gt;&lt;a href=&quot;#异常&quot; class=&quot;headerlink&quot; title=&quot;异常&quot;&gt;&lt;/a&gt;异常&lt;/h1&gt;&lt;h2 id=&quot;ZeroDivisionError&quot;&gt;&lt;a href=&quot;#ZeroDivis
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
</feed>
