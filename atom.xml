<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>EWSUN</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-10-09T08:04:33.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>EtanWatson</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>递归</title>
    <link href="http://yoursite.com/2017/10/09/%E9%80%92%E5%BD%92/"/>
    <id>http://yoursite.com/2017/10/09/递归/</id>
    <published>2017-10-09T07:08:10.000Z</published>
    <updated>2017-10-09T08:04:33.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h1><p>恨它的，爱它的以及恨了几年有爱上它的（比较经典，记下了）</p><h2 id="while与递归的对比"><a href="#while与递归的对比" class="headerlink" title="while与递归的对比"></a>while与递归的对比</h2><p>伪代码实现<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#while 实现</span></div><div class="line">def look_for_key(main_box):</div><div class="line">pile = main_box.make_a_pile_to_look_through()</div><div class="line"><span class="keyword">while</span> pile is not empty:</div><div class="line">box = pile.grab_a_box()</div><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> box:</div><div class="line"><span class="keyword">if</span> item.is_a_box():</div><div class="line">pile.append(item)</div><div class="line"><span class="keyword">elif</span> item.is_a_key():</div><div class="line"><span class="built_in">print</span>(“found the key!”)</div><div class="line"><span class="comment">#递归实现</span></div><div class="line">def look_for_key(box):</div><div class="line"><span class="keyword">for</span> item <span class="keyword">in</span> box:</div><div class="line"><span class="keyword">if</span> item.is_a_box():</div><div class="line">look_for_key(item)</div><div class="line"><span class="keyword">elif</span> item.is_a_key():</div><div class="line"><span class="built_in">print</span>(“found the key!”)</div></pre></td></tr></table></figure></p><p>#这两种方法的作用相同，第二种方法更清晰。递归只是让解决方案更清晰 ，并没有性能上的优势，实际上，在有些情况下，使用循环的性能更好。<br>再引入一句大神说的话：Loops may achieve a performance gain for your program. Recursion may achieve a performance gain for your programmer. Choose which is more important in your situation!</p><h2 id="基线条件和递归条件"><a href="#基线条件和递归条件" class="headerlink" title="基线条件和递归条件"></a>基线条件和递归条件</h2><p>每个递归函数都有两个部分：基线条件和递归条件<br>递归条件指的是函数调用自己，而基线条件则指的是函数不在调用自己，从而避免形成无线循环<br>ex：<br>def countdown(i):<br>    print i<br>    countdown(i-1)<br>上述代码，会导致死循环<br>我们来给countdown添加基线条件<br>def countdown(i):<br>    print i</p><pre><code>#基线条件if i &lt;= 0:returnelse:  #递归条件countdown(i-1)</code></pre><h2 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h2><h3 id="调用栈"><a href="#调用栈" class="headerlink" title="调用栈"></a>调用栈</h3><p>def greet(name):<br>    print (“hello, “ + name + “i”)<br>    greet2(name)<br>    print (getting ready to say bye…)<br>    bye()</p><p>#这个函数问候用户，在调用另外两个函数。这两个函数的代码如下。<br>def greet2(name):<br>    print (”how are you, ” + name + ‘?’)<br>    def bye():<br>        print(“ok bye!”)</p><h3 id="递归调用栈"><a href="#递归调用栈" class="headerlink" title="递归调用栈"></a>递归调用栈</h3><h4 id="计算阶乘的"><a href="#计算阶乘的" class="headerlink" title="计算阶乘的"></a>计算阶乘的</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def fact(x):</div><div class="line"><span class="keyword">if</span> x == 1:</div><div class="line"><span class="built_in">return</span> 1</div><div class="line"><span class="keyword">else</span>:</div><div class="line">   <span class="built_in">return</span> x*fact(x-1)</div></pre></td></tr></table></figure><p>使用栈虽然很方便，但是也要付出代价:存储详尽的信息可能占用大量的内存。每个函数调 用都要占用一定的内存，如果栈很高，就意味着计算机存储了大量函数调用的信息。在这种情况 下，你有两种选择。<br> 重新编写代码，转而使用循环。<br> 使用尾递归。这是一个高级递归主题，不在本书的讨论范围内。另外，并非所有的语言<br>都支持尾递归。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p> 递归指的是调用自己的函数。<br> 每个递归函数都有两个条件:基线条件和递归条件。  栈有两种操作:压入和弹出。<br> 所有函数调用都进入调用栈。<br> 调用栈可能很长，这将占用大量的内存。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;递归&quot;&gt;&lt;a href=&quot;#递归&quot; class=&quot;headerlink&quot; title=&quot;递归&quot;&gt;&lt;/a&gt;递归&lt;/h1&gt;&lt;p&gt;恨它的，爱它的以及恨了几年有爱上它的（比较经典，记下了）&lt;/p&gt;
&lt;h2 id=&quot;while与递归的对比&quot;&gt;&lt;a href=&quot;#while与
      
    
    </summary>
    
      <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
      <category term="algorithm" scheme="http://yoursite.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>选择排序</title>
    <link href="http://yoursite.com/2017/10/09/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/"/>
    <id>http://yoursite.com/2017/10/09/选择排序/</id>
    <published>2017-10-09T01:39:40.000Z</published>
    <updated>2017-10-09T07:07:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h1><h2 id="链表和数组"><a href="#链表和数组" class="headerlink" title="链表和数组"></a>链表和数组</h2><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>优点：读取的时间复杂度为O(1) 常量时间<br>缺点：插入的时间复杂度为O(n) 线性时间</p><h3 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h3><p>中间插入，删除</p><h2 id="选择排序-1"><a href="#选择排序-1" class="headerlink" title="选择排序"></a>选择排序</h2><h3 id="需要检查的元素越来越少"><a href="#需要检查的元素越来越少" class="headerlink" title="需要检查的元素越来越少"></a>需要检查的元素越来越少</h3><p>第一次需要检查n个元素，但随后检查的元素 数依次为n  1, n – 2, …, 2和1。平均每次检查的元素数为1/2 × n，因此运行时间为O(n × 1/2 × n)。</p><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def findSmallest(arr):</div><div class="line"><span class="comment"># 存储最小的值</span></div><div class="line">smallest = arr[0]</div><div class="line"><span class="comment"># 存储最小元素的索引</span></div><div class="line">smallest_index = 0</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> reange(1, len(arr)):</div><div class="line"><span class="keyword">if</span> arr[i] &lt; smallest:</div><div class="line">smallest = arr[i]</div><div class="line">smallest_index = i</div><div class="line"><span class="built_in">return</span> smallest_index</div><div class="line"></div><div class="line"><span class="comment">#现在可以使用这个函数来编写选择排序算法了</span></div><div class="line">def selectionSort(arr):</div><div class="line">newArr = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</div><div class="line">smallest = findSmallest(arr)    <span class="comment">#找出数组中最小的元素，</span></div><div class="line"><span class="comment">#并将其加入到新数组中</span></div><div class="line">newArr.append(arr.pop(smallest))</div><div class="line"><span class="built_in">return</span> newArr</div></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p> 计算机内存犹如一大堆抽屉。<br> 需要存储多个元素时，可使用数组或链表。<br> 数组的元素都在一起。<br> 链表的元素是分开的，其中每个元素都存储了下一个元素的地址。<br> 数组的读取速度很快。<br> 链表的插入和删除速度很快。<br> 在同一个数组中，所有元素的类型都必须相同(都为int、double等)。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;选择排序&quot;&gt;&lt;a href=&quot;#选择排序&quot; class=&quot;headerlink&quot; title=&quot;选择排序&quot;&gt;&lt;/a&gt;选择排序&lt;/h1&gt;&lt;h2 id=&quot;链表和数组&quot;&gt;&lt;a href=&quot;#链表和数组&quot; class=&quot;headerlink&quot; title=&quot;链表和数组&quot;&gt;
      
    
    </summary>
    
      <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
      <category term="algorithm" scheme="http://yoursite.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>二分查找</title>
    <link href="http://yoursite.com/2017/10/09/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"/>
    <id>http://yoursite.com/2017/10/09/二分查找/</id>
    <published>2017-10-09T00:27:35.000Z</published>
    <updated>2017-10-09T01:37:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h1><h2 id="二分查找的工作原理"><a href="#二分查找的工作原理" class="headerlink" title="二分查找的工作原理"></a>二分查找的工作原理</h2><p>你的目标是以最少的次数猜到这个数字。你每次猜测后，我会说小了、大了或对了。 假设你从1开始依次往上猜，猜测过程会是这样。（一种糟糕的猜数法）<br>一种好的猜数法，从50开始。如果小了，接下来猜75……,不管是什么数，在第七次一定能猜到<br>一般而言，对于包含n个元素的列表，用二分查找最多需要log2n步，而简单查找最多需要n步。<br>注：仅当列表有序的时候，二分查找才管用</p><h3 id="代码实现（python-2-7）"><a href="#代码实现（python-2-7）" class="headerlink" title="代码实现（python 2.7）"></a>代码实现（python 2.7）</h3><pre><code class="bash"><span class="comment">#范围</span>low = 0heigh = len(list) - 1<span class="comment">#每次检查的中间的元素</span>mid = (low + heigh) / 2guess = list[mid]<span class="comment">#如果数猜小了，就相应的修改low</span><span class="keyword">if</span> guess &lt; item:    low = mid + 1<span class="comment">#如果数猜大了，就修改heigh</span><span class="comment">#完整代码如下</span>def binary_search(list, item)    low = 0    heigh = len(list) - 1    <span class="keyword">while</span> low &lt;= heigh:        mid = (low + heigh)        guess = list[mid]        <span class="keyword">if</span> guess == item:     <span class="comment">#放在第一个判断可以减少下面的操作</span>            <span class="built_in">return</span> mid        <span class="keyword">if</span> guess &lt; item:            low = mid + 1        <span class="keyword">else</span>:            heigh = mid - 1    <span class="built_in">return</span> None</code></pre><h2 id="运行时间"><a href="#运行时间" class="headerlink" title="运行时间"></a>运行时间</h2><p>最多需要猜测的次数与列表长度相同，这被称为线性 时间(linear time)。<br>二分查找的运行时间为对数时间(或log时间)</p><h2 id="大O表示法"><a href="#大O表示法" class="headerlink" title="大O表示法"></a>大O表示法</h2><h3 id="运算时间以不同的速度增加"><a href="#运算时间以不同的速度增加" class="headerlink" title="运算时间以不同的速度增加"></a>运算时间以不同的速度增加</h3><pre><code>简单查找          二分查找</code></pre><p>100个元素                100毫秒          7毫秒<br>10000个元素            10秒        14毫秒<br>1000000000个元素        11天        32毫秒<br>注：以上表示的是至多查找时间<br>有鉴于此，仅知道算法 需要多长时间才能运行完毕还不够，还需知道运行时间如何随列表增长 而增加。这正是大O表示法的用武之地。</p><h3 id="理解不同的大O运行时间"><a href="#理解不同的大O运行时间" class="headerlink" title="理解不同的大O运行时间"></a>理解不同的大O运行时间</h3><p>O(n) 与 O（log n）  注：非特殊说明，均以2为底</p><h3 id="大O表示法指出了最糟糕情况下的运行时间"><a href="#大O表示法指出了最糟糕情况下的运行时间" class="headerlink" title="大O表示法指出了最糟糕情况下的运行时间"></a>大O表示法指出了最糟糕情况下的运行时间</h3><h3 id="一些常见的大O运行时间"><a href="#一些常见的大O运行时间" class="headerlink" title="一些常见的大O运行时间"></a>一些常见的大O运行时间</h3><p>O(log n)，也叫对数时间，这样的算法包括二分查找。<br>O(n)，也叫线性时间，这样的算法包括简单查找。<br>O(n*log n)，这样的算法包括第4章将介绍的快速排序——一种速度较快的排序算法<br>O(n^2)，这样的算法包括第2章将介绍的选择排序——一种速度较慢的排序算法。<br>O(n!)，这样的算法包括接下来将介绍的旅行商问题的解决方案——一种非常慢的算法。</p><p>主要启示：<br>算法的速度并非时间，二十操作数的增速<br>谈论算法的速度时，我们说的是随着输入增加，其运行时间将以什么样的速度增加。<br>算法的运行时间用大O表示法表示<br>O(logn)比O(n)快，当需要搜索的元素越多时，前者比后者快的越多</p><h3 id="旅行商问题"><a href="#旅行商问题" class="headerlink" title="旅行商问题"></a>旅行商问题</h3><p>有一位旅行商。 他需要前往5个城市。<br>这位旅行商(姑且称之为Opus吧)要前往这5个城市，同时要确保旅程最短。为此，可考虑 前往这些城市的各种可能顺序。<br>对于每种顺序，他都计算总旅程，再挑选出旅程最短的路线。5个城市有120种不同的排列方 式。因此，在涉及5个城市时，解决这个问题需要执行120次操作。涉及6个城市时，需要执行720 次操作(有720种不同的排列方式)。涉及7个城市时，需要执行5040次操作!故，可以推出，涉及n个城市时，需要执行n!(n的阶乘)次操作才能计算出结果。如果涉及的城市 数超过100，根本就不能在合理的时间内计算出结果——等你计算出结果，太阳都没了。O(n!),由于目前还没有更巧妙的算法，所以我们能做的只是去找出近似答案。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>二分查找的速度比简单查找快得多。<br>O(log n)比O(n)快。需要搜索的元素越多，前者比后者就快得越多。  算法运行时间并不以秒为单位。<br>算法运行时间是从其增速的角度度量的。<br>算法运行时间用大O表示法表示。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;二分查找&quot;&gt;&lt;a href=&quot;#二分查找&quot; class=&quot;headerlink&quot; title=&quot;二分查找&quot;&gt;&lt;/a&gt;二分查找&lt;/h1&gt;&lt;h2 id=&quot;二分查找的工作原理&quot;&gt;&lt;a href=&quot;#二分查找的工作原理&quot; class=&quot;headerlink&quot; title=
      
    
    </summary>
    
      <category term="algorithm" scheme="http://yoursite.com/categories/algorithm/"/>
    
    
      <category term="algorithm" scheme="http://yoursite.com/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>python收集（图像识别与文字处理）</title>
    <link href="http://yoursite.com/2017/10/08/python%E6%94%B6%E9%9B%86%EF%BC%88%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%96%87%E5%AD%97%E5%A4%84%E7%90%86%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/08/python收集（图像识别与文字处理）/</id>
    <published>2017-10-08T13:29:13.000Z</published>
    <updated>2017-10-08T14:12:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像识别与文字处理"><a href="#图像识别与文字处理" class="headerlink" title="图像识别与文字处理"></a>图像识别与文字处理</h1><p>将图像翻译成文字一般被称为光学文字识别（Optical Character Recognition，OCR）</p><h2 id="OCR库概述"><a href="#OCR库概述" class="headerlink" title="OCR库概述"></a>OCR库概述</h2><p>重点介绍两个库：Pillow(<a href="http://pillow.readthedocs.org/installation.html" target="_blank" rel="external">http://pillow.readthedocs.org/installation.html</a> )<br>        Tesseract(<a href="https://pypi.python.org/pypi/pytesseract" target="_blank" rel="external">https://pypi.python.org/pypi/pytesseract</a>)<br>或者使用pip进行安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; pip3 install pillow</div><div class="line">&gt;&gt; pip3 install pytesseract</div></pre></td></tr></table></figure></p><h3 id="Pillow"><a href="#Pillow" class="headerlink" title="Pillow"></a>Pillow</h3><p>Pillow也可以轻松的地导入代码，并通过大量的过滤，修饰甚至像素级的变换操作处理图片<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">from  PIL import Image, ImageFilter</div><div class="line"></div><div class="line">kitten = Image.open(<span class="string">"header.png"</span>)</div><div class="line">blurryKitten = kitten.filter(ImageFilter.GaussianBlur)</div><div class="line">blurryKitten.save(<span class="string">'header_blurred.png'</span>)</div><div class="line">blurryKitten.show()</div></pre></td></tr></table></figure></p><p>经过处理的图片会变的非常模糊<br>Pillow 还可以完成许多复杂的图像处理工作。更多的信息，请查看 Pillow 文档 (<a href="http://pillow.readthedocs.org/)。" target="_blank" rel="external">http://pillow.readthedocs.org/)。</a></p><h3 id="Tesseract"><a href="#Tesseract" class="headerlink" title="Tesseract"></a>Tesseract</h3><p>Tesseract 是目前公认最优秀、最精确的开源 OCR 系统。(由Google赞助)<br>高灵活性：通过训练，可以识别除任何字体<br>Tesseract是一个python的命令行工具（不是通过import语句）<br>安装Tesseract（安装以后，要用tesseract命令在Python的外面运行）<br>Mac OS X系统下：<br>如果没有安装Homebrew，先安装Homebrew</p><blockquote><p>$ruby -e “$(curl -fsSL <a href="https://raw.githubusercontent.com/Homebrew/" target="_blank" rel="external">https://raw.githubusercontent.com/Homebrew/</a> \ install/master/install)”<br>$brew install tesseract<br>为了能让Tesseract知道训练的数据文件存储在哪里，我们需要配置环境变量<br>$export TESSDATA_PREFIX=/usr/local/share/</p></blockquote><h3 id="NumPy-训练字符和字体时候，会用到它"><a href="#NumPy-训练字符和字体时候，会用到它" class="headerlink" title="NumPy(训练字符和字体时候，会用到它)"></a>NumPy(训练字符和字体时候，会用到它)</h3><p>在机器学习实战的文章里面有用到</p><blockquote><p>pip install numpy</p></blockquote><h2 id="处理格式规范的文字"><a href="#处理格式规范的文字" class="headerlink" title="处理格式规范的文字"></a>处理格式规范的文字</h2><p>通常规范文字的特点<br>• 使用一个标准字体(不包含手写体、草书，或者十分“花哨的”字体) • 虽然被复印或拍照，字体还是很清晰，没有多余的痕迹或污点<br>• 排列整齐，没有歪歪斜斜的字<br>• 没有超出图片范围，也没有残缺不全，或紧紧贴在图片的边缘</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;图像识别与文字处理&quot;&gt;&lt;a href=&quot;#图像识别与文字处理&quot; class=&quot;headerlink&quot; title=&quot;图像识别与文字处理&quot;&gt;&lt;/a&gt;图像识别与文字处理&lt;/h1&gt;&lt;p&gt;将图像翻译成文字一般被称为光学文字识别（Optical Character Reco
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据采集（采集JavaScript）</title>
    <link href="http://yoursite.com/2017/10/07/python%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%EF%BC%88%E9%87%87%E9%9B%86JavaScript%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/07/python数据采集（采集JavaScript）/</id>
    <published>2017-10-07T06:33:28.000Z</published>
    <updated>2017-10-08T13:54:44.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="采集JavaScript"><a href="#采集JavaScript" class="headerlink" title="采集JavaScript"></a>采集JavaScript</h1><h2 id="常用的JavaScript库"><a href="#常用的JavaScript库" class="headerlink" title="常用的JavaScript库"></a>常用的JavaScript库</h2><h3 id="jQuery"><a href="#jQuery" class="headerlink" title="jQuery"></a>jQuery</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;script src=<span class="string">"http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"</span>&gt;&lt;/ script&gt;</div></pre></td></tr></table></figure><h3 id="Google-Analytics"><a href="#Google-Analytics" class="headerlink" title="Google Analytics"></a>Google Analytics</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">&lt;!-- Google Analytics --&gt;&lt;script <span class="built_in">type</span>=<span class="string">"text/javascript"</span>&gt;</div><div class="line">var _gaq = _gaq || []; </div><div class="line">_gaq.push([<span class="string">'_setAccount'</span>, <span class="string">'UA-4591498-1'</span>]); </div><div class="line">_gaq.push([<span class="string">'_setDomainName'</span>, <span class="string">'oreilly.com'</span>]); </div><div class="line">_gaq.push([<span class="string">'_addIgnoredRef'</span>, <span class="string">'oreilly.com'</span>]); </div><div class="line">_gaq.push([<span class="string">'_setSiteSpeedSampleRate'</span>, 50]); </div><div class="line">_gaq.push([<span class="string">'_trackPageview'</span>]);(<span class="function"><span class="title">function</span></span>() &#123; var ga = document.createElement(<span class="string">'script'</span>); </div><div class="line">ga.type = <span class="string">'text/javascript'</span>; </div><div class="line">ga.async = <span class="literal">true</span>; </div><div class="line">ga.src = (<span class="string">'https:'</span> == document.location.protocol ? <span class="string">'https://ssl'</span> : <span class="string">'http://www'</span>) + <span class="string">'.google-analytics.com/ga.js'</span>; </div><div class="line">var s = document.getElementsByTagName(<span class="string">'script'</span>)[0]; s.parentNode.insertBefore(ga, s); &#125;();&lt;/script&gt;</div><div class="line">``` </div><div class="line"><span class="comment">### Google地图</span></div><div class="line">``` bash </div><div class="line">var marker = new google.maps.Marker(&#123;position: new google.maps.LatLng(-25.363882,131.044922), </div><div class="line">map: map,title: <span class="string">'Some marker text'</span>&#125;);</div><div class="line"><span class="comment">### Ajax（异步JavaScript和XML）和动态HTML（有没有用JavaScript控制HTML和CSS元素）</span></div><div class="line">解决方案：</div><div class="line">1.直接从JavaScript代码里采集内容。</div><div class="line">2.用Python的第三方运行JavaScript，直接采集你在浏览器里看到的页面</div><div class="line"><span class="comment">### 在Python中用Selenium执行JavaScript</span></div><div class="line">Selenium是一个强大的网络数据采集工具，它还被广泛用于获取精确的网站快照，因为它们可以直接运行在浏览器上</div><div class="line">phantomJS是一个无头（headless）浏览器</div><div class="line">Selenium库是一个在WebDriver上调用的API。WebDriver有点儿想可以加载网站的浏览器，但是它也可以像BeautifulSoup对象一样用来查找页面元素，与页面上的元素进行交互（发送文本，点击等），以及执行其他动作来运行</div><div class="line">网络爬虫</div><div class="line">下面代码可以获取前面测试页面上Ajax”墙”后面的内容</div><div class="line">``` bash </div><div class="line">from selenium import webdriver</div><div class="line">import time</div><div class="line">diver = webdriver.PhantomJS(executable_path=‘’) <span class="comment">#你的PhantomJS可执行文件的路径</span></div><div class="line">driver.get(<span class="string">"http://pythonscraping.com/pages/javascript/ajaxDemo.html"</span>)</div><div class="line">time.sleep(3)</div><div class="line"><span class="built_in">print</span>(driver.find_element_by_id(‘content’).text)</div><div class="line">driver.close()</div></pre></td></tr></table></figure><p>#这种方法虽然奏效，但是效率还不高，由于页面加载时间的不确定性，所有有很大的弊端</p><h4 id="Selenium的选择器"><a href="#Selenium的选择器" class="headerlink" title="Selenium的选择器"></a>Selenium的选择器</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">driver.find_element_by_id(<span class="string">'content'</span>).text</div><div class="line">driver.find_element_by_css_selector(<span class="string">"#content"</span>)driver.find_element_by_tag_name(<span class="string">"div"</span>)</div><div class="line">driver.find_elements_by_css_selector(<span class="string">"#content"</span>)driver.find_elements_by_css_selector(<span class="string">"div"</span>)</div><div class="line"><span class="comment">#另外，如果你还想用BeautifulSoup来解析网页内容，可以用WebDriver的page_source</span></div><div class="line">pageSource = driver.page_sourcebsObj = BeautifulSoup(pageSource) </div><div class="line"><span class="built_in">print</span>(bsObj.find(id=<span class="string">"content"</span>).get_text())</div><div class="line">``` </div><div class="line"><span class="comment">#### 使用Selenium不断检查，是否完全加载</span></div><div class="line">``` bash </div><div class="line">from selenium.webdriver.common.by import By</div><div class="line">from selenium.webdriver.support.ui import WebDriverWait</div><div class="line">from selenium.webdriver.support import expected_conditions as EC</div><div class="line">from selenium import webdriver</div><div class="line">import time</div><div class="line"></div><div class="line">driver = webdriver.PhantomJS(executable_path=<span class="string">''</span>)</div><div class="line">driver.get(<span class="string">"http://pythonscraping.com/pages/javascript/ajaxDemo.html"</span>)</div><div class="line">try:</div><div class="line">    element = WebDriverWait(driver, 10).until(</div><div class="line">        EC.presence_of_all_elements_located((By.ID, <span class="string">"loadedButton"</span>)))</div><div class="line">finally:</div><div class="line">    <span class="built_in">print</span>(driver.find_element_by_id(<span class="string">"content"</span>).text)</div><div class="line">    driver.close()</div></pre></td></tr></table></figure><p>需要注意的就是WebDriverWait和expected_conditions,这两个模块组合起来，构成了Selenium的隐式等待</p><h5 id="Selenium隐式等待"><a href="#Selenium隐式等待" class="headerlink" title="Selenium隐式等待"></a>Selenium隐式等待</h5><p>1.没有明确的等待时间，但是有最大等待时间</p><p>#DOM触发的状态是用expected_conditions定义的<br>• 弹出一个提示框<br>• 一个元素被选中(比如文本框)<br>• 页面的标题改变了，或者某个文字显示在页面上或者某个元素里 • 一个元素在DOM中变成可见的，或者一个元素从DOM中消失了</p><h5 id="定位器"><a href="#定位器" class="headerlink" title="定位器"></a>定位器</h5><p>大多数的期望条件在使用前都需要你先指定等待的目标元素，定位器是一种抽象的查询语言，用 By 对象表示，可以用于不同的场合，包括创建选择器。<br>一个定位器被用来查找id是loadedButton -&gt; EX：<br>EC.presence_of_element_located((By.ID, “loadedButton”))<br>定位器还可以用来创建选择器，配合WebDriver的find_element函数使用：<br>print(driver.find_element(By.ID, “content”).text)<br>下面这行代码的功能和示例代码中一样：<br>print(driver.find_element_by_id(“content”).text)</p><p>下面是定位器通过By对象进行选择的策略<br>• ID<br>• CLASS_NAME(HTML的class属性)<br>• CSS_SELECTOR:通过 CSS 的 class、id、tag 属性名来查找元素，<br>           用 #idName、.className、tagName 表示。<br>• LINK_TEXT：通过链接文字查找 HTML 的 <a> 标签。例如，如果一个链接的文字是“Next”，就可以 用(By.LINK_TEXT, “Next”)来选择。<br>• PARTIAL_LINK_TEXT：与 LINK_TEXT 类似，只是通过部分链接文字来查找。<br>• NAME：通过 HTML 标签的 name 属性查找。这在处理 HTML 表单时非常方便。<br>• TAG_NAME：通过 HTML 标签的名称查找。<br>• XPATH：用 XPath 表达式(语法在下面介绍)选择匹配的元素。</a></p><h6 id="XPath语法"><a href="#XPath语法" class="headerlink" title="XPath语法"></a>XPath语法</h6><p>在XPath语法中有四个重要概念。<br>一、根节点和非根节点</p><ol><li>/div选择div节点，只有当它是文档的根节点时</li><li>//div选择文档中所有的div节点（包括非根节点）<br>二、通过属性选择节点</li><li>//@href选择带href属性的所有节点</li><li>//a[@href=‘<a href="http://google.com’]选择页面中所有指向Google网站的链接" target="_blank" rel="external">http://google.com’]选择页面中所有指向Google网站的链接</a><br>三、通过位置选择节点</li><li>//a[3]选着文档中的第三个链接</li><li>//table[last()]选择文档中的最后一个表</li><li>//a[positon() &lt; 3]选择文档中的前三个链接<br>四、星号（*）匹配任意字符串或节点，可以在不同条件下使用</li><li>//table/tr/*选择所有表格行tr标签的所有子节点（这很适合选择th和td标签）</li><li>//div[@0]选择带有任意属性的所有div标签<br>更多，请参考微软的XPath语法页面：<a href="https://msdn.microsoft.com/en-us/enus/library/ms256471" target="_blank" rel="external">https://msdn.microsoft.com/en-us/enus/library/ms256471</a></li></ol><h2 id="处理重定向"><a href="#处理重定向" class="headerlink" title="处理重定向"></a>处理重定向</h2><h3 id="客户端重定向"><a href="#客户端重定向" class="headerlink" title="客户端重定向"></a>客户端重定向</h3><p>是在服务器将页面内容发送到浏览器之前，由浏览器执行 JavaScript 完成的 页面跳转，而不是服务器完成的跳转。<br>在网络采集是的差异：（客户端重定向和服务端重定向）<br>根据具体情况，<br>服务器端重定向一般都可以轻松地通过 Python 的 urllib 库解决，不需要使用 Selenium (更多的介绍请参考第 3 章)。客户端重定向却不能这样处理，除非你有工具可以执行<br>JavaScript。<br>Selenium的问题在于怎么识别一个页面已经完成重定向了</p><h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>首先从页面开始加载时就”监视”DOM中的一个元素，然后重复调用这个元素直到Selenium抛出一个StaleElementRefereceException异常，也就是说，元素不在页面的DOM里了，说明这时网站已经跳转：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">from selenium import webdriver</div><div class="line">import time</div><div class="line">from selenium.webdriver.remote.webelement import WebElement</div><div class="line">from selenium.common.exceptions import StaleElementReferenceException</div><div class="line"></div><div class="line">def waitForLoad(driver):</div><div class="line">    elem = driver.find_element_by_tag_name(<span class="string">"html"</span>)</div><div class="line">    count = 0</div><div class="line">    <span class="keyword">while</span> True:</div><div class="line">        count += 1</div><div class="line">        <span class="keyword">if</span> count &gt; 20:</div><div class="line">            <span class="built_in">print</span>(<span class="string">"Timing out after 10 secods and returning"</span>)</div><div class="line">            <span class="built_in">return</span></div><div class="line">        time.sleep(.5)</div><div class="line">        try:</div><div class="line">            elem == driver.find_element_by_tag_name(<span class="string">"html"</span>)</div><div class="line">        except StaleElementReferenceException:</div><div class="line">            <span class="built_in">return</span></div><div class="line">driver = webdriver.PhantomJS(executable_path=“/****/**/MachineLearning/phantomjs-2.1.1-macosx/bin/phantomjs<span class="string">")</span></div><div class="line"><span class="string">driver.get("</span>http://pythonscraping.com/pages/javascript/redirectDemo1.html<span class="string">")</span></div><div class="line"><span class="string">waitForLoad(driver)</span></div><div class="line"><span class="string">print(driver.page_source)</span></div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;采集JavaScript&quot;&gt;&lt;a href=&quot;#采集JavaScript&quot; class=&quot;headerlink&quot; title=&quot;采集JavaScript&quot;&gt;&lt;/a&gt;采集JavaScript&lt;/h1&gt;&lt;h2 id=&quot;常用的JavaScript库&quot;&gt;&lt;a href=&quot;
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（穿越网页表单与登录窗口进行采集）</title>
    <link href="http://yoursite.com/2017/10/07/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E7%A9%BF%E8%B6%8A%E7%BD%91%E9%A1%B5%E8%A1%A8%E5%8D%95%E4%B8%8E%E7%99%BB%E5%BD%95%E7%AA%97%E5%8F%A3%E8%BF%9B%E8%A1%8C%E9%87%87%E9%9B%86%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/07/python数据收集（穿越网页表单与登录窗口进行采集）/</id>
    <published>2017-10-07T02:47:05.000Z</published>
    <updated>2017-10-07T06:32:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="穿越网页表单与登录窗口进行采集"><a href="#穿越网页表单与登录窗口进行采集" class="headerlink" title="穿越网页表单与登录窗口进行采集"></a>穿越网页表单与登录窗口进行采集</h1><p>如何获取登录窗口背后的信息呢？？？这一节我们重点介绍POST方法，即把消息推送给网络服务器进行存储和分析，像网站搞得URL链接可以帮助用户发送GET请求一样，HTML表单可以帮助用户发出POST请求</p><h2 id="Python-Requests-库（http-www-python-requests-org-）"><a href="#Python-Requests-库（http-www-python-requests-org-）" class="headerlink" title="Python Requests 库（http://www.python-requests.org/）"></a>Python Requests 库（<a href="http://www.python-requests.org/）" target="_blank" rel="external">http://www.python-requests.org/）</a></h2><p>是一个擅长处理那些复杂的HTTP请求。cookie，header（响应头和请求头）等内容的Python第三方库。<br>安装：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt; pip(pip3) install Requests</div></pre></td></tr></table></figure></p><h2 id="提交一个基本表单"><a href="#提交一个基本表单" class="headerlink" title="提交一个基本表单"></a>提交一个基本表单</h2><p>注：如果你想模拟表单提交数据的行为，你就需要保证你的变量名称与字段名称是一一对应的</p><h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line"></div><div class="line">params = &#123;<span class="string">'firstname'</span>: <span class="string">'Ryan'</span>, <span class="string">'listname'</span>: <span class="string">'Mitchell'</span>&#125;</div><div class="line">r = requests.post(<span class="string">"http://pythonscraping.com/files/processing.php"</span>, data=params)</div><div class="line"><span class="built_in">print</span>(r.text)</div><div class="line"><span class="comment">#在大多数情况下，你只需关注两件事：</span></div><div class="line"><span class="comment">#• 你想提交数据的字段名称（name字段）(在这个例子中是email_addr)</span><span class="comment">#• 表单的action属性，也就是表单提交后网站会显示的页面(在这个例子中是http://post.oreilly.com</span></div><div class="line"><span class="comment">#/client/o/oreilly/forms/quicksignup.cgi)</span></div><div class="line"><span class="comment">#运行代码示例</span></div><div class="line">import requestsparams = &#123;<span class="string">'email_addr'</span>: <span class="string">'ryan.e.mitchell@gmail.com'</span>&#125;r = requests.post(<span class="string">"http://post.oreilly.com/client/o/oreilly/forms/                        quicksignup.cgi"</span>, data=params)<span class="built_in">print</span>(r.text)</div></pre></td></tr></table></figure><h2 id="单选按钮、复选框和其他输入"><a href="#单选按钮、复选框和其他输入" class="headerlink" title="单选按钮、复选框和其他输入"></a>单选按钮、复选框和其他输入</h2><p>无论html提供了多么复杂的控件，仍然只有亮剑事是需要关注的：字段名称（name）和值（比较复杂，有可能是通过JavaScript生成的，<br>而取色器有类似于#F03030这样的值）</p><h3 id="跟踪GET请求获取值"><a href="#跟踪GET请求获取值" class="headerlink" title="跟踪GET请求获取值"></a>跟踪GET请求获取值</h3><p>get请求的值一般会在URL中体现，类似于：<a href="http://domainname.com?thing1=foo&amp;thing2=bar" target="_blank" rel="external">http://domainname.com?thing1=foo&amp;thing2=bar</a></p><p>你就会明白这个请求就是下面这种表单：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;form method=<span class="string">"GET"</span> action=<span class="string">"someProcessor.php"</span>&gt;&lt;input <span class="built_in">type</span>=<span class="string">"someCrazyInputType"</span> name=<span class="string">"thing1"</span> value=<span class="string">"foo"</span> /&gt; &lt;input <span class="built_in">type</span>=<span class="string">"anotherCrazyInputType"</span> name=<span class="string">"thing2"</span> value=<span class="string">"bar"</span> /&gt; &lt;input <span class="built_in">type</span>=<span class="string">"submit"</span> value=<span class="string">"Submit"</span> /&gt;&lt;/form&gt;</div></pre></td></tr></table></figure></p><p>对应的python参数就是：<br>{‘thing1’:’foo’, ‘thing2’:’bar’}</p><h2 id="提交文件和图像"><a href="#提交文件和图像" class="headerlink" title="提交文件和图像"></a>提交文件和图像</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">files = &#123;<span class="string">'uploadFile'</span>: open(<span class="string">'./files/header.png'</span>, <span class="string">'rb'</span>)&#125;</div><div class="line">r = requests.post(<span class="string">"http://pythonscraping.com/pages/processing2.php"</span>,files=files)</div><div class="line"></div><div class="line"><span class="built_in">print</span>(r.text)</div></pre></td></tr></table></figure><h2 id="处理登录和cookie"><a href="#处理登录和cookie" class="headerlink" title="处理登录和cookie"></a>处理登录和cookie</h2><p>问题：你可以一整天只提交一次登录表单，但是如果你没有一直关注表单后来回传给你的那个cookie，那么一段时间以后再次访问新页面<br>时，你的登录状态就会丢失，需要重新登录<br>Requests库跟踪cookie同样简单：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">params = &#123;‘username’:’Ryan’,’password’:’password’&#125;</div><div class="line">r = requests.post(<span class="string">"http://pythonscraping.com/pages/cookies/welcome.php"</span>, params) </div><div class="line"><span class="built_in">print</span>(<span class="string">"Cookie is set to:"</span>)<span class="built_in">print</span>(r.cookies.get_dict())<span class="built_in">print</span>(<span class="string">"-----------"</span>)<span class="built_in">print</span>(<span class="string">"Going to profile page..."</span>)r = requests.get(<span class="string">"http://pythonscraping.com/pages/cookies/profile.php"</span>,                      cookies=r.cookies)<span class="built_in">print</span>(r.text)</div></pre></td></tr></table></figure></p><h3 id="使用session"><a href="#使用session" class="headerlink" title="使用session"></a>使用session</h3><p>如果你面对的网站比较复杂，它经常暗自调整cookie，或者如果你从一开始就完全不想要用cookie，该如何处理呢</p><h3 id="HTTP基本接入认证"><a href="#HTTP基本接入认证" class="headerlink" title="HTTP基本接入认证"></a>HTTP基本接入认证</h3><p>RRequests库有一个auth模块专门用来处理HTTP认证：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">from requests.auth import AuthBase</div><div class="line">from request.auth import HTTPBasicAuth</div><div class="line"></div><div class="line">auth = HTTPBasicAuth(<span class="string">'ryan'</span>, <span class="string">'password'</span>)     r = requests.post(url=<span class="string">"http://pythonscraping.com/pages/auth/login.php"</span>, auth=auth)<span class="built_in">print</span>(r.text)</div></pre></td></tr></table></figure></p><h2 id="其他表单问题（CAPTCHA-验证码）"><a href="#其他表单问题（CAPTCHA-验证码）" class="headerlink" title="其他表单问题（CAPTCHA:验证码）"></a>其他表单问题（CAPTCHA:验证码）</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;穿越网页表单与登录窗口进行采集&quot;&gt;&lt;a href=&quot;#穿越网页表单与登录窗口进行采集&quot; class=&quot;headerlink&quot; title=&quot;穿越网页表单与登录窗口进行采集&quot;&gt;&lt;/a&gt;穿越网页表单与登录窗口进行采集&lt;/h1&gt;&lt;p&gt;如何获取登录窗口背后的信息呢？？？这
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（自然语言处理）</title>
    <link href="http://yoursite.com/2017/10/04/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/04/python数据收集（自然语言处理）/</id>
    <published>2017-10-04T03:08:41.000Z</published>
    <updated>2017-10-05T03:30:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="自然语言的处理"><a href="#自然语言的处理" class="headerlink" title="自然语言的处理"></a>自然语言的处理</h1><h2 id="概括数据"><a href="#概括数据" class="headerlink" title="概括数据"></a>概括数据</h2><p>前面已经介绍过了n-gram模型，即n个单词长度的词组<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import string</div><div class="line">import operator</div><div class="line"></div><div class="line">def cleanInput(input):</div><div class="line">    input = re.sub(<span class="string">'\n+'</span>,<span class="string">" "</span>,input).lower()</div><div class="line">    input = re.sub(<span class="string">'\[[0-9]*\]'</span>,<span class="string">""</span>,input)</div><div class="line">    input = re.sub(<span class="string">' +'</span>, <span class="string">" "</span>, input)</div><div class="line">    input = bytes(input, <span class="string">"UTF-8"</span>)</div><div class="line">    input = input.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>)</div><div class="line">    cleanInput = []</div><div class="line">    input = input.split(<span class="string">' '</span>)</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> input:</div><div class="line">        item = item.strip(string.punctuation)</div><div class="line">        <span class="keyword">if</span> len(item) &gt; 1 or (item.lower() == <span class="string">'a'</span> or item.lower() == <span class="string">'i'</span>):</div><div class="line">            cleanInput.append(item)</div><div class="line">    <span class="built_in">return</span> cleanInput</div><div class="line"></div><div class="line">def ngrams(input, n):</div><div class="line">    input = cleanInput(input)</div><div class="line">    output = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input)-n+1):</div><div class="line">        ngramTemp =<span class="string">" "</span>.join(input[i:i+n])</div><div class="line">        <span class="keyword">if</span> ngramTemp not <span class="keyword">in</span> output:</div><div class="line">            output[ngramTemp] = 0</div><div class="line">        output[ngramTemp] += 1</div><div class="line">    <span class="built_in">return</span> output</div><div class="line"></div><div class="line">content = str(urlopen(<span class="string">"http://pythonscraping.com/files/inaugurationSpeech.txt"</span>).<span class="built_in">read</span>(),<span class="string">'utf-8'</span>)</div><div class="line">ngrams = ngrams(content, 2)</div><div class="line">sortedNGrams = sorted(ngrams.items(), key=operator.itemgetter(1), reverse=True)</div><div class="line"><span class="built_in">print</span>(sortedNGrams)</div><div class="line"><span class="comment">#结果</span></div><div class="line">&gt;&gt;&gt; (<span class="string">'of the'</span>, 213), (<span class="string">'in the'</span>, 65), (<span class="string">'to the'</span>, 61), (<span class="string">'by the'</span>, 41), (<span class="string">'the constitution'</span>, 34),</div></pre></td></tr></table></figure></p><p>我们会发现，其实像of the，in the ，对我们来讲一点儿都不重要，而 the constitution相对来说就比较重要</p><h3 id="去掉看上去无用的字符"><a href="#去掉看上去无用的字符" class="headerlink" title="去掉看上去无用的字符"></a>去掉看上去无用的字符</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import string</div><div class="line">import operator</div><div class="line"></div><div class="line"><span class="comment">#最常用的5000个单词列表可以免费获取，我们现在提取出前100个</span></div><div class="line"><span class="comment">#返回boolean类型，如果包含，就返回true</span></div><div class="line">def isCommon(ngram):</div><div class="line">    commonWords = [<span class="string">"the"</span>, <span class="string">"be"</span>, <span class="string">"and"</span>, <span class="string">"of"</span>, <span class="string">"a"</span>, <span class="string">"in"</span>, <span class="string">"to"</span>, <span class="string">"have"</span>, <span class="string">"it"</span>,</div><div class="line">             <span class="string">"i"</span>, <span class="string">"that"</span>, <span class="string">"for"</span>, <span class="string">"you"</span>, <span class="string">"he"</span>, <span class="string">"with"</span>, <span class="string">"on"</span>, <span class="string">"do"</span>, <span class="string">"say"</span>, <span class="string">"this"</span>,</div><div class="line">             <span class="string">"they"</span>, <span class="string">"is"</span>, <span class="string">"an"</span>, <span class="string">"at"</span>, <span class="string">"but"</span>,<span class="string">"we"</span>, <span class="string">"his"</span>, <span class="string">"from"</span>, <span class="string">"that"</span>, <span class="string">"not"</span>,</div><div class="line">             <span class="string">"by"</span>, <span class="string">"she"</span>, <span class="string">"or"</span>, <span class="string">"as"</span>, <span class="string">"what"</span>, <span class="string">"go"</span>, <span class="string">"their"</span>,<span class="string">"can"</span>, <span class="string">"who"</span>, <span class="string">"get"</span>,</div><div class="line">             <span class="string">"if"</span>, <span class="string">"would"</span>, <span class="string">"her"</span>, <span class="string">"all"</span>, <span class="string">"my"</span>, <span class="string">"make"</span>, <span class="string">"about"</span>, <span class="string">"know"</span>, <span class="string">"will"</span>,</div><div class="line">             <span class="string">"as"</span>, <span class="string">"up"</span>, <span class="string">"one"</span>, <span class="string">"time"</span>, <span class="string">"has"</span>, <span class="string">"been"</span>, <span class="string">"there"</span>, <span class="string">"year"</span>, <span class="string">"so"</span>,</div><div class="line">             <span class="string">"think"</span>, <span class="string">"when"</span>, <span class="string">"which"</span>, <span class="string">"them"</span>, <span class="string">"some"</span>, <span class="string">"me"</span>, <span class="string">"people"</span>, <span class="string">"take"</span>,</div><div class="line">             <span class="string">"out"</span>, <span class="string">"into"</span>, <span class="string">"just"</span>, <span class="string">"see"</span>, <span class="string">"him"</span>, <span class="string">"your"</span>, <span class="string">"come"</span>, <span class="string">"could"</span>, <span class="string">"now"</span>,</div><div class="line">             <span class="string">"than"</span>, <span class="string">"like"</span>, <span class="string">"other"</span>, <span class="string">"how"</span>, <span class="string">"then"</span>, <span class="string">"its"</span>, <span class="string">"our"</span>, <span class="string">"two"</span>, <span class="string">"more"</span>,</div><div class="line">             <span class="string">"these"</span>, <span class="string">"want"</span>, <span class="string">"way"</span>, <span class="string">"look"</span>, <span class="string">"first"</span>, <span class="string">"also"</span>, <span class="string">"new"</span>, <span class="string">"because"</span>,</div><div class="line">             <span class="string">"day"</span>, <span class="string">"more"</span>, <span class="string">"use"</span>, <span class="string">"no"</span>, <span class="string">"man"</span>, <span class="string">"find"</span>, <span class="string">"here"</span>, <span class="string">"thing"</span>, <span class="string">"give"</span>,</div><div class="line">             <span class="string">"many"</span>, <span class="string">"well"</span>]</div><div class="line"></div><div class="line">    <span class="keyword">if</span> ngram <span class="keyword">in</span> commonWords:</div><div class="line">        <span class="built_in">return</span> True</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="built_in">return</span> False</div><div class="line"></div><div class="line">def cleanInput(input):</div><div class="line">    input = re.sub(<span class="string">'\n+'</span>,<span class="string">" "</span>,input).lower() <span class="comment"># 匹配换行,用空格替换换行符</span></div><div class="line">    input = re.sub(<span class="string">'\[[0-9]*\]'</span>,<span class="string">""</span>,input) <span class="comment"># 剔除类似[1]这样的引用标记</span></div><div class="line">    input = re.sub(<span class="string">' +'</span>, <span class="string">" "</span>, input) <span class="comment">#把连续多个空格替换成一个空格</span></div><div class="line">    input = bytes(input, <span class="string">"UTF-8"</span>)</div><div class="line">    input = input.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>)</div><div class="line">    cleanInput = []</div><div class="line">    input = input.split(<span class="string">' '</span>)</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> input:</div><div class="line">        item = item.strip(string.punctuation)</div><div class="line">        <span class="keyword">if</span> len(item) &gt; 1 or (item.lower() == <span class="string">'a'</span> or item.lower() == <span class="string">'i'</span>):</div><div class="line">            cleanInput.append(item)</div><div class="line">    <span class="built_in">return</span> cleanInput</div><div class="line"></div><div class="line">def ngrams(input, n):</div><div class="line">    input = cleanInput(input)</div><div class="line">    output = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input)-n+1):</div><div class="line">        ngramTemp =<span class="string">" "</span>.join(input[i:i+n]) <span class="comment">#这句话将n-grams拆分成n个元素组成的列表</span></div><div class="line">        <span class="keyword">if</span> isCommon(ngramTemp.split()[0]) or isCommon(ngramTemp.split()[1]):</div><div class="line">            pass</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">if</span> ngramTemp not <span class="keyword">in</span> output:</div><div class="line">                output[ngramTemp] = 0</div><div class="line">            output[ngramTemp] += 1</div><div class="line">    <span class="built_in">return</span> output</div><div class="line"></div><div class="line">content = str(urlopen(<span class="string">"http://pythonscraping.com/files/inaugurationSpeech.txt"</span>).<span class="built_in">read</span>(),<span class="string">'utf-8'</span>)</div><div class="line">ngrams = ngrams(content, 2)</div><div class="line">sortedNGrams = sorted(ngrams.items(), key=operator.itemgetter(1), reverse=True)</div><div class="line"><span class="built_in">print</span>(sortedNGrams)</div></pre></td></tr></table></figure><p>#结果</p><blockquote><blockquote><blockquote><p>[(‘united states’, 10), (‘general government’, 4), (‘executive department’, 4), (‘mr jefferson’, 3), (‘same causes’, 3),……]<br>我的疑惑：在进行”看似无用”的单词过滤的时候，是不是会将有用的单词过滤掉类似the constitution</p><h3 id="通过中心主题词，归纳文章核心"><a href="#通过中心主题词，归纳文章核心" class="headerlink" title="通过中心主题词，归纳文章核心"></a>通过中心主题词，归纳文章核心</h3><p>一种方法是搜索包含每个核心 n-gram 序列的第一句话，这个方法的理论是英语中段落的首句 往往是对后面内容的概述<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import string</div><div class="line">import operator</div><div class="line"></div><div class="line"><span class="comment">#最常用的5000个单词列表可以免费获取，我们现在提取出前100个</span></div><div class="line"><span class="comment">#返回boolean类型，如果包含，就返回true</span></div><div class="line">def isCommon(ngram):</div><div class="line">    commonWords = [<span class="string">"the"</span>, <span class="string">"be"</span>, <span class="string">"and"</span>, <span class="string">"of"</span>, <span class="string">"a"</span>, <span class="string">"in"</span>, <span class="string">"to"</span>, <span class="string">"have"</span>, <span class="string">"it"</span>,</div><div class="line">             <span class="string">"i"</span>, <span class="string">"that"</span>, <span class="string">"for"</span>, <span class="string">"you"</span>, <span class="string">"he"</span>, <span class="string">"with"</span>, <span class="string">"on"</span>, <span class="string">"do"</span>, <span class="string">"say"</span>, <span class="string">"this"</span>,</div><div class="line">             <span class="string">"they"</span>, <span class="string">"is"</span>, <span class="string">"an"</span>, <span class="string">"at"</span>, <span class="string">"but"</span>,<span class="string">"we"</span>, <span class="string">"his"</span>, <span class="string">"from"</span>, <span class="string">"that"</span>, <span class="string">"not"</span>,</div><div class="line">             <span class="string">"by"</span>, <span class="string">"she"</span>, <span class="string">"or"</span>, <span class="string">"as"</span>, <span class="string">"what"</span>, <span class="string">"go"</span>, <span class="string">"their"</span>,<span class="string">"can"</span>, <span class="string">"who"</span>, <span class="string">"get"</span>,</div><div class="line">             <span class="string">"if"</span>, <span class="string">"would"</span>, <span class="string">"her"</span>, <span class="string">"all"</span>, <span class="string">"my"</span>, <span class="string">"make"</span>, <span class="string">"about"</span>, <span class="string">"know"</span>, <span class="string">"will"</span>,</div><div class="line">             <span class="string">"as"</span>, <span class="string">"up"</span>, <span class="string">"one"</span>, <span class="string">"time"</span>, <span class="string">"has"</span>, <span class="string">"been"</span>, <span class="string">"there"</span>, <span class="string">"year"</span>, <span class="string">"so"</span>,</div><div class="line">             <span class="string">"think"</span>, <span class="string">"when"</span>, <span class="string">"which"</span>, <span class="string">"them"</span>, <span class="string">"some"</span>, <span class="string">"me"</span>, <span class="string">"people"</span>, <span class="string">"take"</span>,</div><div class="line">             <span class="string">"out"</span>, <span class="string">"into"</span>, <span class="string">"just"</span>, <span class="string">"see"</span>, <span class="string">"him"</span>, <span class="string">"your"</span>, <span class="string">"come"</span>, <span class="string">"could"</span>, <span class="string">"now"</span>,</div><div class="line">             <span class="string">"than"</span>, <span class="string">"like"</span>, <span class="string">"other"</span>, <span class="string">"how"</span>, <span class="string">"then"</span>, <span class="string">"its"</span>, <span class="string">"our"</span>, <span class="string">"two"</span>, <span class="string">"more"</span>,</div><div class="line">             <span class="string">"these"</span>, <span class="string">"want"</span>, <span class="string">"way"</span>, <span class="string">"look"</span>, <span class="string">"first"</span>, <span class="string">"also"</span>, <span class="string">"new"</span>, <span class="string">"because"</span>,</div><div class="line">             <span class="string">"day"</span>, <span class="string">"more"</span>, <span class="string">"use"</span>, <span class="string">"no"</span>, <span class="string">"man"</span>, <span class="string">"find"</span>, <span class="string">"here"</span>, <span class="string">"thing"</span>, <span class="string">"give"</span>,</div><div class="line">             <span class="string">"many"</span>, <span class="string">"well"</span>]</div><div class="line"></div><div class="line">    <span class="keyword">if</span> ngram <span class="keyword">in</span> commonWords:</div><div class="line">        <span class="built_in">return</span> True</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="built_in">return</span> False</div><div class="line"></div><div class="line">def cleanInput(input):</div><div class="line">    input = re.sub(<span class="string">'\n+'</span>,<span class="string">" "</span>,input).lower() <span class="comment"># 匹配换行,用空格替换换行符</span></div><div class="line">    input = re.sub(<span class="string">'\[[0-9]*\]'</span>,<span class="string">""</span>,input) <span class="comment"># 剔除类似[1]这样的引用标记</span></div><div class="line">    input = re.sub(<span class="string">' +'</span>, <span class="string">" "</span>, input) <span class="comment">#把连续多个空格替换成一个空格</span></div><div class="line">    input = bytes(input, <span class="string">"UTF-8"</span>)</div><div class="line">    input = input.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>)</div><div class="line">    cleanInput = []</div><div class="line">    input = input.split(<span class="string">' '</span>)</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> input:</div><div class="line">        item = item.strip(string.punctuation)</div><div class="line">        <span class="keyword">if</span> len(item) &gt; 1 or (item.lower() == <span class="string">'a'</span> or item.lower() == <span class="string">'i'</span>):</div><div class="line">            cleanInput.append(item)</div><div class="line">    <span class="built_in">return</span> cleanInput</div><div class="line"></div><div class="line">def ngrams(input, n):</div><div class="line">    input = cleanInput(input)</div><div class="line">    output = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input)-n+1):</div><div class="line">        ngramTemp =<span class="string">" "</span>.join(input[i:i+n]) <span class="comment">#这句话将n-grams拆分成n个元素组成的列表</span></div><div class="line">        <span class="keyword">if</span> isCommon(ngramTemp.split()[0]) or isCommon(ngramTemp.split()[1]):</div><div class="line">            pass</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">if</span> ngramTemp not <span class="keyword">in</span> output:</div><div class="line">                output[ngramTemp] = 0</div><div class="line">            output[ngramTemp] += 1</div><div class="line">    <span class="built_in">return</span> output</div><div class="line"></div><div class="line"><span class="comment">#获取核心词在的句子</span></div><div class="line">def getFirstSentenceCOntaining(ngram, content):</div><div class="line">    sentences = content.split(<span class="string">'.'</span>)</div><div class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</div><div class="line">        <span class="keyword">if</span> ngram <span class="keyword">in</span> sentence:</div><div class="line">            <span class="built_in">return</span> sentence</div><div class="line">    <span class="built_in">return</span> <span class="string">""</span></div><div class="line">content = str(urlopen(<span class="string">"http://pythonscraping.com/files/inaugurationSpeech.txt"</span>).<span class="built_in">read</span>(),<span class="string">'utf-8'</span>)</div><div class="line">ngrams = ngrams(content, 2)</div><div class="line"><span class="comment">#核心词</span></div><div class="line">sortedNGrams = sorted(ngrams.items(), key=operator.itemgetter(1), reverse=True)</div><div class="line"><span class="built_in">print</span>(sortedNGrams)</div><div class="line"><span class="comment">#核心句</span></div><div class="line"><span class="keyword">for</span> top3 <span class="keyword">in</span> range(3):</div><div class="line">    <span class="built_in">print</span>(<span class="string">"&gt;"</span>+getFirstSentenceCOntaining(sortedNGrams[top3][0],content.lower()))</div><div class="line">``` </div><div class="line">结果：</div><div class="line">&gt; the constitution of the united states is the instrument containing this grant of power to the several departments composing the government</div><div class="line"></div><div class="line">&gt; the general government has seized upon none of the reserved rights of the states</div><div class="line"></div><div class="line">&gt; such a one was afforded by the executive department constituted by the constitution</div><div class="line"></div><div class="line">我的困惑：这里返回的仅仅是匹配到的第一句话（也就是该核心词匹配的到第一句话，后面的都放弃了，是否会有不妥）</div><div class="line"></div><div class="line"><span class="comment">## 马尔可夫模型</span></div><div class="line">随机事件的特点 是一个离散事件发生之后，另一个离散事件将在前一个事件的条件下以一定的概率发生。</div><div class="line">图：马尔科夫模型描述理论天气系统</div><div class="line">• 任何一个节点引出的所有可能的总和必须等于100%。无论是多么复杂的系统，必然会 在下一步发生若干事件中的一个事件。• 虽然这个天气系统在任一时间都只有三种可能，但是你可以用这个模型生成一个天气状 态的无限次转移列表。• 只有当前节点的状态会影响后一天的状态。如果你在“晴天”节点上，即使前100天都 是晴天或雨天都没关系，明天晴天的概率还是 70%。• 有些节点可能比其他节点较难到达。这个现象的原因用数学来解释非常复杂，但是可以 直观地看出，在这个系统中任意时间节点上，第二天是“雨天”的可能性(指向它的箭 头概率之和小于“100%”)比“晴天”或“多云”要小很多</div><div class="line">``` bash </div><div class="line"><span class="comment">#生成链为100的马尔可夫链</span></div><div class="line">from urllib.request import urlopen</div><div class="line">from random import randint</div><div class="line"></div><div class="line">def wordlistSum(wordList):</div><div class="line">    sum = 0</div><div class="line">    <span class="keyword">for</span> word, value <span class="keyword">in</span> wordList.items():</div><div class="line">        sum += value</div><div class="line">    <span class="built_in">return</span> sum</div><div class="line"></div><div class="line">def retrieveRandomWord(wordlist):</div><div class="line">    randIndex = randint(1, wordlistSum(wordlist))</div><div class="line">    <span class="keyword">for</span> word, value <span class="keyword">in</span> wordlist.items():</div><div class="line">        randIndex -= value</div><div class="line">        <span class="keyword">if</span> randIndex &lt;= 0:</div><div class="line">            <span class="built_in">return</span> word</div><div class="line"></div><div class="line">def buildWordDict(text):</div><div class="line">    <span class="comment"># 剔除换行符和引号</span></div><div class="line">    text = text.replace(<span class="string">"\n"</span>, <span class="string">" "</span>)</div><div class="line">    text = text.replace(<span class="string">"\""</span>, <span class="string">""</span>)</div><div class="line">    <span class="comment"># 保证每个标点符号都和前面的单词在一起</span></div><div class="line">    <span class="comment"># 这样不会被剔除，保留在马尔可夫链中</span></div><div class="line">    punctuation = [<span class="string">','</span>, <span class="string">'.'</span>, <span class="string">';'</span>,<span class="string">':'</span>]</div><div class="line">    <span class="keyword">for</span> symbol <span class="keyword">in</span> punctuation:</div><div class="line">        text = text.replace(symbol, <span class="string">" "</span>+symbol+<span class="string">" "</span>)</div><div class="line">    words = text.split(<span class="string">" "</span>)</div><div class="line">    <span class="comment"># 过滤空单词</span></div><div class="line">    words = [word <span class="keyword">for</span> word <span class="keyword">in</span> words <span class="keyword">if</span> word != <span class="string">""</span>]</div><div class="line"></div><div class="line">    wordDict = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(1, len(words)):</div><div class="line">        <span class="keyword">if</span> words[i-1] not <span class="keyword">in</span> wordDict: <span class="comment"># 为单词新建一个词典</span></div><div class="line">            wordDict[words[i-1]] = &#123;&#125;</div><div class="line">        <span class="keyword">if</span> words[i] not <span class="keyword">in</span> wordDict[words[i-1]]:</div><div class="line">            wordDict[words[i-1]][words[i]] = 0</div><div class="line">        wordDict[words[i-1]][words[i]] = wordDict[words[i-1]][words[i]] + 1</div><div class="line">    <span class="built_in">return</span> wordDict</div><div class="line"></div><div class="line">text = str(urlopen(<span class="string">"http://pythonscraping.com/files/inaugurationSpeech.txt"</span>)</div><div class="line">           .<span class="built_in">read</span>(), <span class="string">'utf-8'</span>)</div><div class="line">wordDict = buildWordDict(text)</div><div class="line">length = 100</div><div class="line">chain = <span class="string">""</span></div><div class="line">currentWord = <span class="string">"I"</span></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(0, length):</div><div class="line">    chain += currentWord+<span class="string">" "</span></div><div class="line">    currentWord = retrieveRandomWord(wordDict[currentWord])</div><div class="line"><span class="built_in">print</span>(chain)</div><div class="line">``` </div><div class="line">上述代码会随机生成一段100个单词的马尔可夫链，至于句子的含义，就是胡言乱语</div><div class="line"><span class="comment">## 维基百科六度分割：终结篇</span></div><div class="line">在寻找有向图的最短路径问题中，即找出维基百科中凯文 ·贝肯词条和其他词条之间最短链接路径的方法中，效果 最好且最常用的一种方法是广度优先搜索(breadth-first search)。</div><div class="line">``` bash </div><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import pymysql</div><div class="line"></div><div class="line">conn = pymysql.connect(host=<span class="string">'127.0.0.1'</span>, unix_socket=<span class="string">'/tmp/mysql.sock'</span>,</div><div class="line">                       user=<span class="string">'root'</span>, passwd=<span class="string">'wyt629szk'</span>, db=<span class="string">'mysql'</span>, charset=<span class="string">'utf8'</span>)</div><div class="line">cur = conn.cursor()</div><div class="line">cur.execute(<span class="string">"USE wikipedia"</span>)</div><div class="line"></div><div class="line">class SolutionFound(RuntimeError):</div><div class="line">    def __init__(self, message):</div><div class="line">        self.message = message</div><div class="line"></div><div class="line">def getLinks(fromPageId):</div><div class="line">    cur.execute(<span class="string">"SELECT toPageId FROM links WHERE fromPageId = %s"</span>, (fromPageId))</div><div class="line">    <span class="keyword">if</span> cur.rowcount == 0:</div><div class="line">        <span class="built_in">return</span> None</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="built_in">return</span> [x[0] <span class="keyword">for</span> x <span class="keyword">in</span> cur.fetchall()]</div><div class="line"></div><div class="line">def constructDict(currentPageId):</div><div class="line">    links = getLinks(currentPageId)</div><div class="line">    <span class="keyword">if</span> links:</div><div class="line">        <span class="built_in">return</span> dict(zip(links,[&#123;&#125;]*len(links)))</div><div class="line">    <span class="built_in">return</span> &#123;&#125;</div><div class="line"></div><div class="line"><span class="comment"># 链接树要么为空，要么包含多个链接</span></div><div class="line">def searchDepth(targetPageId, currentPageId, linkTree, depth):</div><div class="line">    <span class="keyword">if</span> depth == 0:</div><div class="line">        <span class="comment">#停止递归，返回结果</span></div><div class="line">        <span class="built_in">return</span> linkTree</div><div class="line">    <span class="keyword">if</span> not linkTree:</div><div class="line">        <span class="comment">#如果函数获取的链接字典是空的，就对当前页面的链接进行搜索。如果当前页面也没链</span></div><div class="line">        <span class="comment"># 接，就返回空链接字典。</span></div><div class="line">        linkTree = constructDict(currentPageId)</div><div class="line">        <span class="keyword">if</span> not linkTree:</div><div class="line">            <span class="comment">#若此节点无连接，则跳过此节点</span></div><div class="line">            <span class="built_in">return</span> &#123;&#125;</div><div class="line">    <span class="keyword">if</span> targetPageId <span class="keyword">in</span> linkTree.keys():</div><div class="line">        <span class="built_in">print</span>(<span class="string">"TARGET"</span> + str(targetPageId) + <span class="string">" FOUND!"</span>)</div><div class="line">        raise SolutionFound(<span class="string">"PAGE: "</span>+ str(currentPageId))</div><div class="line"></div><div class="line">    <span class="keyword">for</span> branchKey, branchValue <span class="keyword">in</span> linkTree.items():</div><div class="line">        try:</div><div class="line">            <span class="comment">#递归建立链接树</span></div><div class="line">            linkTree[branchKey] = searchDepth(targetPageId, branchKey, branchValue,</div><div class="line">                                              depth-1)</div><div class="line">        except SolutionFound as e:</div><div class="line">            <span class="built_in">print</span>(e.message)</div><div class="line">            raise  SolutionFound(<span class="string">"PAGE: "</span>+str(currentPageId))</div><div class="line">    <span class="built_in">return</span> linkTree</div><div class="line">try:</div><div class="line">    searchDepth(134951, 1, &#123;&#125;, 4)</div><div class="line">    <span class="built_in">print</span>(<span class="string">"No solution found"</span>)</div><div class="line">except SolutionFound as e:</div><div class="line">    <span class="built_in">print</span>(e.message)</div></pre></td></tr></table></figure></p></blockquote></blockquote></blockquote><p>由于数据库中的数据被我删除了，所以没有测试（^~^）</p><h2 id="自然语言工具包（NLTK）"><a href="#自然语言工具包（NLTK）" class="headerlink" title="自然语言工具包（NLTK）"></a>自然语言工具包（NLTK）</h2><p>一个python库，用于识别和标记英语文本中各个词的词性（<a href="http://www.nltk.org/install.html）" target="_blank" rel="external">http://www.nltk.org/install.html）</a></p><blockquote><blockquote><blockquote><p>import nltk<br>nltk.download()<br>两行命令会打开NLTK的下载器</p></blockquote></blockquote></blockquote><h3 id="用NLTK做统计分析"><a href="#用NLTK做统计分析" class="headerlink" title="用NLTK做统计分析"></a>用NLTK做统计分析</h3><p>文字的单词数量，单词频率和单词词性<br>一般从Text对象开始<br>from nltk import word_tokenize<br>from nltk import Text</p><p>tokens = word_tokenize(“Here is some not very interesting text”)<br>text = Text(tokens)</p><p>NLTK库里面已经内置了几本书，可以用import函数导入：<br>from nltk.book import *</p><blockquote><blockquote><blockquote><p>len(text6)/len(words)</p></blockquote></blockquote></blockquote><p>你还可以将文本对象放到一个频率分布对象FreqDist中，查看哪些单词是最常用的，以及单词的频率是多少</p><blockquote><blockquote><blockquote><p>from nltk import FreqDist<br>fdist = FreqDist(text6)<br>fdist.most_common(10)<br>[(‘:’, 1197), (‘.’, 816), (‘!’, 801), (‘,’, 731), (“‘“, 421), (‘[‘, 3 19), (‘]’, 312), (‘the’, 299), (‘I’, 255), (‘ARTHUR’, 225)]<br>fdist[“Grail”]<br>34</p></blockquote></blockquote></blockquote><p>你可以用NLTK非常轻松的创建并搜索一个2-gram模型（还有一个trigrams 即：3-grams）：</p><blockquote><blockquote><blockquote><p>from nltk import bigrams<br>bigrams = bigrams(text6)<br>bigramsDist = FreqDist(bigrams) &gt;&gt;&gt; bigramsDist[(“Sir”, “Robin”)]<br>18</p></blockquote></blockquote></blockquote><p>对于更一般的情形，你还可以导入ngrams模块：</p><blockquote><blockquote><blockquote><p>from nltk import ngrams<br>fourgrams = ngrams(text6, 4)<br>fourgramsDist = FreqDist(fourgrams)<br>fourgramsDist[(“father”, “smelt”, “of”, “elderberries”)]<br>1</p></blockquote></blockquote></blockquote><p>频率分布，文本对象和n-gram还可以整合在一个循环中进行迭代（下面程序就是打印文本中所以以”coconut”）<br>from nltk.book import *<br>from nltk import ngrams<br>fourgrams = ngrams(text6, 4) for fourgram in fourgrams:<br>    if fourgram[0] == “coconut”:<br>        print(fourgram)</p><h3 id="用NLTK做词性分析"><a href="#用NLTK做词性分析" class="headerlink" title="用NLTK做词性分析"></a>用NLTK做词性分析</h3><p>考虑同一个词在不同的语境中可能会导致意思混乱<br>ex:”He was objective in achieving his objective of writing an objective philosophy, primarily using verbs in the objective case”<br>爬虫会认为（objective）被用了四次，进而简单地忽略这四个单词各自不同的含义</p><p>还有要分析普通英文单词组成的公司名称，或者分析某个人对一个公司的评价，像<br>ACME Products is good”和“ACME Products is not bad”意思是一样的</p><p>Penn Treebank语意标记</p><p>除了度量语言，NLTK还可以用它的超级大字典分析文本内容，帮助人们寻找单词的含义。<br>NLTK的一个基本功能就是识别句子中各个词性</p><blockquote><blockquote><blockquote><p>from nltk.book import *<br>from nltk import word_tokenize<br>text = word_tokenize(“Strange women lying in ponds distributing swords is no basis for a system of government. Supreme executive power derives from a mandate from the masses, not from some farcical aquatic ceremony.”)<br>from nltk import pos_tag<br>pos_tag(text)<br>[(‘Strange’, ‘NNP’), (‘women’, ‘NNS’), (‘lying’, ‘VBG’), (‘in’, ‘IN’)<br>, (‘ponds’, ‘NNS’), (‘distributing’, ‘VBG’), (‘swords’, ‘NNS’), (‘is’<br>, ‘VBZ’), (‘no’, ‘DT’), (‘basis’, ‘NN’), (‘for’, ‘IN’), (‘a’, ‘DT’),<br>(‘system’, ‘NN’), (‘of’, ‘IN’), (‘government’, ‘NN’), (‘.’, ‘.’),<br>(‘Supreme’, ‘NNP’), (‘executive’, ‘NN’), (‘power’, ‘NN’), (‘derives’, ‘NNS’),<br>(‘from’, ‘IN’), (‘a’, ‘DT’), (‘mandate’, ‘NN’), (‘from’, ‘IN’),<br>(‘the’, ‘DT’), (‘masses’, ‘NNS’), (‘,’, ‘,’), (‘not’, ‘RB’), (‘from’, ‘IN’),<br> (‘some’, ‘DT’), (‘farcical’, ‘JJ’), (‘aquatic’, ‘JJ’), (‘ceremony’, ‘NN’), (‘.’, ‘.’)]</p></blockquote></blockquote></blockquote><p>但是要正确的完成任务其实很复杂，用下面的例子看更直观</p><blockquote><blockquote><blockquote><p>text = word_tokenize(“The dust was thick so he had to dust”)<br>pos_tag(text)<br>[(‘The’, ‘DT’), (‘dust’, ‘NN’), (‘was’, ‘VBD’), (‘thick’, ‘JJ’), (‘so’, ‘RB’),<br>(‘he’, ‘PRP’), (‘had’, ‘VBD’), (‘to’, ‘TO’), (‘dust’, ‘VB’)]</p></blockquote></blockquote></blockquote><p>需要注意的是dust出现了两次，一个是名称，另外一个是动词（NLTK用英语的上下文无关法识别词性）</p><p>注：机器学习和机器训练<br>你也可以对NLTK进行训练，创建一个全新的上下文无关文法规则，比如，一种外语 的上下文无关文法规则。如果你用 Penn Treebank 词性标记手工完成了那种语言的大部 分文本的语义标记，那么你就可以把结果传给NLTK，然后训练它对其他未标记的文 本进行语义标记</p><h4 id="示例（找出google作为名称而不是动词的句子）"><a href="#示例（找出google作为名称而不是动词的句子）" class="headerlink" title="示例（找出google作为名称而不是动词的句子）"></a>示例（找出google作为名称而不是动词的句子）</h4><p>``` bash<br>from nltk import word_tokenize, sent_tokenize, pos_tag<br>sentences = sent_tokenize(“Google is one of the best companies in the world. I constantly google myself to see what I’m up to.”)<br>nouns = [‘NN’, ‘NNS’, ‘NNP’, ‘NNPS’]<br>for sentence in sentences:<br>    if “google” in sentence.lower():<br>        taggedWords = pos_tag(word_tokenize(sentence))<br>            for word in taggleWords:<br>                if word[0].lower() == “google” and word[1] in nouns:                         print(sentence)</p><h3 id="其他资源"><a href="#其他资源" class="headerlink" title="其他资源"></a>其他资源</h3><p>Natural Language Processing with Python(http:// shop.oreilly.com/product/9780596516499.do)<br>Natural Language Annotation for Machine Learning(<a href="http://shop.oreilly.com/product/0636920020578.do" target="_blank" rel="external">http://shop.oreilly.com/product/0636920020578.do</a>)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;自然语言的处理&quot;&gt;&lt;a href=&quot;#自然语言的处理&quot; class=&quot;headerlink&quot; title=&quot;自然语言的处理&quot;&gt;&lt;/a&gt;自然语言的处理&lt;/h1&gt;&lt;h2 id=&quot;概括数据&quot;&gt;&lt;a href=&quot;#概括数据&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>自然语言处理</title>
    <link href="http://yoursite.com/2017/10/04/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2017/10/04/自然语言处理/</id>
    <published>2017-10-04T03:07:53.000Z</published>
    <updated>2017-10-04T03:07:53.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python数据收集（数据清洗）</title>
    <link href="http://yoursite.com/2017/10/03/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/03/python数据收集（数据清洗）/</id>
    <published>2017-10-03T12:51:22.000Z</published>
    <updated>2017-10-04T02:52:09.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><h2 id="编写代码清洗数据"><a href="#编写代码清洗数据" class="headerlink" title="编写代码清洗数据"></a>编写代码清洗数据</h2><p>n-gram<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line"></div><div class="line"><span class="comment">#清洗规则函数</span></div><div class="line"></div><div class="line"><span class="comment">#剔除单字符的“单词”，除非这个字符是“i”或“a”;</span></div><div class="line"><span class="comment">#剔除维基百科的引用标记(方括号包裹的数字，如[1]);</span></div><div class="line"><span class="comment">#剔除标点符号(注意:这个规则有点儿矫枉过正，在第9章我们将详细介绍，本例暂时这样处理)。</span></div><div class="line">import string</div><div class="line"></div><div class="line">def cleanInput(input):</div><div class="line">    input = re.sub(<span class="string">'\n+'</span>,<span class="string">' '</span>, input)</div><div class="line">    input = re.sub(<span class="string">'\[[0-9]*\]'</span>,<span class="string">""</span>, input)</div><div class="line">    input = re.sub(<span class="string">' +'</span>,<span class="string">" "</span>, input)</div><div class="line">    input = bytes(input,<span class="string">"UTF-8"</span>)</div><div class="line">    input = input.decode(<span class="string">"ascii"</span>, <span class="string">"ignore"</span>)</div><div class="line">    cleanInput = []</div><div class="line">    input = input.split(<span class="string">' '</span>)</div><div class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> input:</div><div class="line">        item = item.strip(string.punctuation)</div><div class="line">        <span class="keyword">if</span> len(item) &gt; 1 or (item.lower() == <span class="string">'i'</span>):</div><div class="line">            cleanInput.append(item)</div><div class="line">    <span class="built_in">return</span> cleanInput</div><div class="line"></div><div class="line">def ngrams(input, n):</div><div class="line">    input = input.upper()</div><div class="line">    input = cleanInput(input)</div><div class="line">    output = []</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input)-n+1):</div><div class="line">        output.append(input[i:i+n])</div><div class="line">    <span class="built_in">return</span> output</div><div class="line">html = urlopen(<span class="string">"http://en.wikipedia.org/wiki/Python_(programming_language)"</span>)</div><div class="line">bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)</div><div class="line">content = bsObj.find(<span class="string">'div'</span>,&#123;<span class="string">'id'</span>:<span class="string">'mw-content-text'</span>&#125;).get_text()</div><div class="line">ngrams = ngrams(content, 2)</div><div class="line"><span class="built_in">print</span>(ngrams)</div><div class="line"><span class="built_in">print</span>(<span class="string">"2-grams count is: "</span>+str(len(ngrams)))</div></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><p>import string<br>print(string.punctuation)<br>!”#$%&amp;’()*+,-./:;&lt;=&gt;?@[]^_`{|}~</p><h3 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h3><p>python的字典是无序的，不能想数组一样直接对n-gran序列频率进行排序，字典内部的元素位置排序以后再次使用时还是<br>会变化，在python的collections库里面有一个OrderedDict可以解决这个问题</p><pre><code class="bash">from collection import OrderedDict……ngrams = OrderedDict(sorted(ngrams.items(), key=lambda t: t[1), reverse=True)</code></pre><p>没有跑起来，网速不好（下次测试）</p></blockquote></blockquote></blockquote><h2 id="数据存储后在清洗"><a href="#数据存储后在清洗" class="headerlink" title="数据存储后在清洗"></a>数据存储后在清洗</h2><p>OpenRefine</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据清洗&quot;&gt;&lt;a href=&quot;#数据清洗&quot; class=&quot;headerlink&quot; title=&quot;数据清洗&quot;&gt;&lt;/a&gt;数据清洗&lt;/h1&gt;&lt;h2 id=&quot;编写代码清洗数据&quot;&gt;&lt;a href=&quot;#编写代码清洗数据&quot; class=&quot;headerlink&quot; title=&quot;编
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（六）</title>
    <link href="http://yoursite.com/2017/10/03/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E5%85%AD%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/03/python数据收集（六）/</id>
    <published>2017-10-03T07:36:23.000Z</published>
    <updated>2017-10-03T12:49:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="读取文档"><a href="#读取文档" class="headerlink" title="读取文档"></a>读取文档</h1><h2 id="文档编码"><a href="#文档编码" class="headerlink" title="文档编码"></a>文档编码</h2><p>纯文本文件，视频文件和图像文件的唯一区别，就是它们的0和1面向用户的转换方式不同</p><h3 id="纯文本"><a href="#纯文本" class="headerlink" title="纯文本"></a>纯文本</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">textPage = urlopen(<span class="string">"http://www.pythonscraping.com/pages/warandpeace/chapter1.txt"</span>)</div><div class="line"><span class="built_in">print</span>(textPage.read())</div></pre></td></tr></table></figure><p>像这种纯文本的，使用BeautifulSoup库就没有用了，如果变成BeautifulSoup反而适得其反</p><h2 id="文本编码和全球互联网"><a href="#文本编码和全球互联网" class="headerlink" title="文本编码和全球互联网"></a>文本编码和全球互联网</h2><p>UTF-8:<br>在 UTF-8 设计过程中，设计师决定利用 ASCII 文档里的“填充位”，让所有以“0”开头的 字节表示这个字符只用 1 个字节，从而把 ASCII 和 UTF-8 编码完美地结合在一起。因此， 下面的字符在 ASCII 和 UTF-8 两种编码方式中都是有效的:<br>      01000001 - A<br>      01000010 - B<br>      01000011 - C<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># from urllib.request import urlopen</div><div class="line"># textPage = urlopen(&quot;http://www.pythonscraping.com/pages/warandpeace/chapter1.txt&quot;)</div><div class="line"># print(textPage.read())</div><div class="line"></div><div class="line"></div><div class="line"># from urllib.request import urlopen</div><div class="line"># textPage = urlopen(&quot;http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt&quot;)</div><div class="line"># print(str(textPage.read(),&apos;utf-8&apos;))</div><div class="line"></div><div class="line">#用BeautifulSoup和python3.x对文档进行UTF-8编码，如下所示</div><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">html = urlopen(&quot;http://en.wikipedia.org/wiki/Python_(programming_language)&quot;)</div><div class="line">bsObj = BeautifulSoup(html)</div><div class="line">content = bsObj.find(&quot;div&quot;, &#123;&quot;id&quot;:&quot;mw-content-text&quot;&#125;).get_text()</div><div class="line">content = bytes(content, &quot;UTF-8&quot;)</div><div class="line">content = content.decode(&quot;UTF-8&quot;)</div></pre></td></tr></table></figure></p><h2 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h2><h3 id="读取CSV文件"><a href="#读取CSV文件" class="headerlink" title="读取CSV文件"></a>读取CSV文件</h3><p>• 手动把CSV文件下载到本机，然后用Python定位文件位置;<br>• 写Python程序下载文件，读取之后再把源文件删除;<br>• 从网上直接把文件读成一个字符串，然后转换成一个StringIO对象，使它具有文件的<br>属性。（这个方法比较可行）</p><h2 id="PDF"><a href="#PDF" class="headerlink" title="PDF"></a>PDF</h2><p>pdf转字符串(直接上代码)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#把PDF 读成字符串，然后用 StringIO 转换成文件对象</span></div><div class="line">from urllib.request import urlopen</div><div class="line">from pdfminer.pdfinterp import PDFResourceManager, process_pdf</div><div class="line">from pdfminer.converter import TextConverter</div><div class="line">from pdfminer.layout import LAParams</div><div class="line">from io import StringIO</div><div class="line">from io import open</div><div class="line"></div><div class="line">def readPDF(pdfFile):</div><div class="line">    rsrcmgr = PDFResourceManager()</div><div class="line">    retstr = StringIO()</div><div class="line">    laparams = LAParams()</div><div class="line">    device = TextConverter(rsrcmgr, retstr, laparams=laparams)</div><div class="line">    process_pdf(rsrcmgr, device, pdfFile)</div><div class="line">    device.close()</div><div class="line">    content = retstr.getvalue()</div><div class="line">    retstr.close()</div><div class="line">    <span class="built_in">return</span> content</div><div class="line">pdfFile = urlopen(<span class="string">"http://pythonscraping.com/pages/warandpeace/chapter1.pdf"</span>)</div><div class="line">outputString = readPDF(pdfFile)</div><div class="line"><span class="built_in">print</span>(outputString)</div><div class="line">pdfFile.close()</div><div class="line">``` </div><div class="line"><span class="comment">#如果格式里面有图片，各式各样的文本格式，或者带有表格和数据图的时候，输出结果可能不是很完美</span></div><div class="line"></div><div class="line"><span class="comment">## 微软Word和.docx</span></div><div class="line"><span class="comment">### 读取Microsoft Office 文件</span></div><div class="line">第一步是从文件读取XML</div><div class="line">``` bash </div><div class="line">from zipfile import ZipFile</div><div class="line">from urllib.request import urlopen</div><div class="line">from io import BytesIO</div><div class="line">from bs4 import BeautifulSoup</div><div class="line"></div><div class="line">wordFile = urlopen(<span class="string">"http://pythonscraping.com/pages/AWordDocument.docx"</span>).<span class="built_in">read</span>()</div><div class="line">wordFile = BytesIO(wordFile)</div><div class="line">document = ZipFile(wordFile)</div><div class="line">xml_content = document.read(<span class="string">'word/document.xml'</span>)</div><div class="line"></div><div class="line">wordObj = BeautifulSoup(xml_content.decode(<span class="string">'utf-8'</span>),<span class="string">'html.parser'</span>)</div><div class="line"><span class="comment"># textStrings = wordObj.findAll("w:t")</span></div><div class="line"><span class="comment"># for textElem in textStrings:</span></div><div class="line"><span class="comment">#     print(textElem.text)</span></div><div class="line"><span class="comment">#print(wordObj.text)</span></div><div class="line">textStrings = wordObj.findAll(<span class="string">"w:t"</span>)</div><div class="line"><span class="keyword">for</span> textElem <span class="keyword">in</span> textStrings:</div><div class="line">    closeTag = <span class="string">""</span></div><div class="line">    try:</div><div class="line">        style = textElem.parent.previousSibling.find(<span class="string">"w:pstyle"</span>)</div><div class="line">        <span class="keyword">if</span> style is not None and style[<span class="string">"w:val"</span>] == <span class="string">"Title"</span>: <span class="built_in">print</span>(<span class="string">"&lt;h1&gt;"</span>)</div><div class="line">        closeTag = <span class="string">"&lt;/h1&gt;"</span></div><div class="line">    except AttributeError: <span class="comment">#不打印标签</span></div><div class="line">        pass</div><div class="line">        <span class="built_in">print</span>(textElem.text)</div><div class="line">        <span class="built_in">print</span>(closeTag)</div></pre></td></tr></table></figure></p><p>由于按照书上的做法是得到的是一个空的textStrings,但是可用wordObj.text属性找出，但是格式不太对</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;读取文档&quot;&gt;&lt;a href=&quot;#读取文档&quot; class=&quot;headerlink&quot; title=&quot;读取文档&quot;&gt;&lt;/a&gt;读取文档&lt;/h1&gt;&lt;h2 id=&quot;文档编码&quot;&gt;&lt;a href=&quot;#文档编码&quot; class=&quot;headerlink&quot; title=&quot;文档编码&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>读取文档</title>
    <link href="http://yoursite.com/2017/10/03/%E8%AF%BB%E5%8F%96%E6%96%87%E6%A1%A3/"/>
    <id>http://yoursite.com/2017/10/03/读取文档/</id>
    <published>2017-10-03T07:35:49.000Z</published>
    <updated>2017-10-03T07:35:49.000Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>python数据收集（五）</title>
    <link href="http://yoursite.com/2017/10/02/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/02/python数据收集（五）/</id>
    <published>2017-10-02T07:56:58.000Z</published>
    <updated>2017-10-03T07:34:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="存储数据"><a href="#存储数据" class="headerlink" title="存储数据"></a>存储数据</h1><h2 id="媒体文件"><a href="#媒体文件" class="headerlink" title="媒体文件"></a>媒体文件</h2><h3 id="盗链"><a href="#盗链" class="headerlink" title="盗链"></a>盗链</h3><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><h4 id="下载单个图片"><a href="#下载单个图片" class="headerlink" title="下载单个图片"></a>下载单个图片</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlretrieve </div><div class="line">from urllib.request import urlopen </div><div class="line">from bs4 import BeautifulSouphtml = urlopen(<span class="string">"http://www.pythonscraping.com"</span>)bsObj = BeautifulSoup(html)imageLocation = bsObj.find(<span class="string">"a"</span>, &#123;<span class="string">"id"</span>: <span class="string">"logo"</span>&#125;).find(<span class="string">"img"</span>)[<span class="string">"src"</span>]urlretrieve (imageLocation, <span class="string">"logo.jpg"</span>)</div><div class="line">注：</div><div class="line">open newline可选参数：None，’’，\n，\r，\r\n</div></pre></td></tr></table></figure><h4 id="下载src下的所有资源（该页面）"><a href="#下载src下的所有资源（该页面）" class="headerlink" title="下载src下的所有资源（该页面）"></a>下载src下的所有资源（该页面）</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#将 http://pythonscraping. com 主页上的所有src属性的文件下载下来</span></div><div class="line">import os</div><div class="line">from urllib.request import urlretrieve</div><div class="line">from urllib.request import urlopen</div><div class="line">from  bs4 import BeautifulSoup</div><div class="line"></div><div class="line">downloadDirectory = <span class="string">"downloaded"</span></div><div class="line">baseUrl = <span class="string">"http://pythonscraping.com"</span></div><div class="line"></div><div class="line">def getAbsoluteURL(baseUrl, <span class="built_in">source</span>):</div><div class="line">    <span class="keyword">if</span> source.startswith(<span class="string">"http://www."</span>):</div><div class="line">        url = <span class="string">"http://"</span>+<span class="built_in">source</span>[11:]</div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"http://"</span>):</div><div class="line">        url = <span class="built_in">source</span></div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"www."</span>):</div><div class="line">        url = <span class="built_in">source</span>[4:]</div><div class="line">        url = <span class="string">"http://"</span> + <span class="built_in">source</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        url = baseUrl+<span class="string">"/"</span>+<span class="built_in">source</span></div><div class="line">    <span class="keyword">if</span> baseUrl not <span class="keyword">in</span> url:</div><div class="line">        <span class="built_in">return</span> None</div><div class="line">    <span class="built_in">return</span> url</div><div class="line"></div><div class="line">def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):</div><div class="line">    path = absoluteUrl.replace(<span class="string">"www."</span>,<span class="string">""</span>)</div><div class="line">    path = path.replace(baseUrl,<span class="string">""</span>)</div><div class="line">    path = downloadDirectory + path</div><div class="line">    directory = os.path.dirname(path)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> not os.path.exists(directory):</div><div class="line">        os.makedirs(directory)</div><div class="line">    <span class="built_in">return</span> path</div><div class="line"></div><div class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com"</span>)</div><div class="line">bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)</div><div class="line">downloadList = bsObj.findAll(src = True)</div><div class="line"></div><div class="line"><span class="keyword">for</span> download <span class="keyword">in</span> downloadList:</div><div class="line">    fileUrl = getAbsoluteURL(baseUrl, download[<span class="string">"src"</span>])</div><div class="line">    <span class="keyword">if</span> fileUrl is not None:</div><div class="line">        <span class="built_in">print</span>(fileUrl)</div><div class="line">        urlretrieve(fileUrl,getDownloadPath(baseUrl, fileUrl, downloadDirectory))</div></pre></td></tr></table></figure><h4 id="保存为CSV格式"><a href="#保存为CSV格式" class="headerlink" title="保存为CSV格式"></a>保存为CSV格式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#将 http://pythonscraping. com 主页上的所有src属性的文件下载下来</span></div><div class="line">import os</div><div class="line">from urllib.request import urlretrieve</div><div class="line">from urllib.request import urlopen</div><div class="line">from  bs4 import BeautifulSoup</div><div class="line"></div><div class="line">downloadDirectory = <span class="string">"downloaded"</span></div><div class="line">baseUrl = <span class="string">"http://pythonscraping.com"</span></div><div class="line"></div><div class="line">def getAbsoluteURL(baseUrl, <span class="built_in">source</span>):</div><div class="line">    <span class="keyword">if</span> source.startswith(<span class="string">"http://www."</span>):</div><div class="line">        url = <span class="string">"http://"</span>+<span class="built_in">source</span>[11:]</div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"http://"</span>):</div><div class="line">        url = <span class="built_in">source</span></div><div class="line">    <span class="keyword">elif</span> source.startswith(<span class="string">"www."</span>):</div><div class="line">        url = <span class="built_in">source</span>[4:]</div><div class="line">        url = <span class="string">"http://"</span> + <span class="built_in">source</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        url = baseUrl+<span class="string">"/"</span>+<span class="built_in">source</span></div><div class="line">    <span class="keyword">if</span> baseUrl not <span class="keyword">in</span> url:</div><div class="line">        <span class="built_in">return</span> None</div><div class="line">    <span class="built_in">return</span> url</div><div class="line"></div><div class="line">def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):</div><div class="line">    path = absoluteUrl.replace(<span class="string">"www."</span>,<span class="string">""</span>)</div><div class="line">    path = path.replace(baseUrl,<span class="string">""</span>)</div><div class="line">    path = downloadDirectory + path</div><div class="line">    directory = os.path.dirname(path)</div><div class="line"></div><div class="line">    <span class="keyword">if</span> not os.path.exists(directory):</div><div class="line">        os.makedirs(directory)</div><div class="line">    <span class="built_in">return</span> path</div><div class="line"></div><div class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com"</span>)</div><div class="line">bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)</div><div class="line">downloadList = bsObj.findAll(src = True)</div><div class="line"></div><div class="line"><span class="keyword">for</span> download <span class="keyword">in</span> downloadList:</div><div class="line">    fileUrl = getAbsoluteURL(baseUrl, download[<span class="string">"src"</span>])</div><div class="line">    <span class="keyword">if</span> fileUrl is not None:</div><div class="line">        <span class="built_in">print</span>(fileUrl)</div><div class="line">        urlretrieve(fileUrl,getDownloadPath(baseUrl, fileUrl, downloadDirectory))</div></pre></td></tr></table></figure><h2 id="MySql"><a href="#MySql" class="headerlink" title="MySql"></a>MySql</h2><p>之前安装好了mysql，但是今天从终端连接的时候 mysql -u root -p居然报错了（心中有一万个草泥马飞过）<br>俺是文明人，错误代码忘记截图了，忽略以上信息<br>du -sh *<br>lsof -i:3306<br>ps -A|grep mysql<br>查看MySQL的默认日志文件的位置<br>show variables like ‘general_log_file’;</p><p>#我的默认位置<br>/usr/local/mysql-5.7.19-macos10.12-x86_64/data/<strong>*</strong>MacBook-Air.log</p><h3 id="与python整合"><a href="#与python整合" class="headerlink" title="与python整合"></a>与python整合</h3><p>开源库 PyMySQL<br>OK 默认你的数据库中已经有一张pages的表<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import pymysql</div><div class="line">conn = pymysql.connect(host=<span class="string">'127.0.0.1'</span>, unix_socket=<span class="string">'/tmp/mysql.sock'</span>,</div><div class="line">                       user=<span class="string">'root'</span>,passwd=‘******’, db=<span class="string">'mysql'</span>)</div><div class="line">cur = conn.cursor()</div><div class="line">cur.execute(<span class="string">"USE scraping"</span>)</div><div class="line"></div><div class="line">cur.execute(<span class="string">"SELECT * FROM pages WHERE id = 1"</span>)</div><div class="line"><span class="built_in">print</span>(cur.fetchone())</div><div class="line">cur.close()</div><div class="line">conn.close()</div></pre></td></tr></table></figure></p><p>测试数据库是否能正常连接，以及从数据库中读取数据</p><h4 id="连接-光标模式"><a href="#连接-光标模式" class="headerlink" title="连接/光标模式"></a>连接/光标模式</h4><p>连接模式除了要链接数据库外，还要发送数据库信息，处理回滚操作，创建新的光标对象等等<br>注意：用完光标和链接后，如果不关闭就会导致连接泄露，造成一种为未关闭连接的现象，这种现象会一直想好数据库资源<br>所以用完数据库之后记得关闭连接</p><h4 id="设置数据库字符"><a href="#设置数据库字符" class="headerlink" title="设置数据库字符"></a>设置数据库字符</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ALTER DATABASE scraping CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci; </div><div class="line">ALTER TABLE pages CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; </div><div class="line">ALTER TABLE pages CHANGE title title VARCHAR(200) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;ALTER TABLE pages CHANGE content content VARCHAR(10000) CHARACTER SET utf8mb4 CO LLATE utf8mb4_unicode_ci;</div></pre></td></tr></table></figure><p>虽然不晓得utf8mb4与utf8mb4_unicode_ci有什么区别，但是还是设置了（好像这是一种东西，使用COLLATE不同）<br>不过要比utf-8要多一个位元</p><h4 id="将网页上爬取的信息插入到数据库中"><a href="#将网页上爬取的信息插入到数据库中" class="headerlink" title="将网页上爬取的信息插入到数据库中"></a>将网页上爬取的信息插入到数据库中</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import datetime</div><div class="line">import random</div><div class="line">import pymysql</div><div class="line"></div><div class="line">conn = pymysql.connect(host=<span class="string">'127.0.0.1'</span>, unix_socket=<span class="string">'/tmp/mysql.sock'</span>,</div><div class="line">                       user=<span class="string">'root'</span>,passwd=<span class="string">'wyt629szk'</span>, db=<span class="string">'mysql'</span>, charset=<span class="string">'utf-8'</span>)</div><div class="line">cur = conn.cursor()</div><div class="line">cur.execute(<span class="string">"USE scraping"</span>)</div><div class="line"></div><div class="line">random.seed(datetime.datetime.now())</div><div class="line"></div><div class="line">def store(title, content):</div><div class="line">    cur.execute(<span class="string">"INSERT INTO pages(title, content) VALUES (\"%s\",\"%s\"),(title. content)"</span>)</div><div class="line">    cur.connection.commit()</div><div class="line"></div><div class="line">def getLinks(articleUrl):</div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+articleUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    title = bsObj.find(<span class="string">"h1"</span>).get_text()</div><div class="line">    content = bsObj.find(<span class="string">"div"</span>,&#123;<span class="string">"id"</span>:<span class="string">"mw-content-text"</span>&#125;).find(<span class="string">"p"</span>).get_text()</div><div class="line">    store(title, content)</div><div class="line">    <span class="built_in">return</span> bsObj.find(<span class="string">"div"</span>,&#123;<span class="string">"id"</span>:<span class="string">"bodyContent"</span>&#125;).findAll(<span class="string">"a"</span>,href=re.compile(<span class="string">"^(/wiki/)((?!:).)*$"</span>))</div><div class="line"></div><div class="line">links = getLinks(<span class="string">"/wiki/Kevin_Bacon"</span>)</div><div class="line">try:</div><div class="line">    <span class="keyword">while</span> len(links) &gt; 0:</div><div class="line">        newArticle = links[random.randint(0, len(links)-1)].attrs[<span class="string">'href'</span>]</div><div class="line">        <span class="built_in">print</span>(newArticle)</div><div class="line">        links = getLinks(newArticle)</div><div class="line">finally:</div><div class="line">    cur.close()</div><div class="line">    conn.close()</div></pre></td></tr></table></figure><h3 id="数据库技术与最佳实践"><a href="#数据库技术与最佳实践" class="headerlink" title="数据库技术与最佳实践"></a>数据库技术与最佳实践</h3><h4 id="主动创建一个id字段"><a href="#主动创建一个id字段" class="headerlink" title="主动创建一个id字段"></a>主动创建一个id字段</h4><h4 id="用智能索引"><a href="#用智能索引" class="headerlink" title="用智能索引"></a>用智能索引</h4><p>额外的索引血药占用更多的空间，而且插入新行的时候也需要花费更多的时间<br>例如：如果你经常要查询一个字段<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;SELECT * FROM dictionary WHERE definition=<span class="string">"A small furry animal that says meow"</span>;</div><div class="line"><span class="comment">#你可以给definition建立一个该字段前16个字符的智能索引</span></div><div class="line">CREATE INDEX definition ON dictionary (id, definition(16));</div></pre></td></tr></table></figure></p><h4 id="数据查询时间和数据库空间的问题"><a href="#数据查询时间和数据库空间的问题" class="headerlink" title="数据查询时间和数据库空间的问题"></a>数据查询时间和数据库空间的问题</h4><p>拆表 -&gt; 可以去除冗余</p><h3 id="MySQL里的”六度空间游戏”"><a href="#MySQL里的”六度空间游戏”" class="headerlink" title="MySQL里的”六度空间游戏”"></a>MySQL里的”六度空间游戏”</h3><p>设计一个带有两张表的数据库来分别存储页面和链接，两张表都带有创建时间和独立的ID号，代码如下所示:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE `wikipedia`.`pages` (`id` INT NOT NULL AUTO_INCREMENT,`url` VARCHAR(255) NOT NULL,`created` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`));</div><div class="line"></div><div class="line">CREATE TABLE `wikipedia`.`links` ( `id` INT NOT NULL AUTO_INCREMENT, `fromPageId` INT NULL, `toPageId` INT NULL,`created` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`));</div></pre></td></tr></table></figure></p><p>注：因为页面标题要在你进入页面后读取内容才能抓到。那么如果我们想创建一个高效的爬虫来填充这些数据表，那么只存储页的<br>链接就可以保存词条页面了，甚至不需要访问词条页面（不是很懂）</p><h2 id="Email（代码没有跑通，Connection-refused，先把书上的源码贴上来）"><a href="#Email（代码没有跑通，Connection-refused，先把书上的源码贴上来）" class="headerlink" title="Email（代码没有跑通，Connection refused，先把书上的源码贴上来）"></a>Email（代码没有跑通，Connection refused，先把书上的源码贴上来）</h2><pre><code class="bash">import smtplibfrom email.mime.text import MIMETextfrom bs4 import BeautifulSoupfrom urllib.request import urlopenimport timedef sendMail(subject, body):    msg = MIMEText(body)    msg[<span class="string">'Subject'</span>] = subject    msg[<span class="string">'From'</span>] = <span class="string">"christmas_alerts@pythonscraping.com"</span>    msg[<span class="string">'To'</span>] = <span class="string">"ryan@pythonscraping.com"</span>    s = smtplib.SMTP(<span class="string">'localhost'</span>)    s.send_message(msg)    s.quit()bsObj = BeautifulSoup(urlopen(<span class="string">"https://isitchristmas.com/"</span>))<span class="keyword">while</span>(bsObj.find(<span class="string">"a"</span>, {<span class="string">"id"</span>:<span class="string">"answer"</span>}).attrs[<span class="string">'title'</span>] == <span class="string">"NO"</span>):    <span class="built_in">print</span>(<span class="string">"It is not Christmas yet."</span>)    time.sleep(3600)    bsObj = BeautifulSoup(urlopen(<span class="string">"https://isitchristmas.com/"</span>))    sendMail(<span class="string">"It's Christmas!"</span>,              <span class="string">"According to http://itischristmas.com, it is Christmas!"</span>)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;存储数据&quot;&gt;&lt;a href=&quot;#存储数据&quot; class=&quot;headerlink&quot; title=&quot;存储数据&quot;&gt;&lt;/a&gt;存储数据&lt;/h1&gt;&lt;h2 id=&quot;媒体文件&quot;&gt;&lt;a href=&quot;#媒体文件&quot; class=&quot;headerlink&quot; title=&quot;媒体文件&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（四）</title>
    <link href="http://yoursite.com/2017/10/02/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://yoursite.com/2017/10/02/python数据收集（四）/</id>
    <published>2017-10-02T03:03:10.000Z</published>
    <updated>2017-10-02T07:55:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用API"><a href="#使用API" class="headerlink" title="使用API"></a>使用API</h1><h2 id="API与普通网站的区别"><a href="#API与普通网站的区别" class="headerlink" title="API与普通网站的区别"></a>API与普通网站的区别</h2><p>1.API请求使用非常严谨的语法 2.其次API用JSON或XML格式表示数据，而不是HTML</p><h2 id="API的通用规则"><a href="#API的通用规则" class="headerlink" title="API的通用规则"></a>API的通用规则</h2><p>第一次使用API的时候，建议阅读文档</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>利用HTTP从网络服务获取信息的四种方式：<br>1.GET<br>2.POST<br>3.PUT<br>4.DELETE</p><h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><h3 id="服务器响应"><a href="#服务器响应" class="headerlink" title="服务器响应"></a>服务器响应</h3><p>XML/JSON -&gt; json更小，在js框架中更好处理<br>API调用</p><h3 id="Echo-Nest"><a href="#Echo-Nest" class="headerlink" title="Echo Nest"></a>Echo Nest</h3><h3 id="解析JSON数据"><a href="#解析JSON数据" class="headerlink" title="解析JSON数据"></a>解析JSON数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">import json</div><div class="line">from urllib.request import urlopen</div><div class="line"></div><div class="line">def getCountry(ipAddress):</div><div class="line">    response = urlopen(<span class="string">"http://freegeoip.net/json/"</span>+ipAddress).<span class="built_in">read</span>().decode(<span class="string">'utf-8'</span>)</div><div class="line">    responseJson = json.loads(response)</div><div class="line">    <span class="built_in">return</span> responseJson.get(<span class="string">'country_code'</span>)</div><div class="line"><span class="built_in">print</span>(getCountry(<span class="string">"50.78.253.58"</span>))</div><div class="line">下面的例子将json字符串处理成python可以处理的json对象</div><div class="line">import json</div><div class="line">jsonString = <span class="string">'&#123;"arrayOfNums":[&#123;"number":0&#125;,&#123;"number":1&#125;,&#123;"number":2&#125;],'</span> \</div><div class="line">             <span class="string">'"arrayOfFruits":[&#123;"fruit":"apple"&#125;,&#123;"fruit":"banana"&#125;,&#123;"fruit":"pear"&#125;]&#125;'</span></div><div class="line">jsonObj = json.loads(jsonString)</div><div class="line"><span class="built_in">print</span>(jsonObj.get(<span class="string">"arrayOfNums"</span>))</div><div class="line"><span class="built_in">print</span>(jsonObj.get(<span class="string">"arrayOfNums"</span>)[1])</div><div class="line"><span class="built_in">print</span>(jsonObj.get(<span class="string">"arrayOfNums"</span>)[1].get(<span class="string">"number"</span>)+</div><div class="line">jsonObj.get(<span class="string">"arrayOfNums"</span>)[2].get(<span class="string">"number"</span>))</div><div class="line"><span class="built_in">print</span>(jsonObj.get(<span class="string">"arrayOfFruits"</span>)[2].get(<span class="string">"fruit"</span>))</div><div class="line">可以着重注意一下load()函数</div></pre></td></tr></table></figure><h3 id="Python的集合类型简介"><a href="#Python的集合类型简介" class="headerlink" title="Python的集合类型简介"></a>Python的集合类型简介</h3><p>集合是无序的，set集合的好处是它存储的值不会重复<br>在未来可能需要扩展的代码，在决定使用集合还是列表时，有两件事情需要考虑：虽然列表迭代速度比集合稍微快一点，<br>但集合查找速度更快</p><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><pre><code class="bash">from urllib.request import urlopen,HTTPErrorfrom bs4 import BeautifulSoupimport datetimeimport randomimport reimport jsonrandom.seed(datetime.datetime.now())def getLinks(articleUrl):    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+articleUrl)    bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)    <span class="built_in">return</span> bsObj.find(<span class="string">"div"</span>,{<span class="string">"id"</span>:<span class="string">"bodyContent"</span>}).findAll(<span class="string">"a"</span>,                                                          href=re.compile(<span class="string">"^(/wiki/)((?!:).)*$"</span>))def getHistoryIPs(pageUrl):    <span class="comment"># 编辑历史页面URL链接格式是：</span>    <span class="comment"># http://en.wikipedia.org/w/index.php?title=Title_in_URL&amp;action=history</span>    pageUrl = pageUrl.replace(<span class="string">"/wiki/"</span>,<span class="string">""</span>)    historyUrl = <span class="string">"http://en.wikipedia.org/w/index.php?title="</span> +pageUrl+<span class="string">"&amp;action=history"</span>    <span class="built_in">print</span>(<span class="string">"history url is: "</span> + historyUrl)    html = urlopen(historyUrl)    bsObj = BeautifulSoup(html,<span class="string">'html.parser'</span>)    <span class="comment"># 找出class属性是"mw-anonuserlink"的链接</span>    <span class="comment"># 它们用IP地址代替用户</span>    ipAddressse = bsObj.findAll(<span class="string">"a"</span>,{<span class="string">"class"</span>:<span class="string">"mw-anonuserlink"</span>})    addressList = <span class="built_in">set</span>()    <span class="keyword">for</span> ipAddress <span class="keyword">in</span> ipAddressse:        addressList.add(ipAddress.get_text())        <span class="built_in">return</span> addressListdef getCountry(ipAddress):    try:        response = urlopen(<span class="string">"http://freegeoip.net/json/"</span>+ipAddress)\            .<span class="built_in">read</span>().decode(<span class="string">'utf-8'</span>)    except HTTPError:        <span class="built_in">return</span> None    responseJson = json.loads(response)    <span class="built_in">return</span> responseJson.get(<span class="string">"country_code"</span>)links = getLinks(<span class="string">"/wiki/Python_(programming_language)"</span>)<span class="keyword">while</span>(len(links) &gt; 0):    <span class="keyword">for</span> link <span class="keyword">in</span> links:        <span class="built_in">print</span>(<span class="string">"------------------------"</span>)        historyIPs = getHistoryIPs(link.attrs[<span class="string">"href"</span>])        <span class="keyword">for</span> historyIP <span class="keyword">in</span> historyIPs:            country = getCountry(historyIP)            <span class="keyword">if</span> country is not None:                <span class="built_in">print</span>(historyIP+ <span class="string">" is from "</span> + country)    newLink = links[random.randint(0, len(links)-1)].attrs[<span class="string">'herf'</span>]    links = getLinks(newLink)</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;使用API&quot;&gt;&lt;a href=&quot;#使用API&quot; class=&quot;headerlink&quot; title=&quot;使用API&quot;&gt;&lt;/a&gt;使用API&lt;/h1&gt;&lt;h2 id=&quot;API与普通网站的区别&quot;&gt;&lt;a href=&quot;#API与普通网站的区别&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（三）</title>
    <link href="http://yoursite.com/2017/09/30/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/30/python数据收集（三）/</id>
    <published>2017-09-30T12:10:29.000Z</published>
    <updated>2017-10-02T03:01:06.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开始采集"><a href="#开始采集" class="headerlink" title="开始采集"></a>开始采集</h1><h2 id="遍历单个域名"><a href="#遍历单个域名" class="headerlink" title="遍历单个域名"></a>遍历单个域名</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import datetime</div><div class="line">import random</div><div class="line">import re</div><div class="line"></div><div class="line">random.seed(datetime.datetime.now())</div><div class="line">def getLinks(articleUrl):</div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+articleUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    <span class="built_in">return</span> bsObj.find(<span class="string">"div"</span>, &#123;<span class="string">"id"</span>:<span class="string">"bodyContent"</span>&#125;).findAll(<span class="string">"a"</span>,</div><div class="line">                          href=re.compile(<span class="string">"^(/wiki/)((?!:).)*$"</span>))</div><div class="line">links = getLinks(<span class="string">"/wiki/Kevin_Bacon"</span>)</div><div class="line"><span class="keyword">while</span> len(links) &gt; 0:</div><div class="line">    newArticle = links[random.randint(0, len(links)-1)].attrs[<span class="string">"href"</span>]</div><div class="line">    <span class="built_in">print</span>(newArticle)</div><div class="line">    links = getLinks(newArticle)</div><div class="line"><span class="comment">#伪随机数和随机数种子</span></div><div class="line">random.seed(datetime.datetime.now()) <span class="comment">#种子重复，会导致随机数重复，所以这里采用系统时间作为种子</span></div><div class="line">random.randint(0,len(links)-1) <span class="comment">#返回0-len(links)-1的随机整数</span></div><div class="line"><span class="comment">#python的伪随机数生成器用的是梅森旋转算法，它产生的随机数很难预测且均匀分布</span></div></pre></td></tr></table></figure><p>#要注意异常处理</p><h2 id="采集整个网站"><a href="#采集整个网站" class="headerlink" title="采集整个网站"></a>采集整个网站</h2><p>1.深网（deep Web）和暗网(dark Web)<br>2.遍历整个网站的网络数据采集有许多好处<br>  <1 生成网站地图="" <2="" 收集数据="" 链接去重="" 集合set类型="" <figure="" class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line"></div><div class="line">pages = <span class="built_in">set</span>()</div><div class="line">def getLinks(pageUrl):</div><div class="line">    global pages</div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+pageUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> bsObj.findAll(<span class="string">"a"</span>, href=re.compile(<span class="string">"^(/wiki/)"</span>)):</div><div class="line">        <span class="keyword">if</span> <span class="string">'href'</span> <span class="keyword">in</span> link.attrs:</div><div class="line">            <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] not <span class="keyword">in</span> pages:</div><div class="line">                <span class="comment"># 我们遇到了新页面</span></div><div class="line">                newPage = link.attrs[<span class="string">'href'</span>]</div><div class="line">                <span class="built_in">print</span>(newPage)</div><div class="line">                pages.add(newPage)</div><div class="line">                getLinks(newPage)</div><div class="line">getLinks(<span class="string">""</span>)</div><div class="line"><span class="comment">#关于递归的警告</span></div><div class="line">如果递归运行的次数非常多，前面的递归程序就很可能崩溃</div><div class="line">python默认的递归限制（自我调用次数）是1000次</div><div class="line"><span class="comment">### 收集整个网站数据</span></div><div class="line">``` bash</div><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">pages = <span class="built_in">set</span>()</div><div class="line">def getLinks(pageUrl):</div><div class="line">    global pages</div><div class="line">    html = urlopen(<span class="string">"http://en.wikipedia.org"</span>+pageUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    try:</div><div class="line">        <span class="built_in">print</span>(bsObj.h1.get_text())</div><div class="line">        <span class="built_in">print</span>(bsObj.find(id=<span class="string">"mw-content-text"</span>).findAll(<span class="string">"p"</span>)[0])</div><div class="line">        <span class="built_in">print</span>(bsObj.find(id=<span class="string">"ca-edit"</span>).find(<span class="string">"span"</span>).find(<span class="string">"a"</span>).attrs[<span class="string">'href'</span>])</div><div class="line">    except AttributeError: <span class="built_in">print</span>(<span class="string">"页面缺少一些属性!不过不用担心!"</span>)</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> bsObj.findAll(<span class="string">"a"</span>, href=re.compile(<span class="string">"^(/wiki/)"</span>)):</div><div class="line">        <span class="keyword">if</span> <span class="string">'href'</span> <span class="keyword">in</span> link.attrs:</div><div class="line">            <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] not <span class="keyword">in</span> pages:</div><div class="line">                <span class="comment"># 我们遇到了新页面</span></div><div class="line">                newPage = link.attrs[<span class="string">'href'</span>]</div><div class="line">                <span class="built_in">print</span>(<span class="string">"----------------\n"</span>+newPage)</div><div class="line">                pages.add(newPage)</div><div class="line">                getLinks(newPage)</div><div class="line">getLinks(<span class="string">""</span>)</div></pre></td></tr></table></1></p><p>#在一个异常处理语句中包裹多岗语句显然是有点儿危险的，你没有办法识别除究竟哪行代码出现了异常，其次，如果有个页面没</p><p>#有前面的标题内容，却有”编辑”按，那么由于前面已经发生异常，后面的编辑连接就不会出现</p><p>#偶尔都是一些数据，只要保存详细的日志就不是什么问题了</p><h2 id="通过互联网采集（多个域名采集）"><a href="#通过互联网采集（多个域名采集）" class="headerlink" title="通过互联网采集（多个域名采集）"></a>通过互联网采集（多个域名采集）</h2><p>相比之前的单个域名采集，互联网采集要难得多——不同网站的布局迥然不同。这就意味着我们必须在要寻找的信息<br>以及查找方式上都极具灵活性<br>在你写爬虫随意跟外链跳转之前，请问自己几个问题<br>1.我要搜集那些数据？这些数据可以通过采集几个已经确定的网站完成吗？（永远是最简单的做法）？<br>  或者我的爬虫需要发现那些我可能不知道的网站吗？<br>2.当我的爬虫到某个网站，它是立即顺着下一个出站链接跳到一个新网站，还是在网站上待一会儿，深入采集网站的内容<br>3.有没有我不想采集的一类网站？我对非英文网站的内容感兴趣吗？<br>4.如果我的网络爬虫引起了某个网管的怀疑，我该如何避免法律责任</p><p>几个灵活的python函数组合起来就可以实现不同类型的网络爬虫<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">import datetime</div><div class="line">import random</div><div class="line">pages = <span class="built_in">set</span>()</div><div class="line">random.seed(datetime.datetime.now())</div><div class="line"><span class="comment"># 获取页面所有内链的列表</span></div><div class="line">def getInternalLinks(bsObj, includeUrl):</div><div class="line">    internalLinks = []</div><div class="line">    <span class="comment"># 找出所有以"/"开头的链接</span></div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> bsObj.findAll(<span class="string">"a"</span>, href=re.compile(<span class="string">"^(/|.*"</span>+includeUrl+<span class="string">")"</span>)):</div><div class="line">        <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] is not None:</div><div class="line">            <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] not <span class="keyword">in</span> internalLinks:</div><div class="line">                internalLinks.append(link.attrs[<span class="string">'href'</span>])</div><div class="line">    <span class="built_in">return</span> internalLinks</div><div class="line"><span class="comment"># 获取页面所有外链的列表</span></div><div class="line">def getExternalLinks(bsObj, excludeUrl):</div><div class="line">    externalLinks = []</div><div class="line">    <span class="comment"># 找出所有以"http"或"www"开头且不包含当前URL的链接</span></div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> bsObj.findAll(<span class="string">"a"</span>,href=re.compile(<span class="string">"^(http|www)((?!"</span> + excludeUrl + <span class="string">").)*$"</span>)):</div><div class="line">        <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] is not None:</div><div class="line">            <span class="keyword">if</span> link.attrs[<span class="string">'href'</span>] not <span class="keyword">in</span> externalLinks: externalLinks.append(link.attrs[<span class="string">'href'</span>])</div><div class="line">    <span class="built_in">return</span> externalLinks</div><div class="line">def splitAddress(address):</div><div class="line">    addressParts = address.replace(<span class="string">"http://"</span>, <span class="string">""</span>).split(<span class="string">"/"</span>)</div><div class="line">    <span class="built_in">return</span> addressParts</div><div class="line">def getRandomExternalLink(startingPage):</div><div class="line">    html = urlopen(startingPage)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0])</div><div class="line">    <span class="keyword">if</span> len(externalLinks) == 0:</div><div class="line">        internalLinks = getInternalLinks(startingPage)</div><div class="line">        <span class="built_in">return</span> getExternalLinks(internalLinks[random.randint(0,len(internalLinks) - 1)])</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="built_in">return</span> externalLinks[random.randint(0, len(externalLinks) - 1)]</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def followExternalOnly(startingSite):</div><div class="line">    externalLink = getRandomExternalLink(<span class="string">"http://oreilly.com"</span>)</div><div class="line">    <span class="built_in">print</span>(<span class="string">"随机外链是:"</span>+externalLink)</div><div class="line">    followExternalOnly(externalLink)</div><div class="line">followExternalOnly(<span class="string">"http://oreilly.com"</span>)</div></pre></td></tr></table></figure></p><p>#网站首页上并不能保证一直能发现外链。这时为了能够发现外链，就需要用一种类似前面案例中使用的采集方法，即递归</p><p>#深入一个网站直到找到一个外链才停止<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">allExtLinks = <span class="built_in">set</span>()</div><div class="line">allIntLinks = <span class="built_in">set</span>()</div><div class="line">def getAllExternalLinks(siteUrl):</div><div class="line">    html = urlopen(siteUrl)</div><div class="line">    bsObj = BeautifulSoup(html)</div><div class="line">    internalLinks = getInternalLinks(bsObj, splitAddress(siteUrl)[0])</div><div class="line">    externalLinks = getExternalLinks(bsObj, splitAddress(siteUrl)[0])</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> externalLinks:</div><div class="line">        <span class="keyword">if</span> link not <span class="keyword">in</span> allExtLinks:</div><div class="line">            allExtLinks.add(link)</div><div class="line">        <span class="built_in">print</span>(link)</div><div class="line">    <span class="keyword">for</span> link <span class="keyword">in</span> internalLinks:</div><div class="line">        <span class="keyword">if</span> link not <span class="keyword">in</span> allIntLinks:</div><div class="line">            <span class="built_in">print</span>(<span class="string">"即将获取链接的URL是:"</span> + link)</div><div class="line">        allIntLinks.add(link)</div><div class="line">        getAllExternalLinks(link)</div><div class="line">getAllExternalLinks(<span class="string">"http://oreilly.com"</span>)</div></pre></td></tr></table></figure></p><p>#写代码之前拟个大纲或画个流程图是很好的编程习惯，这么做不仅可以为你后期处理节省好多时间，更重要的是可以防止<br>自己在爬虫变得越来越复杂是乱了方寸<br>*处理网页重定向<br> 重定向分为两种：服务器端重定向，客户端重定向；在服务器端的重定向urllib库一般会给你自动处理，但是你要注意，<br> 你要采集的页面URL可能并不是你当前所在的页面的URL</p><h2 id="用Scrapy采集"><a href="#用Scrapy采集" class="headerlink" title="用Scrapy采集"></a>用Scrapy采集</h2><h3 id="创建新的Scrapy项目"><a href="#创建新的Scrapy项目" class="headerlink" title="创建新的Scrapy项目"></a>创建新的Scrapy项目</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="variable">$scrapy</span> startproject wikiSpider</div><div class="line"><span class="comment">#然后在spiders文件夹（一定要注意路径）里创建一个articleSpider.py文件，另外需要在items.py文件中，我们需要定义一个Article类</span></div><div class="line">from scrapy import Item,Field</div><div class="line">class Ariticle(Item): <span class="comment"># 每个Item对象表示网站上的一个页面 我们现在只收集title字段</span></div><div class="line">    <span class="comment"># define the fields for your item here like:</span></div><div class="line">    <span class="comment"># name = scrapy.Field()</span></div><div class="line">    title = Field()</div></pre></td></tr></table></figure><h3 id="填坑？"><a href="#填坑？" class="headerlink" title="填坑？"></a>填坑？</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#from scrapy.contrib.spiders import CrawlSpider, Rule</span><span class="comment">#from wikiSpider.items import Article</span><span class="comment">#from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor</span></div><div class="line">from scrapy.spiders import CrawlSpider, Rule</div><div class="line">from wikiSpider.items import Article</div><div class="line">from scrapy.linkextractors import LinkExtractor</div><div class="line"></div><div class="line">class ArticleSpider(CrawlSpider):</div><div class="line">    name=<span class="string">"article"</span></div><div class="line">    allowed_domains = [<span class="string">"en.wikipedia.org"</span>]</div><div class="line">    start_urls = [<span class="string">"http://en.wikipedia.org/wiki/Main_Page"</span>,</div><div class="line">                    <span class="string">"http://en.wikipedia.org/wiki/Python_%28programming_language%29"</span>]</div><div class="line">    rules = [Rule(LinkExtractor(allow=(<span class="string">'(/wiki/)((?!:).)*$'</span>),),</div><div class="line">                                            callback=<span class="string">"parse_item"</span>, follow=True)]</div><div class="line">    def parse(self, response):</div><div class="line">        item = Article()</div><div class="line">        title = response.xpath(<span class="string">'//h1/text()'</span>)[0].extract()</div><div class="line">        <span class="built_in">print</span>(<span class="string">"Title is: "</span>+title)</div><div class="line">        item[<span class="string">'title'</span>] = title</div><div class="line">        <span class="built_in">return</span> item</div></pre></td></tr></table></figure><p>由于我用的是scrapy1.4+python3,上面是书上原来的代码scrapy版本不详，python2.7，使用上面的代码运行，你会发现好多的函数和包都丢弃或者换位置了，虽然本机有2.7python 但是scrapy的版本不知道该如何弄，所以找了1.4的版本的文旦<br>查阅了一下，改了一下包引用以及函数，但是结果不知道是否达到预期，慢慢学习吧</p><h3 id="Scrapy处理日志"><a href="#Scrapy处理日志" class="headerlink" title="Scrapy处理日志"></a>Scrapy处理日志</h3><p>五种级别日志 CRITICAL ERROR WARNING DEBUG INFO<br>如果级别设置为ERROR，那么只有CRITICAL 和ERROR日志会显示出来<br>将日志输出到一个独立的文件中<br>$ scrapy crawl article -s LOG_FILE=wiki.log<br>如果目录中没有wiki.log,那么运行程序会创建一个新文件，然后把所有的日志都保存在里面，如果已经存在，会在原文后面加入新的日志文件</p><h3 id="输出不同格式"><a href="#输出不同格式" class="headerlink" title="输出不同格式"></a>输出不同格式</h3><p>$ scrapy crawl article -o articles.csv -t csv<br>$ scrapy crawl article -o articles.json -t json<br>$ scrapy crawl article -o articles.xml -t xml<br>当然，你也可以自定义Item对象</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;开始采集&quot;&gt;&lt;a href=&quot;#开始采集&quot; class=&quot;headerlink&quot; title=&quot;开始采集&quot;&gt;&lt;/a&gt;开始采集&lt;/h1&gt;&lt;h2 id=&quot;遍历单个域名&quot;&gt;&lt;a href=&quot;#遍历单个域名&quot; class=&quot;headerlink&quot; title=&quot;遍历单个域
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（二）</title>
    <link href="http://yoursite.com/2017/09/30/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/30/python数据收集（二）/</id>
    <published>2017-09-30T07:54:29.000Z</published>
    <updated>2017-09-30T12:03:43.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="复杂的HTML解析"><a href="#复杂的HTML解析" class="headerlink" title="复杂的HTML解析"></a>复杂的HTML解析</h1><h2 id="不是一直都要用锤子"><a href="#不是一直都要用锤子" class="headerlink" title="不是一直都要用锤子"></a>不是一直都要用锤子</h2><p>例如：<br>$ bsObj.findAll(“table”)[4].findAll(“tr”)[2].find(“td”).findAll(“div”)[1].find(“a”)<br>虽然可以达到目标，但是除了代码欠缺美感之外，还有就是管理员对网页稍作修改，代码就会失效，甚至会毁了整个<br>网络爬虫<br>如何解决该问题呢？<br>1.寻找样式更友好的移动版或者看看有没有打印此页的链接<br>2.寻找隐藏在javascript文件里的信息<br>3.你要的信息或许也可以从网页的URL链接里获取<br>4.找找你要的信息是不是该网站从别的网站上抓取出来的</p><h2 id="再端一碗BeautifulSoup"><a href="#再端一碗BeautifulSoup" class="headerlink" title="再端一碗BeautifulSoup"></a>再端一碗BeautifulSoup</h2><h3 id="利用好css中的class和ID属性"><a href="#利用好css中的class和ID属性" class="headerlink" title="利用好css中的class和ID属性"></a>利用好css中的class和ID属性</h3><p>ex:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line"></div><div class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com/pages/warandpeace.html"</span>)</div><div class="line">bsObj = BeautifulSoup(html,<span class="string">"html.parser"</span>)</div><div class="line">nameList = bsObj.findAll(<span class="string">"span"</span>,&#123;<span class="string">"class"</span>:<span class="string">"green"</span>&#125;)</div><div class="line"><span class="keyword">for</span> name <span class="keyword">in</span> nameList:</div><div class="line">    <span class="built_in">print</span>(name.get_text())</div><div class="line"><span class="comment">#使用name.get_text()会将标签中的内容提取出来，如果你正在处理一个包含许多超链接，段落和标签的</span></div><div class="line"><span class="comment">#大段源码，.get_text()会将这些处理成只剩下一串不带标签的文字，一般情况下，尽可能的保留HTML文档的标签结构</span></div></pre></td></tr></table></figure></p><h3 id="BeautifulSoup的find-和findAll"><a href="#BeautifulSoup的find-和findAll" class="headerlink" title="BeautifulSoup的find()和findAll()"></a>BeautifulSoup的find()和findAll()</h3><p>函数定义：<br>findAll(tag, attributes, recursive, text, limit, keywords)<br>find(tag, attributes, recursive, text, keywords)<br>下面来介绍一下各个参数<br>recursive:bool型 True：查找标签参数的所有子标签（默认）<br>        False：只查找文档的一级标签<br>text:更具标签的文本内容去匹配<br>$ nameList = bsObj.findAll(text=”the prince”)<br>$ print(len(nameList))<br>limit:find等价于finall的limit等于1，获取前几项是按照网页上的顺序排序的<br>keyword<br>以下每组效果相同<br>$ bsObj.findAll(id=”text”)<br>$ bsObj.findAll(“”, {“id”:”text”})</p><p>$ bsObj.findAll(class<em>=”green”) #由于class是python的保留字，所以必须在其后面添加</em><br>$ bsObj.findAll(“”, {“class”:”green”})</p><h3 id="其他BeautifulSoup对象"><a href="#其他BeautifulSoup对象" class="headerlink" title="其他BeautifulSoup对象"></a>其他BeautifulSoup对象</h3><p>常用对象： 1.BeautifulSoup 2.Tag对象（前面已经提到过，例如：bsObj.div.h1）<br>不常用对象：NavigableString对象：用来表示标签里的文字<br>     Comment 对象：用来查找HTML文档的注释标签 &lt;!—- 像这样 —&gt;</p><h3 id="导航树"><a href="#导航树" class="headerlink" title="导航树"></a>导航树</h3><p>如果你需要通过标签在文档中的位置来查找标签，就使用导航树(Navigating Trees)<br>$ bsObj.tag.subTag.anotherSubTag</p><h4 id="处理子标签和其他后代标签"><a href="#处理子标签和其他后代标签" class="headerlink" title="处理子标签和其他后代标签"></a>处理子标签和其他后代标签</h4><p>$ bsObj.div.findAll(“img”)<br>如果你只想找出子标签，可以用.children标签<br>$ from urllib.request import urlopen<br>$ from bs4 import BeautifulSoup<br>$ html = urlopen(“<a href="http://www.pythonscraping.com/pages/page3.html" target="_blank" rel="external">http://www.pythonscraping.com/pages/page3.html</a>“)<br>$ bsObj = BeautifulSoup(html)<br>$ for child in bsObj.find(“table”,{“id”:”giftList”}).children:<br>    print(child)<br>如果你用descendants()函数，就会将其所有后代标签打印出来</p><h4 id="处理兄弟标签"><a href="#处理兄弟标签" class="headerlink" title="处理兄弟标签"></a>处理兄弟标签</h4><p>$ from urllib.request import urlopen<br>$ from bs4 import BeautifulSoup<br>$ html = urlopen(“<a href="http://www.pythonscraping.com/pages/page3.html" target="_blank" rel="external">http://www.pythonscraping.com/pages/page3.html</a>“)<br>$ bsObj = BeautifulSoup(html)<br>$ for sibling in bsObj.find(“table”,{“id”:”giftList”}).tr.next_siblings:<br>    print(sibling)</p><p>#这里需要注意的是，next_siblings只会找到他后面的兄弟标签</p><p>#让标签的选择更具体(如果可能的话）<br>$ bsObj.find(“table”,{“id”:”giftList”}).tr</p><p>#罗列一下查找子标签的四个函数<br>next_sibling 和 previous_sibling<br>next_siblings 和 previous_siblings<br>从命名规范中我们也可以发现其各自的用途以及区别，这里不在赘述</p><h4 id="父标签处理"><a href="#父标签处理" class="headerlink" title="父标签处理"></a>父标签处理</h4><p>$ from urllib.request import urlopen<br>$ from bs4 import BeautifulSoup<br>$ html = urlopen(“<a href="http://www.pythonscraping.com/pages/page3.html" target="_blank" rel="external">http://www.pythonscraping.com/pages/page3.html</a>“)<br>$ bsObj = BeautifulSoup(html)<br>$ print(bsObj.find(“img”,{“src”:”../img/gifts/img1.jpg”}).parent.previous_sibling.get_text())</p><h2 id="正则表达式：如果你有一个问题打算用正则表达式来解决，那么就是两个问题了"><a href="#正则表达式：如果你有一个问题打算用正则表达式来解决，那么就是两个问题了" class="headerlink" title="正则表达式：如果你有一个问题打算用正则表达式来解决，那么就是两个问题了"></a>正则表达式：如果你有一个问题打算用正则表达式来解决，那么就是两个问题了</h2><p>ex：邮箱正则表达式：[A-Za-z0-9._+]+@[A-Za-z]+.(com|org\edu\net)<br>OK,接下来我们学习一下12中python中最常用的正则表达式符号</p><ul><li>: 匹配前面的字符，子表达式或括号里的字符0次或多次    a<em>b</em>      aaaaaaaaa,aaabbb,bbbb</li></ul><ul><li>: 匹配前面的字符，子表达式或括号里的字符至少一次    a+b+       aaaabbb ab<br>[]: 匹配任意一个字符（相当于”任选一个”）        [A-Z]<em>   APPLe,GAPITALS<br>(): 表达式编组（在正则表达式的规划里编组会优先运行）    (a</em>b)<em>      aaabaab<br>{m,n}: 匹配前面的字符，子表达式或括号里的字符m到n次（包含m或n）     a{2,3}b{2,3}<br>[^]: 匹配任意一个不在括号里的字符            [^A-Z]</em>     sdasdasd<br>| : 匹配任意一个由竖线分割哥的字符，子表达式        b(a|i|e) bad,bid,bed<br>. : 匹配任意单个字符（包括符号，数字和空格等）        b.d     bad,bzd,b$d,b d<br>^ : 字符串开始位置的字符或子表达式            ^a     apple,asdf,a<br>\ : 转义字符（把有特殊含义的字符转换成字面形式）         . | \<br>$ : 经常用在正则表达式的末尾，表示”从字符串的末端匹配”。如果不用它，<br>  每个正则表达式实际都带着’.<em>’模式，只会从字符串的开头进行匹配。这个符号<br>  可以看做是^符号的反义词<br>?!  “不包含”。这个奇怪的组合通常会放在字符或正则表达式前面，表示字符不能<br>  出现在诺表字符串里。字符通常会在字符窜的不同部位出现。如果要在整个字符串<br>  中全部排除某个字符，就加上^和$符号               ^((?![A-Z]).)</em>$    <h2 id="正则表达式和BeautifulSoup"><a href="#正则表达式和BeautifulSoup" class="headerlink" title="正则表达式和BeautifulSoup"></a>正则表达式和BeautifulSoup</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopen</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">import re</div><div class="line">html = urlopen(<span class="string">"http://www.pythonscraping.com/pages/page3.html"</span>)</div><div class="line">bsObj = BeautifulSoup(html)</div><div class="line">images = bsObj.findAll(<span class="string">"img"</span>,&#123;<span class="string">"src"</span>:re.compile(<span class="string">"\.\.\/img\/gifts/img.*\.jpg"</span>)&#125;)</div><div class="line"><span class="keyword">for</span> image <span class="keyword">in</span> images:</div><div class="line">    <span class="built_in">print</span>(image[<span class="string">"src"</span>])</div></pre></td></tr></table></figure></li></ul><h2 id="获取属性"><a href="#获取属性" class="headerlink" title="获取属性"></a>获取属性</h2><p>对于一个标签对象，可以用下面的代码获取它的全部属性：<br>$ myTag.attrs #返回的是Python字典对象<br>$ myImgTag.attrs[“src”]</p><h2 id="Lambda表达式：本质上就是一个函数"><a href="#Lambda表达式：本质上就是一个函数" class="headerlink" title="Lambda表达式：本质上就是一个函数"></a>Lambda表达式：本质上就是一个函数</h2><p>BeautifulSoup允许我们把特定函数类型当做findAll函数的参数-&gt;限定：标签作为参数，返回结果是boolean型<br>BeautifulSoup用这个函数来评估他遇到的每个标签对象，最后把评估结果为真的标签保留，把其他标签剔除<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">soup.findAll(lambda tag: len(tag.attrs) == 2) <span class="comment">#这个在我的环境下不起作用，先标注一下</span></div><div class="line"><span class="comment">#返回结果</span>&lt;div class=<span class="string">"body"</span> id=<span class="string">"content"</span>&gt;&lt;/div&gt;&lt;span style=<span class="string">"color:red"</span> class=<span class="string">"title"</span>&gt;&lt;/span&gt;</div></pre></td></tr></table></figure></p><h2 id="超越BeautifulSoup"><a href="#超越BeautifulSoup" class="headerlink" title="超越BeautifulSoup"></a>超越BeautifulSoup</h2><p>lxml -&gt; <a href="http://lxml.de/" target="_blank" rel="external">http://lxml.de/</a> 用c语言写的，处理绝大多数HTML文档时的速度都非常快（学习成本高）<br>HTML parser -&gt; python自带解析库，不需要重新安装</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;复杂的HTML解析&quot;&gt;&lt;a href=&quot;#复杂的HTML解析&quot; class=&quot;headerlink&quot; title=&quot;复杂的HTML解析&quot;&gt;&lt;/a&gt;复杂的HTML解析&lt;/h1&gt;&lt;h2 id=&quot;不是一直都要用锤子&quot;&gt;&lt;a href=&quot;#不是一直都要用锤子&quot; class
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>python数据收集（一）</title>
    <link href="http://yoursite.com/2017/09/30/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/30/python数据收集（一）/</id>
    <published>2017-09-30T02:46:43.000Z</published>
    <updated>2017-09-30T07:52:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>之前的python爬虫初体验，是看视频教学的，感觉不太踏实，正好现在需要系统的学一下爬虫，所有就找了一本python数据采集的书来看看，为了区分学习计划，所以将这个作为数据采集第一篇笔记，加油，坚持下去！</p><h1 id="初见爬虫"><a href="#初见爬虫" class="headerlink" title="初见爬虫"></a>初见爬虫</h1><h2 id="关于urllib"><a href="#关于urllib" class="headerlink" title="关于urllib"></a>关于urllib</h2><p>urllib被分为：urllib.request、urllib.parse和urllib.error 三个子模块</p><h2 id="安装BeautifulSoup"><a href="#安装BeautifulSoup" class="headerlink" title="安装BeautifulSoup"></a>安装BeautifulSoup</h2><h2 id="用虚拟环境保存库文件"><a href="#用虚拟环境保存库文件" class="headerlink" title="用虚拟环境保存库文件"></a>用虚拟环境保存库文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$ virtualenv scrapingEnv</div><div class="line"><span class="comment">#这样就创建了一个叫作scrapingEnv的新环境，你需要先激活它再使用：</span></div><div class="line">$ <span class="built_in">cd</span> scrapingEnv/</div><div class="line">$ <span class="built_in">source</span> bin/activate</div><div class="line"><span class="comment">#你可以在新建的scrapingEnv环境里，安装并使用BeautifulSoup</span></div><div class="line"><span class="comment">#当你不在使用虚拟机环境的库时，可以通过释放命令来退出环境：</span></div><div class="line">$ deactivate</div></pre></td></tr></table></figure><h2 id="运行BeautifulSoup"><a href="#运行BeautifulSoup" class="headerlink" title="运行BeautifulSoup"></a>运行BeautifulSoup</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(<span class="string">"http://www.pythonscraping.com/pages/page1.html"</span>) bsObj = BeautifulSoup(html.read())<span class="built_in">print</span>(bsObj.h1)</div></pre></td></tr></table></figure><h2 id="可靠的网络连接"><a href="#可靠的网络连接" class="headerlink" title="可靠的网络连接"></a>可靠的网络连接</h2><h3 id="html-urlopen-“http-news-baidu-com-“-是如何处理异常的？"><a href="#html-urlopen-“http-news-baidu-com-“-是如何处理异常的？" class="headerlink" title="html = urlopen(“http://news.baidu.com/“)是如何处理异常的？"></a>html = urlopen(“<a href="http://news.baidu.com/“)是如何处理异常的？" target="_blank" rel="external">http://news.baidu.com/“)是如何处理异常的？</a></h3><p>这行代码主要可能会发生两种异常</p><ol><li><p>页面在服务器上不存在<br> 404 Page Not Found<br> 500 Internal Server Error<br>所有这些类似的情形，urlopen函数都会抛出”HTTPError” 异常<br>处理这种异常的方式：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">try:html = urlopen(<span class="string">"http://news.baidu.com/"</span>)except HTTPError as e: </div><div class="line"><span class="built_in">print</span>(e)<span class="comment"># 返回空值，中断程序，或者执行另一个方案 </span></div><div class="line"><span class="keyword">else</span>:<span class="comment"># 程序继续。注意:如果你已经在上面异常捕捉那一段代码里返回或中断(break)， </span></div><div class="line"><span class="comment"># 那么就不需要使用else语句了，这段代码也不会执行</span></div></pre></td></tr></table></figure></li><li><p>服务器不存在</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> html is None:<span class="built_in">print</span>(<span class="string">"URL is not found"</span>)<span class="keyword">else</span>:<span class="comment"># 程序继续</span></div></pre></td></tr></table></figure></li><li><p>标签不存在</p><pre><code class="bash"> <span class="comment">#如果BeautifulSoup对象里面没有nonExistentTag标签</span> <span class="built_in">print</span>(bsObj.nonExistentTag) <span class="comment">#返回None对象</span> <span class="comment">#如果这个时候在继续调用nonExistentTag的子标签</span> <span class="built_in">print</span>(bsObj.nonExistentTag.someTag) <span class="comment">#会抛出 AttributeError: 'NoneType' object has no attribute 'someTag'</span>解决方法： try:     badContent = bsObj.nonExistingTag.anotherTag except AttributeError as e:      <span class="built_in">print</span>(<span class="string">"Tag was not found"</span>) <span class="keyword">else</span>:     <span class="keyword">if</span> badContent == None:         <span class="built_in">print</span> (<span class="string">"Tag was not found"</span>)      <span class="keyword">else</span>:         <span class="built_in">print</span>(badContent)</code></pre><p>我们会发现，这样写会有些累赘，换一种写法<br>from urllib.request import urlopen<br>from urllib.error import HTTPError<br>from bs4 import BeautifulSoup<br>def getTitle(url):<br> try:</p><pre><code>html = urlopen(url) </code></pre><p> except HTTPError as e:</p><pre><code>return None </code></pre><p> try:</p><pre><code>        bsObj = BeautifulSoup(html.read())title = bsObj.body.h1 </code></pre><p> except AttributeError as e:</p><pre><code>return None </code></pre><p> return title<br>title = getTitle(“<a href="http://news.baidu.com/" target="_blank" rel="external">http://news.baidu.com/</a>“)<br>if title == None:<br> print(“Title could not be found”)<br>else:<br> print(title)</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;之前的python爬虫初体验，是看视频教学的，感觉不太踏实，正好现在需要系统的学一下爬虫，所有就找了一本python数据采集的书来看看，为了区分学习计划，所以将这个作为数据采集第一篇笔记，加油，坚持下去！&lt;/p&gt;
&lt;h1 id=&quot;初见爬虫&quot;&gt;&lt;a href=&quot;#初见爬虫&quot; 
      
    
    </summary>
    
      <category term="python数据收集" scheme="http://yoursite.com/categories/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
    
      <category term="python数据收集" scheme="http://yoursite.com/tags/python%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>概率论与数理统计（一）</title>
    <link href="http://yoursite.com/2017/09/28/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/28/概率论与数理统计（一）/</id>
    <published>2017-09-28T10:25:13.000Z</published>
    <updated>2017-09-28T12:02:08.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="离散型随机变量及其分布规律"><a href="#离散型随机变量及其分布规律" class="headerlink" title="离散型随机变量及其分布规律"></a>离散型随机变量及其分布规律</h1><h2 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a>0-1分布</h2><h2 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h2><h2 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h2><h1 id="随机变量的分布函数"><a href="#随机变量的分布函数" class="headerlink" title="随机变量的分布函数"></a>随机变量的分布函数</h1><p>#连续性随机变量及其概率密度</p><h2 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h2><h2 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h2><h2 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h2><h1 id="多维随机变量及其分布"><a href="#多维随机变量及其分布" class="headerlink" title="多维随机变量及其分布"></a>多维随机变量及其分布</h1><h2 id="二维随机变量"><a href="#二维随机变量" class="headerlink" title="二维随机变量"></a>二维随机变量</h2><h2 id="边缘分布"><a href="#边缘分布" class="headerlink" title="边缘分布"></a>边缘分布</h2><h2 id="条件分布"><a href="#条件分布" class="headerlink" title="条件分布"></a>条件分布</h2><h2 id="互相独立的随机变量"><a href="#互相独立的随机变量" class="headerlink" title="互相独立的随机变量"></a>互相独立的随机变量</h2><h2 id="两个随机变量的函数的分布"><a href="#两个随机变量的函数的分布" class="headerlink" title="两个随机变量的函数的分布"></a>两个随机变量的函数的分布</h2><h3 id="卷积公式"><a href="#卷积公式" class="headerlink" title="卷积公式"></a>卷积公式</h3><h1 id="随机变量的数字特征"><a href="#随机变量的数字特征" class="headerlink" title="随机变量的数字特征"></a>随机变量的数字特征</h1><h2 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h2><h2 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h2><h2 id="协方差与相关系数"><a href="#协方差与相关系数" class="headerlink" title="协方差与相关系数"></a>协方差与相关系数</h2><h2 id="矩、协方差矩阵"><a href="#矩、协方差矩阵" class="headerlink" title="矩、协方差矩阵"></a>矩、协方差矩阵</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;离散型随机变量及其分布规律&quot;&gt;&lt;a href=&quot;#离散型随机变量及其分布规律&quot; class=&quot;headerlink&quot; title=&quot;离散型随机变量及其分布规律&quot;&gt;&lt;/a&gt;离散型随机变量及其分布规律&lt;/h1&gt;&lt;h2 id=&quot;0-1分布&quot;&gt;&lt;a href=&quot;#0-1分
      
    
    </summary>
    
      <category term="概率论" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="概率论" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（四）</title>
    <link href="http://yoursite.com/2017/09/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/27/机器学习实战（四）/</id>
    <published>2017-09-27T13:38:18.000Z</published>
    <updated>2017-09-29T12:51:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>为什么先写四，而没有二，三呢？二三的代码都码了一遍，理解的还不是很充分，今天看到四，所以先把四的概念写写：朴素贝叶斯</p><h1 id="基于概率论的分类方法：朴素贝叶斯"><a href="#基于概率论的分类方法：朴素贝叶斯" class="headerlink" title="基于概率论的分类方法：朴素贝叶斯"></a>基于概率论的分类方法：朴素贝叶斯</h1><p>贝叶斯决策论的核心思想：选着具有最高概率的决策</p><h2 id="条件概率-p-x-y-c1"><a href="#条件概率-p-x-y-c1" class="headerlink" title="条件概率 p(x,y|c1)"></a>条件概率 p(x,y|c1)</h2><p><img src="https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D113/sign=62231e180df431adb8d247387837ac0f/35a85edf8db1cb1399c0c799dc54564e93584b8b.jpg" alt="“条件概率公式”"><br><img src="https://gss1.bdstatic.com/9vo3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D280/sign=51f198b6c4fdfc03e178e4b0e43f87a9/aec379310a55b31972c9ec3d44a98226cffc1741.jpg" alt="“贝叶斯公式”"></p><h2 id="使用条件概率来分类"><a href="#使用条件概率来分类" class="headerlink" title="使用条件概率来分类"></a>使用条件概率来分类</h2><h2 id="词集模型-词袋模型"><a href="#词集模型-词袋模型" class="headerlink" title="词集模型 词袋模型"></a>词集模型 词袋模型</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;为什么先写四，而没有二，三呢？二三的代码都码了一遍，理解的还不是很充分，今天看到四，所以先把四的概念写写：朴素贝叶斯&lt;/p&gt;
&lt;h1 id=&quot;基于概率论的分类方法：朴素贝叶斯&quot;&gt;&lt;a href=&quot;#基于概率论的分类方法：朴素贝叶斯&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习实战（一）</title>
    <link href="http://yoursite.com/2017/09/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/24/机器学习实战（一）/</id>
    <published>2017-09-24T11:27:53.000Z</published>
    <updated>2017-09-24T14:00:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天开始了《机器学习实战》的学习，下面就是读书笔记咯</p><h1 id="numpy函数库基础"><a href="#numpy函数库基础" class="headerlink" title="numpy函数库基础"></a>numpy函数库基础</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">random.rand(4,4)</div><div class="line"><span class="comment">#调用mat()函数可以将数组转化为矩阵</span></div><div class="line">randMat = mat(random.rand(4,4)</div><div class="line"><span class="comment">#逆矩阵</span></div><div class="line">invRandMat = randMat.I</div><div class="line"><span class="comment">#逆矩阵*矩阵</span></div><div class="line">invRandMat * randMat <span class="comment">#这里应该是单位矩阵的，但是计算机运算有误差</span></div><div class="line"><span class="comment">#计算误差值</span></div><div class="line">myEye = randMat * invRandMat</div><div class="line">myEye - eye(4) <span class="comment">#用我们计算得到的矩阵减单位矩阵</span></div></pre></td></tr></table></figure><h1 id="K-临近算法"><a href="#K-临近算法" class="headerlink" title="K-临近算法"></a>K-临近算法</h1><p>简单来说，K-临近算法采用测量不同特征值之间的距离方法进行分类</p><h2 id="优缺点："><a href="#优缺点：" class="headerlink" title="优缺点："></a>优缺点：</h2><pre><code>优点：精度高，对异常值不敏感，无数据输入假定缺点：计算复杂度高，空间复杂度高适用数据范围：数值型和标称型</code></pre><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">计算已知类别数据集中的每个点以此执行以下操作</div><div class="line">按照距离递增次序排序</div><div class="line">选取与当前点距离最小的k个点</div><div class="line">确定前k个点所在类别的出现频率</div><div class="line">返回前k个点出现频率最高的类别的类当做当前点的预测分类</div></pre></td></tr></table></figure><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">def classify0(inX,dataSet,labels,k):</div><div class="line">    dataSetSize = dataSet.shape[0]</div><div class="line">    diffMat = tile(inX,(dataSetSize,1)) - dataSet</div><div class="line">    sqDiffMat = diffMat**2</div><div class="line">    sqDistances = sqDiffMat.sum(axis=1)</div><div class="line">    distances = sqDistances**0.5</div><div class="line">    sortedDistIndicies = distances.argsort()</div><div class="line">    classCount = &#123;&#125;</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</div><div class="line">        voteIlabel = labels[sortedDistIndicies[i]]</div><div class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1</div><div class="line">    sortedClassCount = sorted(classCount.items(),</div><div class="line">                              key = operator.itemgetter(1),reverse=True)</div><div class="line">    <span class="built_in">return</span> sortedClassCount[0][0]</div></pre></td></tr></table></figure><p>需要注意的是，当你在终端导入一个模块的时候，如果你给这个模块增加了一个新的函数的时候，你必须先退出python3环境，重新导入才能生效<br>array.shape[0] #放回数组的长度<br>tile() # Construct an array by repeating A the number of times given by reps.<br>tile(a,x):   x是控制a重复几次的，结果是一个一维数组<br>tile(a,(x,y))：   结果是一个二维矩阵，其中行数为x，列数是一维数组a的长度和y的乘积<br>tile(a,(x,y,z)):   结果是一个三维矩阵，其中矩阵的行数为x，矩阵的列数为y，而z表示矩阵每个单元格里a重复的次数。(三维矩阵可以看成一个二维矩阵，每个矩阵的单元格里存者一个一维矩阵a)<br>sqDiffMat.sum(axis=1)：将二维数组按行相加，结果是一个一维数组<br>classCount.get(voteIlabel,0)：字典的get方法，查找第一个参数key，如果不存在，返回参数2<br>sorted(classCount.items(),key = operator.itemgetter(1),reverse=True):根据字典中的第二域进行排序<br>sortedClassCount[0][0]：将匹配到的key值取出来</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天开始了《机器学习实战》的学习，下面就是读书笔记咯&lt;/p&gt;
&lt;h1 id=&quot;numpy函数库基础&quot;&gt;&lt;a href=&quot;#numpy函数库基础&quot; class=&quot;headerlink&quot; title=&quot;numpy函数库基础&quot;&gt;&lt;/a&gt;numpy函数库基础&lt;/h1&gt;&lt;figure
      
    
    </summary>
    
      <category term="机器学习" scheme="http://yoursite.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>python网络爬虫 初体验（一）</title>
    <link href="http://yoursite.com/2017/09/24/python%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-%E5%88%9D%E4%BD%93%E9%AA%8C%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://yoursite.com/2017/09/24/python网络爬虫-初体验（一）/</id>
    <published>2017-09-24T09:12:19.000Z</published>
    <updated>2017-09-24T09:16:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天看了一个关于爬虫的视频，心血来潮，就想弄弄爬虫，BUT只弄了一个简单页面，不过也算是体验一把，以后会慢慢深入研究（以后是什么鬼？？）</p><h1 id="代码示例（大神勿喷吐槽）"><a href="#代码示例（大神勿喷吐槽）" class="headerlink" title="代码示例（大神勿喷吐槽）"></a>代码示例（大神勿喷吐槽）</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">import requests</div><div class="line">from bs4 import BeautifulSoup</div><div class="line">from datetime import datetime</div><div class="line"></div><div class="line">res = requests.get(<span class="string">'https://news.sina.cn/gn/2017-09-24/'</span></div><div class="line">                   <span class="string">'detail-ifymenmt6515409.d.html?vt=4&amp;pos=8&amp;wm=8017_0001&amp;cid=56261'</span>)</div><div class="line">res.encoding = <span class="string">'utf-8'</span></div><div class="line">soup = BeautifulSoup(res.text,<span class="string">'html.parser'</span>)</div><div class="line">header = soup.select(<span class="string">'h1'</span>)</div><div class="line"></div><div class="line"><span class="comment">#找出含有a标签的元素</span></div><div class="line"><span class="comment"># a_link = soup.select('a')</span></div><div class="line"><span class="comment"># print(a_link[0])</span></div><div class="line"></div><div class="line"><span class="comment">#找出class为art_p的元素</span></div><div class="line"><span class="comment"># for art_p in soup.select('.art_p'):</span></div><div class="line"><span class="comment">#     print(art_p)</span></div><div class="line"></div><div class="line"><span class="comment">#找出所有a标签内的链接</span></div><div class="line"><span class="comment"># alinks = soup.select('a')</span></div><div class="line"><span class="comment"># for link in alinks:</span></div><div class="line"><span class="comment">#     print(link['href'])</span></div><div class="line"></div><div class="line"><span class="comment">#找出文章标题，文章内容，以及文章时间和出处</span></div><div class="line">art_detail = &#123;&#125;</div><div class="line">art_time = soup.select(<span class="string">'.weibo_time'</span>)[0].contents[1].text + \</div><div class="line">           soup.select(<span class="string">'.weibo_time'</span>)[0].contents[2].strip()</div><div class="line">art_title = soup.select(<span class="string">'h1'</span>)[0].text</div><div class="line"><span class="comment">#字符串转时间</span></div><div class="line">art_time = <span class="string">'2017年'</span>+art_time</div><div class="line">dt = datetime.strptime(art_time,<span class="string">'%Y年%m月%d日%H:%M'</span>)</div><div class="line">dt = datetime.strftime(dt,<span class="string">'%Y年%m月%d日%H:%M'</span>)</div><div class="line"><span class="comment">#时间转字符串</span></div><div class="line"><span class="comment">#datetime.strftime("%Y-M-%d")</span></div><div class="line"><span class="comment">#处理内容</span></div><div class="line">art_content = soup.select(<span class="string">'.art_p'</span>)</div><div class="line">new_source = art_content[-1].text.lstrip(<span class="string">'来源：'</span>)</div><div class="line">art_content =<span class="string">'\n'</span>.join([p.text.strip() <span class="keyword">for</span> p <span class="keyword">in</span> art_content[:-1]])</div><div class="line"><span class="comment">#将拿到的信息放入到art_detail字典中</span></div><div class="line">art_detail[<span class="string">'art_content'</span>] = art_content</div><div class="line">art_detail[<span class="string">'art_time'</span>] = dt</div><div class="line">art_detail[<span class="string">'art_title'</span>] = art_title</div><div class="line">art_detail[<span class="string">'new_source'</span>] = new_source</div><div class="line"><span class="built_in">print</span>(art_detail)</div></pre></td></tr></table></figure><h1 id="备注一个问题"><a href="#备注一个问题" class="headerlink" title="备注一个问题"></a>备注一个问题</h1><p>object of type ‘Response’ has no len()<br>解决方案：soup = BeautifulSoup(res.text,’html.parser’)</p><p>OK 今天就这样 ^~^</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天看了一个关于爬虫的视频，心血来潮，就想弄弄爬虫，BUT只弄了一个简单页面，不过也算是体验一把，以后会慢慢深入研究（以后是什么鬼？？）&lt;/p&gt;
&lt;h1 id=&quot;代码示例（大神勿喷吐槽）&quot;&gt;&lt;a href=&quot;#代码示例（大神勿喷吐槽）&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
      <category term="python" scheme="http://yoursite.com/categories/python/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
  </entry>
  
</feed>
